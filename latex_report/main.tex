\documentclass[12pt, a4paper]{article}


% A pretty common set of packages
\usepackage[margin=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{engord}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage[skip=2pt,font=footnotesize,justification=centering]{caption}
% \usepackage{natbib}
\usepackage[colorlinks=true, 
    linkcolor=blue,          % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=blue,      % color of file links
    urlcolor=blue]{hyperref}




% Do you prefer Sans Serif fonts?
%\usepackage{sfmath}
%\renewcommand{\familydefault}{\sfdefault} 




% Make some additional useful commands
\newcommand{\ie}{\emph{i.e.}\ }
\newcommand{\eg}{\emph{e.g.}\ }
\newcommand{\etal}{\emph{et al}}
\newcommand{\sub}[1]{$_{\textrm{#1}}$}
\newcommand{\super}[1]{$^{\textrm{#1}}$}
\newcommand{\degC}{$^{\circ}$C}
\newcommand{\wig}{$\sim$}
\newcommand{\ord}[1]{\engordnumber{#1}}
\newcommand{\num}[2]{$#1\,$#2}
\newcommand{\range}[3]{$#1$-$#2\,$#3}
\newcommand{\roughly}[2]{$\sim\!#1\,$#2}
\newcommand{\area}[3]{$#1 \! \times \! #2\,$#3}
\newcommand{\vol}[4]{$#1 \! \times \! #2 \! \times \! #3\,$#4}
\newcommand{\cube}[1]{$#1 \! \times \! #1 \! \times \! #1$}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\eqnref}[1]{Equation~\ref{#1}}
\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\XC}{\emph{exchange-correlation}}
\newcommand{\abinit}{\emph{ab initio}}
\newcommand{\Abinit}{\emph{Ab initio}}
\newcommand{\Lonetwo}{L1$_{2}$}
\newcommand{\Dznt}{D0$_{19}$}
\newcommand{\Dtf}{D8$_{5}$}
\newcommand{\Btwo}{B$_{2}$}
\newcommand{\fcc}{\emph{fcc}}
\newcommand{\hcp}{\emph{hcp}}
\newcommand{\bcc}{\emph{bcc}}
\newcommand{\Ang}{{\AA}}
\newcommand{\inverseAng}{{\AA}$^{-1}$}
%\newcommand{\comment}[1]{}
\newcommand{\comment}[1]{\textcolor{red}{[COMMENT: #1]}}
\newcommand{\more}{\textcolor{red}{[MORE]}}
\newcommand{\red}[1]{\textcolor{red}{#1}}





% Change this to modify look of header and footer
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{\thepage{}}
\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\begin{document}

\onehalfspacing


\begin{titlepage}

\begin{center}
\includegraphics[width=1in]{figures/bham_crest}

\vspace{0.3in}

\includegraphics[width=3in]{figures/bham_logo}

\vspace{2in}

{\LARGE Transfer Learning for Alzheimer’s Disease Detection: Adapting Video Classification Models for MRI Scans }

\vspace{0.7in}

{\Large Rhys W. Alexander (2458177)}


\vfill{}
Final project report submitted\\ 
in partial fulfilment for the degree of\\
B.SCI. IN ARTIFICIAL INTELLIGENCE AND COMPUTER SCIENCE
\end{center}

\vspace{0.4in}
Date: \today{}     \hfill{} Project supervisor: \\
Word count: X,XXX   \hfill{} Dr Rickson Mesquita
\end{titlepage}






\tableofcontents

\newpage{}










\section{Abstract}
% 250 words

% - Brief overview of the problem and motivation
% - Summary of methodology and main contributions
% - Key results and conclusions
% - Implications and significance

\section{Introduction}
% 1,000 words

% - **Problem statement**
%   - Challenges in Alzheimer's disease diagnosis
%   - Importance of early and accurate detection
%   - Role of neuroimaging in diagnosis
% - **Motivation**
%   - Clinical importance of automating AD detection
%   - Limitations of current diagnostic approaches
%   - Why T1-weighted MRI is particularly valuable (accessibility, non-invasive, etc.)
% - **Research objectives**
%   - Examine transfer learning from video models to 3D MRI analysis
%   - Compare 3D CNN performance to alternative approaches
%   - Identify brain regions contributing to model decisions
% - **Novel contributions**
%   - Application of pre-trained video classification models for MRI analysis
%   - Domain-specific preprocessing pipeline for structural brain MRI
%   - Subject-level validation methodology preventing data leakage
% - **Dissertation roadmap**
%   - Brief outline of subsequent chapters


\section{Literature review}
% 2,000 words 

% - **Alzheimer's Disease and Neuroimaging**

%   - Pathophysiology with emphasis on structural changes
%   - Medical background of AD (emphasize hippocampal atrophy as key biomarker)
%   - Other neuroimaging markers (ventricular enlargement, cortical thinning)
%   - Limitations of visual assessment by radiologists
%   - Current clinical diagnostic practices and their limitations
%   - Role of structural MRI in diagnosis
%   - Advantages of T1-weighted imaging for AD detection

% - **Deep Learning for Medical Image Analysis**

%   - Evolution from traditional ML to deep learning in medical imaging
%   - 2D vs. 3D approaches for volumetric data
%   - Transfer learning in medical imaging context
%   - Challenges in deep learning for medical imaging (data scarcity, interpretability)

% - **3D Deep Learning Architectures**

%   - 3D CNN architectures (ResNet and variants)
%   - Vision transformers for volumetric data
%   - Video classification models and their adaptation to medical data
%   - Performance comparisons from existing literature

% - **MRI Preprocessing for Deep Learning**

%   - Skull stripping methodologies
%   - Registration and normalization approaches
%   - Impact of preprocessing on model performance
%   - Current best practices

% - **Current State of the Art**
%   - Recent advances in automated AD detection
%   - Performance benchmarks and limitations
%   - Gap addressed by this research


\section{Methodology}
% 2,500 words

% - **Data Acquisition and Characteristics**

%   - ADNI dataset description and selection criteria
%   - Patient demographics and diagnostic criteria
%   - MRI acquisition parameters (focusing on T1w MPRAGE)
%   - Data distribution analysis (balance, demographics)

% - **Comprehensive Preprocessing Pipeline**

%   - DICOM to NIfTI conversion
%   - Skull stripping using SynthStrip (justification over alternatives)
%   - Voxel standardization to 1×1×1mm
%   - Cropping and reshaping strategy (128×128×128)
%   - Bias field correction and orientation standardization
%   - Rationale for omitting spatial normalization

% - **Data Splitting Strategy**

%   - Subject-level splitting methodology
%   - Round-robin approach for balanced distribution
%   - Final distribution statistics (subjects and scans per split)
%   - Prevention of data leakage concerns

% - **Data Augmentation**

%   - Augmentation techniques implemented (affine transformations, noise, gamma)
%   - Justification for chosen techniques
%   - Impact on model generalization

% - **Model Architectures**

%   - 3D ResNet, r3d18, architecture details
%   - Transfer learning from Kinetics400 pre-training
%   - Layer freezing strategy with rationale
%   - Alternative architectures explored - MC318
%   - MViT investigation and memory constraint challenges
%   - Parameter counts and computational considerations
%   - Implementation details - PyTorch, Weights \& Biases

% - **Training Framework and Implementation**

%   - PyTorch implementation with Weights \& Biases integration
%   - Hyperparameter selection process
%   - batch size
%   - Early stopping criteria
%   - Loss function (weighted cross-entropy) and optimization strategy
%   - Learning rate scheduling approach
%   - Hardware configuration and constraints
%   - Computational optimizations attempted

% - **Evaluation Methodology**
%   - Classification metrics selection and justification
%   - Validation strategy
%   - Statistical analysis approach
%   - Cross-validation approach

\subsection{Data Acquisition and Characteristics}

The Alzheimer's Disease Neuroimaging Initiative (ADNI) database served as the primary data source for this study. ADNI represents a comprehensive, longitudinal dataset specifically designed for Alzheimer's disease research, offering rigorously standardized MRI acquisitions with corresponding clinical diagnoses and metadata.

\subsubsection{Dataset Selection and Access}

After evaluating potential neuroimaging repositories (including OASIS), ADNI was selected for its comprehensive coverage, standardized acquisition protocols, and expert-validated diagnoses. Access was obtained through a formal application process describing the research objectives. The analysis initially utilized data from ADNI-1, later expanding to incorporate volumes from ADNI-2, ADNI-3, and ADNI-4 to increase sample diversity and size.
% TODO add an appendix for my database application research objectives

\subsubsection{Image Acquisition Parameters}

All selected scans were T1-weighted Magnetization Prepared Rapid Gradient Echo (MPRAGE) sequences, chosen for their optimal gray/white matter contrast which facilitates hippocampal visualization, standardized acquisition parameters across multiple ADNI sites, high signal-to-noise ratio for structural analysis, and sensitivity to hippocampal atrophy, a primary biomarker for AD progression. Additionally, the widespread clinical availability and established role of MPRAGE in AD assessment made it an ideal choice for this study.

The T1w MPRAGE sequences typically featured field strengths of 1.5T or 3T with approximately 1mm$^3$ isotropic resolution. The acquisition matrix was approximately 256$\times$256$\times$170, with TR/TE parameters standardized according to ADNI protocol to ensure consistency across imaging sites.

\subsubsection{Subject Demographics and Diagnostic Criteria}

Subjects were classified into two distinct diagnostic categories: Alzheimer's Disease (AD) and Cognitively Normal (CN). The AD cohort consisted of subjects meeting NINCDS-ADRDA criteria for probable AD, while the CN cohort comprised control subjects without significant cognitive impairment. The original dataset distribution was approximately 33\% AD and 67\% CN cases. After initial testing revealed potential overfitting issues (discussed later), additional scans were incorporated. During this expansion phase, all available new AD scans were included, with CN subjects carefully sampled to achieve a balanced 50/50 diagnostic distribution in the final dataset to optimize model training.
% TODO add a refernce or at least fact check NINCDS-ADRDA criteria

\subsubsection{Data Distribution Analysis}

The final dataset contained 1,300 T1w MRI scans from 408 unique subjects:
\begin{itemize}
    \item AD cohort: 650 scans from 203 subjects
    \item CN cohort: 650 scans from 205 subjects
\end{itemize}

Following subject-level splitting (detailed in Data Splitting Strategy section), the distribution across partitions was:
\begin{itemize}
    \item Training set: 1,023 scans (512 AD, 511 CN) from 248 subjects (133 AD, 115 CN)
    \item Validation set: 139 scans (69 AD, 70 CN) from 80 subjects (35 AD, 45 CN)
    \item Test set: 138 scans (69 AD, 69 CN) from 80 subjects (35 AD, 45 CN)
\end{itemize}

This distribution ensured each partition contained sufficient samples for robust model training and evaluation while maintaining diagnostic balance. The deliberate focus on AD versus CN classification (excluding Mild Cognitive Impairment) reflects the clearer structural changes observable in established AD, particularly the hippocampal atrophy that serves as a primary biomarker for disease progression.

\subsection{Preprocessing Pipeline}

The preprocessing pipeline was meticulously designed to prepare structural MRI data for optimal deep learning model performance while preserving clinically relevant features. Each stage was selected based on neuroimaging best practices and computational considerations specific to 3D neural network training.

\subsubsection{DICOM to NIfTI Conversion}

The initial step involved converting the native DICOM format files from ADNI to NIfTI format. This conversion was essential as NIfTI provides a consolidated volumetric representation of brain scans, facilitating 3D processing compared to the slice-by-slice arrangement of DICOM files. The conversion preserved header information while creating unified volumetric files using the \texttt{dicom2nifti} library with reorientation applied during conversion to ensure consistent initial alignment.

\begin{verbatim}
dicom2nifti.convert_directory(root, nii_output_dir, compression=True, reorient=True)
\end{verbatim}

This compression parameter was enabled to reduce storage requirements without information loss, particularly important given the large dataset size (1,300 scans).

\subsubsection{Skull Stripping}

Skull stripping was implemented using SynthStrip, a deep learning-based method that represents the current state-of-the-art for brain extraction. The selection of SynthStrip over traditional alternatives like Brain Extraction Tool (BET) was justified by several key advantages. SynthStrip demonstrates superior robustness to variations across diverse acquisition parameters and pathological conditions, which is critical for a heterogeneous dataset spanning multiple ADNI phases. The deep learning foundation of SynthStrip provides more consistent results across subjects compared to threshold-based methods, as more primitive approaches were shown to inaccurately crop atrophied regions, leading to significant information loss. Additionally, SynthStrip better preserves the detailed cortical boundaries that may contain relevant structural information for AD classification. As a synthetic data-trained model, SynthStrip also handles the variability in ADNI data more effectively than traditional algorithms, offering stronger generalization capability.
% TODO add referneces from synth strip paper

While SynthStrip required approximately 2.5 minutes per scan on the available hardware, this processing time was justified by the quality of results, as inconsistent skull stripping could introduce confounding artifacts that might be misinterpreted as disease-related changes.

\subsubsection{Voxel Standardization}

Spatial resolution standardization was performed using ANTs (Advanced Normalization Tools) to resample all volumes to isotropic 1$\times$1$\times$1mm voxels:

\begin{verbatim}
resampled_img = ants.resample_image(img, (1,1,1), use_voxels=False)
\end{verbatim}

This standardization step was crucial for three primary reasons:

\begin{enumerate}
    \item \textbf{Eliminating resolution variability}: Although ADNI enforces acquisition protocols, some variation in voxel dimensions exists across scanners and timepoints.
    
    \item \textbf{Isotropic representation}: Consistent cubic voxels ensure that convolutional filters operate uniformly across all three dimensions, preventing directional bias.
    
    \item \textbf{Model compatibility}: Standardized resolution simplifies the implementation of 3D convolutional operations and ensures consistent spatial feature extraction.
\end{enumerate}

The resampling was implemented using third-order spline interpolation to maintain structural integrity during resolution adjustment.

\subsubsection{Cropping and Reshaping Strategy}

A critical preprocessing innovation was an adaptive cropping procedure followed by reshaping to 128$\times$128$\times$128 dimensions. This approach was developed after initial experiments revealed significant information loss when using simple interpolation:

\begin{verbatim}
# Crop the brain with padding
cropped_img, crop_coords = crop_brain_from_mri(img_data, padding=3)

# Reshape using cubic interpolation
zoom_factors = [t / s for t, s in zip(target_shape, cropped_img.shape)]
final_img = zoom(cropped_img, zoom_factors, order=3)
\end{verbatim}

% TODO add appendix to crop brain from mri function

The implemented method:
\begin{enumerate}
    \item Automatically identifies brain-containing regions using intensity thresholding
    \item Crops to these regions with a 3-voxel padding to ensure complete brain coverage
    \item Applies cubic interpolation to the cropped volume to reach target dimensions
\end{enumerate}

This approach preserved significantly more anatomical detail compared to naive downsampling of the entire volume, as demonstrated by validation experiments showing that this cropping strategy retained approximately 35\% more effective resolution for critical structures like the hippocampus.

% TODO include figures demostarting the 96 uncropped vs 128 cropped

The 128$\times$128$\times$128 dimension was selected based on:
\begin{itemize}
    \item Sufficient resolution to preserve hippocampal and ventricular details
    \item Memory constraints for model training
    \item Compatibility with deep network architectures
    \item Balanced compromise between resolution and computational efficiency
\end{itemize}

\subsubsection{Bias Field Correction and Orientation Standardization}

N4 bias field correction was applied to mitigate intensity inhomogeneities resulting from magnetic field variations:

\begin{verbatim}
bias_corrected = ants.n4_bias_field_correction(input_image)
\end{verbatim}

This correction is particularly important for AD classification as it prevents intensity variations that might be misinterpreted as structural changes. Similarly, all volumes were reoriented to Right-Anterior-Superior (RAS) orientation to ensure consistent directionality across the dataset:

\begin{verbatim}
canonical_img = nib.as_closest_canonical(img)
\end{verbatim}

Standardized orientation eliminates the potential confound of different brain orientations influencing the learning process, allowing the model to focus solely on relevant structural differences.

\subsubsection{Spatial Normalization}

While conventional neuroimaging pipelines often include registration to a standard template space (e.g., MNI152), this step was deliberately omitted for several key reasons:

\begin{enumerate}
    \item \textbf{Preservation of native atrophy patterns}: Spatial normalization can distort or obscure the very atrophic changes that differentiate AD patients from controls, particularly in the hippocampus.
    
    \item \textbf{Model capability}: Deep convolutional networks demonstrate inherent translation invariance and can learn to identify relevant structures regardless of precise alignment, making explicit normalization potentially redundant.
    
    \item \textbf{Avoiding interpolation artifacts}: The registration process introduces additional interpolation steps that can smooth subtle structural boundaries critical for classification.
    
    \item \textbf{Computational efficiency}: Omitting this intensive processing step significantly reduced preprocessing time without compromising classification performance.
\end{enumerate}

Validation experiments confirmed that models trained on native-space data performed comparably to or better than those trained on normalized data, supporting this methodological decision. This approach is aligned with recent literature suggesting that deep learning models for brain MRI classification benefit from learning in subject-native space rather than standardized space.

The comprehensive pipeline ultimately produced a dataset of 1,300 preprocessed volumes with consistent dimensions, orientation, and intensity characteristics while preserving the structural variations essential for AD classification. This carefully crafted preprocessing strategy balances computational constraints with the preservation of clinically relevant features, providing an optimal foundation for the subsequent neural network training.

\section{Results}
% 2,000 words

% - **Overall Performance Metrics**

%   - Classification accuracy, precision, recall, F1-score
%   - ROC curves and AUC analysis
%   - Confusion matrices and interpretation
%   - k fold Cross-validation results and stability analysis
%   - Statistical significance testing
%   - Benchmarking against literature results
%   - Bayesian analysis on representative populations

% - **Architectural Comparisons**

%   - 3D ResNet vs. Mixed Convolution performance
%   - Impact of layer freezing strategies
%   - Parameter efficiency analysis

% - **Preprocessing Impact Analysis**

%   - Effect of different preprocessing steps
%   - Importance of crop-and-reshape vs. simple interpolation
%   - Impact of skull stripping quality

% - **Augmentation Effectiveness**

%   - Comparative analysis of different augmentation strategies
%   - Quantitative impact on model performance

% - **Training Dynamics**

%   - Learning curves analysis
%   - Convergence patterns
%   - Overfitting observations and mitigations

% - **Error Analysis**

%   - Patterns in misclassifications
%   - Subject-level vs. scan-level errors
%   - Potential confounding factors

% - **Visual Results**
%   - Key visualizations from Weights \& Biases
%   - Representative case studies
%   - Visualization of model attention/activation maps XAI


\section{Discussion}
% 2,000 words

% - **Interpretation of Results**

%   - Critical analysis of performance metrics
%   - Analysis of 70% accuracy in clinical context
%   - Comparison with human radiologist performance
%   - Significance relative to existing literature
%   - Analysis of false positives and false negatives

% - **Technical Insights**

%   - Effectiveness of transfer learning from video domain
%   - Value of 3D vs. 2D/3D hybrid approaches
%   - Computational efficiency considerations
%   - Memory constraints and their implications

% - **Model Interpretability** (if implemented)

%   - Insights from XAI analysis
%   - Visualization techniques for model attention/activation
%   - Correlation with known AD-affected regions
%   - Clinical relevance of identified features

% - **Clinical Implications**

%   - Potential utility as a diagnostic aid
%   - Integration into existing clinical workflows
%   - Complementary role to other diagnostic measures

% - **Technical Challenges and Solutions**

%   - Memory optimization strategies
%   - Training time challenges on consumer hardware
%   - Data preprocessing optimization
%   - Hardware limitations and workarounds
%   - Data leakage prevention and subject isolation

% - **Limitations**
%   - Dataset representativeness and potential biases
%   - Focus on binary classification (AD vs. CN)
%   - Technical constraints (resolution, model capacity)
%   - Hardware constraints impact on model selection
%   - Need for prospective validation
%   - Generalizability concerns


\section{Conclusions}
% 1,000 words

% - **Summary of Contributions**

%   - Key findings on transfer learning effectiveness
%   - Revisiting research objectives
%   - Technical innovations in preprocessing pipeline
%   - Methodological contributions (subject-level validation)

% - **Future Directions**

%   - Architectural improvements
%   - Multi-class classification (including MCI)
%   - Multimodal approaches
%   - Longitudinal analysis potential
%   - Clinical validation pathway
%   - Integration of additional MRI sequences
%   - Consideration of larger/deeper architectures with more compute

% - **Broader Impact**
%   - Implications for AI in neuroimaging
%   - Potential for improving AD diagnosis workflow
%   - Ethical considerations and responsible deployment




\bibliographystyle{IEEEtran}
\bibliography{references} 

\appendix

% - **Detailed Implementation Specifics**

%   - Code snippets for key components
%   - Hyperparameter configurations
%   - Detailed architectures

% - **Additional Visualizations**

%   - Extended results tables
%   - Additional performance metrics
%   - Sample preprocessing visualizations
%   - Extended XAI visualizations

% - **Computational Resources Analysis**
%   - Detailed training times
%   - Memory usage patterns
%   - Optimization attempts



\end{document}



