\documentclass[12pt, a4paper]{article}


% A pretty common set of packages
\usepackage[margin=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{engord}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage[skip=2pt,font=footnotesize,justification=centering]{caption}
% \usepackage{natbib}
\usepackage[colorlinks=true, 
    linkcolor=blue,          % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=blue,      % color of file links
    urlcolor=blue]{hyperref}




% Do you prefer Sans Serif fonts?
%\usepackage{sfmath}
%\renewcommand{\familydefault}{\sfdefault} 




% Make some additional useful commands
\newcommand{\ie}{\emph{i.e.}\ }
\newcommand{\eg}{\emph{e.g.}\ }
\newcommand{\etal}{\emph{et al}}
\newcommand{\sub}[1]{$_{\textrm{#1}}$}
\newcommand{\super}[1]{$^{\textrm{#1}}$}
\newcommand{\degC}{$^{\circ}$C}
\newcommand{\wig}{$\sim$}
\newcommand{\ord}[1]{\engordnumber{#1}}
\newcommand{\num}[2]{$#1\,$#2}
\newcommand{\range}[3]{$#1$-$#2\,$#3}
\newcommand{\roughly}[2]{$\sim\!#1\,$#2}
\newcommand{\area}[3]{$#1 \! \times \! #2\,$#3}
\newcommand{\vol}[4]{$#1 \! \times \! #2 \! \times \! #3\,$#4}
\newcommand{\cube}[1]{$#1 \! \times \! #1 \! \times \! #1$}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\eqnref}[1]{Equation~\ref{#1}}
\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\XC}{\emph{exchange-correlation}}
\newcommand{\abinit}{\emph{ab initio}}
\newcommand{\Abinit}{\emph{Ab initio}}
\newcommand{\Lonetwo}{L1$_{2}$}
\newcommand{\Dznt}{D0$_{19}$}
\newcommand{\Dtf}{D8$_{5}$}
\newcommand{\Btwo}{B$_{2}$}
\newcommand{\fcc}{\emph{fcc}}
\newcommand{\hcp}{\emph{hcp}}
\newcommand{\bcc}{\emph{bcc}}
\newcommand{\Ang}{{\AA}}
\newcommand{\inverseAng}{{\AA}$^{-1}$}
%\newcommand{\comment}[1]{}
\newcommand{\comment}[1]{\textcolor{red}{[COMMENT: #1]}}
\newcommand{\more}{\textcolor{red}{[MORE]}}
\newcommand{\red}[1]{\textcolor{red}{#1}}





% Change this to modify look of header and footer
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{\thepage{}}
\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\begin{document}

\onehalfspacing


\begin{titlepage}

\begin{center}
\includegraphics[width=1in]{figures/bham_crest}

\vspace{0.3in}

\includegraphics[width=3in]{figures/bham_logo}

\vspace{2in}

{\LARGE Transfer Learning for Alzheimer’s Disease Detection: Adapting Video Classification Models for MRI Scans }

\vspace{0.7in}

{\Large Rhys W. Alexander (2458177)}


\vfill{}
Final project report submitted\\ 
in partial fulfilment for the degree of\\
B.SCI. IN ARTIFICIAL INTELLIGENCE AND COMPUTER SCIENCE
\end{center}

\vspace{0.4in}
Date: \today{}     \hfill{} Project supervisor: \\
Word count: X,XXX   \hfill{} Dr Rickson Mesquita
\end{titlepage}





\setcounter{tocdepth}{2}
\tableofcontents

\newpage{}










\section{Abstract}
% 250 words

% - Brief overview of the problem and motivation
% - Summary of methodology and main contributions
% - Key results and conclusions
% - Implications and significance

\section{Introduction}
% 1,000 words

% - **Problem statement**
%   - Challenges in Alzheimer's disease diagnosis
%   - Importance of early and accurate detection
%   - Role of neuroimaging in diagnosis
% - **Motivation**
%   - Clinical importance of automating AD detection
%   - Limitations of current diagnostic approaches
%   - Why T1-weighted MRI is particularly valuable (accessibility, non-invasive, etc.)
% - **Research objectives**
%   - Examine transfer learning from video models to 3D MRI analysis
%   - Compare 3D CNN performance to alternative approaches
%   - Identify brain regions contributing to model decisions
% - **Novel contributions**
%   - Application of pre-trained video classification models for MRI analysis
%   - Domain-specific preprocessing pipeline for structural brain MRI
%   - Subject-level validation methodology preventing data leakage
% - **Dissertation roadmap**
%   - Brief outline of subsequent chapters


\section{Literature Review}
% 2,000 words 
% Writing tips:
% 1. Begin each section with broader concepts before narrowing to specifics
% 2. Make explicit connections between sections to create a coherent narrative
% 3. Balance technical detail with accessibility for interdisciplinary readers
% 4. Maintain a critical perspective, noting limitations of existing approaches
% 5. Use transitions to highlight how each topic relates to your research focus
% 6. Conclude each major section with implications for your research approach


\subsection{Alzheimer's Disease and Neuroimaging}

\subsubsection{Pathophysiology with Emphasis on Structural Changes}

% - Definition, prevalence, and burden of Alzheimer's Disease
% - Progression of pathological changes (amyloid plaques, neurofibrillary tangles)
% - Timeline of structural vs. cognitive symptoms
% - Regional patterns of neurodegeneration
% - Suggested references:
% - Teipel et al. (2013) for relevance of MRI in early detection
% - Jack et al. (2013) for AD pathophysiology progression models

\subsubsection{Hippocampal Atrophy as Primary Biomarker}

% - Hippocampal volume reduction patterns and progression
% - Quantification methods and established thresholds
% - Correlation with cognitive decline and disease progression
% - Sensitivity and specificity as a diagnostic marker
% - Suggested references:
% - Teipel et al. (2013) for hippocampal changes
% - Cuingnet et al. (2011) for hippocampal measurement methods
% - Jack et al (1992) for hippocampal volumetry

\subsubsection{Additional Neuroimaging Markers}

% - Ventricular enlargement patterns and diagnostic value
% - Cortical thinning in temporal, parietal, and frontal regions
% - White matter changes and connectivity disruptions
% - Relative sensitivity of different markers
% - Suggested references:
% - Teipel et al. (2013) for multiple imaging markers
% - L Ferrarini et al. (2006) for ventricular changes
% - L Gutiérrez-Galve et al. (2009) on cortical thickness

\subsubsection{Current Clinical Diagnostic Practices and Limitations}

% - Diagnostic criteria (NINCDS-ADRDA) and workflow
% - Multi-modal diagnostic approach (clinical, cognitive, biomarkers)
% - Inter-reader variability in visual assessment
% - Challenges in early and accurate detection
% - Delay between pathological changes and clinical diagnosis
% - Suggested references:
% - Cuingnet et al. (2011) for diagnostic challenges
% - jack et al. (2018) for NIA-AA guidelines
% - B Dubois et al. (2007) for NINCDS-ADRDA
% - Klöppel et al. (2008) on radiologist performance

\subsubsection{Role of Structural MRI in Diagnosis}

% - Position in diagnostic algorithm and clinical workflow
% - Complementary role to clinical assessment and other biomarkers
% - Accessibility advantages compared to PET and CSF markers
% - Limitations of visual/manual assessment
% - Suggested references:
% - Teipel et al. (2013) covers comprehensive role of MRI
% - B Dubois et al. (2007) for NINCDS-ADRDA on MRI in current diagnostic guidelines

\subsubsection{Advantages of T1-weighted Imaging for AD Detection}

% - Optimal tissue contrast for detecting atrophy
% - Standardized acquisition protocols (MPRAGE)
% - Wider availability compared to specialized sequences
% - Trade-offs with other imaging modalities
% - Suggested references:
% - Herrera et al. (2013) for MRI techniques in classification

\subsection{Deep Learning for Medical Image Analysis}

\subsubsection{Evolution from Traditional ML to Deep Learning}

% - Historical progression from handcrafted features to learned representations
% - Traditional machine learning techniques in neuroimaging
% - Key milestones in deep learning for medical imaging
% - Performance comparisons between approaches
% - Suggested references:
% - Litjens et al. (2017) for survey on deep learning in medical imaging
% - Cuingnet et al. (2011) for earlier machine learning approaches
% - M Bari Antoret al. (2021) for a comparative analysis of ML algorithms to predict AD

\subsubsection{2D vs. 3D Approaches for Volumetric Data}

% - Fundamental differences in information extraction
% - Trade-offs between slice-based and volumetric analysis
% - Memory and computational considerations
% - Information loss in 2D vs. implementation complexity in 3D
% - Suggested references:
% - Gunawardena et al. (2017) for CNNs on AD in structural mri 
% - Yang et al. (2021) for dimensionality considerations
% - Payan and Montana (2015) for early 3D CNN work
% - Sarraf and Tofighi (2016) and Liang et al. (2021) for 2D approaches

\subsubsection{Transfer Learning in Medical Imaging}

% - Definition and rationale for transfer learning
% - Domain shift challenges between natural images and medical imaging
% - Pre-training strategies (natural images vs. medical domain) and layer freezing approaches
% - Previous successes in neuroimaging applications
% - Suggested references:
% - Hon and Khan (2017), Ebrahimi-Ghahnavieh et al. (2019), Acharya et al. (2021) for transfer learning for AD
% - Maqsood et al. (2019) and Wu et al. (2022) for 3D transfer learning
% - Mehmood et al. (2021) for early diagnosis applications
% - Francis et al. (2025) for attention mechanisms with transfer learning
% - A Ebrahimi et al. (2020) on a practically indistinguishable project to mine

\subsubsection{Challenges in Deep Learning for Medical Imaging}

% - Data scarcity and class imbalance issues
% - Interpretability requirements in clinical context
% - Validation challenges and risks of overfitting
% - Privacy and ethical considerations
% - Suggested references:
% - Litjens et al. (2017) for broad challenges
% - C Davatzikos et al. (2019) for ML neuroimaging challenges

\subsection{3D Deep Learning Architectures}

\subsubsection{3D CNN Architectures (ResNet and Variants)}

% - Core principles of 3D CNNs
% - Residual learning principles applied to 3D volumes
% - Architecture details and implementation considerations
% - Parameter efficiency and resource requirements
% - Suggested references:
% - Payan and Montana (2015) for early implementations
% - Wu et al. (2022) for 3D transfer learning networks
% - A Ebrahimi et al. (2020) on 3D ResNet implementations and optimizations

\subsubsection{Vision Transformers for Volumetric Data}

% - Adaptation of transformer architectures to 3D medical data
% - Self-attention mechanisms for spatial context
% - Performance comparisons with CNN-based approaches
% - Advantages and limitations for neuroimaging
% - Suggested references:
% - Y Lyu et al. (2022) for AD vision transformers
% - Lu et al. (2025) for efficient vision transformers
% - Yan et al. (2025) for hybrid ResNet-ViT approach
% - Mubonanyikuzo et al. (2025) for systematic review

\subsubsection{Video Classification Models and Medical Adaptation}

% - Parallels between video sequences and volumetric medical data
% - Key video architectures (MC13, R(2+1)D, r3d_18, etc.)
% - Temporal vs. spatial dimension modeling
% - Transfer learning strategies from video domains
% - Suggested references:
% - A Ebrahimi et al. (2020) on a practically indistinguishable project to mine
% - D Tran et al. (2018) on temporal/spatial modeling strategies plus resnet mc, 2+1, and normal

\subsubsection{Performance Comparisons from Existing Literature}

% - Benchmark results across architecture types
% - Standardized datasets and evaluation metrics
% - Computational efficiency vs. accuracy trade-offs
% - Memory requirements considerations
% - Suggested references:
% - Cuingnet et al. (2011) for early benchmarking
% - Basaia et al. (2019) for more recent comparisons
% - N Garg et al. (2023) updated comparative study

\subsection{MRI Preprocessing for Deep Learning}

\subsubsection{Skull Stripping Methodologies}

% - Importance for AD classification
% - Comparison of traditional vs. learning-based (synthstrip) approaches
% - Quality considerations and failure modes
% - Impact on downstream classification performance
% - Suggested references:
% - A Hoopes et al. (2022) on synthstrip
% - A Fatima et al. (2020) on skull stripping methods and their impact

\subsubsection{Registration and Normalization Approaches}

% - Standard space registration (MNI152, etc.)
% - Intensity normalization techniques
% - Impact of registration accuracy on classification
% - Trade-offs between standardization and preserving pathology
% - Suggested references:
% - N Garg et al. (2023) on registration methods and effects on classification

\subsubsection{Impact of Preprocessing on Model Performance}

% - Empirical studies measuring preprocessing effects
% - Sensitivity analysis of different preprocessing steps
% - Relative importance of preprocessing pipeline components
% - Domain-specific considerations for AD
% - Suggested references:
% - V Viswan et al. (2025) on preprocessing effects on deep learning performance

\subsubsection{Current Best Practices}

% - Consensus approaches in neuroimaging literature
% - Standardized pipelines (e.g., FreeSurfer, FSL)
% - Preprocessing considerations specific to deep learning
% - Areas of ongoing research and debate
% - Suggested references:
% - V Viswan et al. (2025) on best practices for neuroimaging preprocessing

\subsubsection{Data Partitioning and Group Leakage Prevention}

% - Definition of group/data leakage in neuroimaging studies
% - Impact of subject-level vs. scan-level partitioning on reported performance
% - Methods for proper cross-validation in longitudinal datasets
% - Critical evaluation of published results that may suffer from leakage
% - Suggested references:
% - Gap: data leakage in neuroimaging studies
% - Gap: methodological challenges in ML for neuroimaging
% - Gap: cross-validation strategies for neuroimaging

\subsection{Current State of the Art}

\subsubsection{Recent Advances in Automated AD Detection}

% - Leading approaches from recent literature (2020-2025)
% - Performance breakthroughs and methodological innovations
% - Multi-modal integration approaches
% - Current performance benchmarks
% - Suggested references:
% - Farooq et al. (2017) for a leading approach
% - Francis et al. (2025), Lu et al. (2025), and Yan et al. (2025), A Ebrahimi et al. (2020) represent recent approaches
% - Mubonanyikuzo et al. (2025) for meta-analysis
% - Saikia and Kalita (2024), Menagadevi et al. (2024) for reviews

\subsubsection{Performance Limitations and Challenges}

% - Current performance ceilings
% - Generalizability concerns and dataset biases
% - Challenges in cross-dataset validation
% - Clinical integration barriers despite high reported accuracy
% - Suggested references:
% - Basaia et al. (2019) for established benchmarks
% - Pradhan et al. (2024) for recent analysis

\subsubsection{Research Gap Addressed by This Work}

% - Synthesis of limitations in existing approaches
% - Novelty of applying video model transfer learning
% - Expected advantages of proposed approach
% - Positioning your research in the current landscape
% - Suggested references:
% - This subsection should reference gaps identified throughout your review
% - Connect to your specific research questions and hypotheses

\section{Methodology}
% 2,500 words

% - **Data Acquisition and Characteristics**

%   - ADNI dataset description and selection criteria
%   - Patient demographics and diagnostic criteria
%   - MRI acquisition parameters (focusing on T1w MPRAGE)
%   - Data distribution analysis (balance, demographics)

% - **Comprehensive Preprocessing Pipeline**

%   - DICOM to NIfTI conversion
%   - Skull stripping using SynthStrip (justification over alternatives)
%   - Voxel standardization to 1×1×1mm
%   - Cropping and reshaping strategy (128×128×128)
%   - Bias field correction and orientation standardization
%   - Rationale for omitting spatial normalization

% - **Data Splitting Strategy**

%   - Subject-level splitting methodology
%   - Round-robin approach for balanced distribution
%   - Final distribution statistics (subjects and scans per split)
%   - Prevention of data leakage concerns

% - **Data Augmentation**

%   - Augmentation techniques implemented
%   - Justification for chosen techniques
%   - Impact on model generalization

% - **Model Architectures**

%   - 3D ResNet, r3d18, architecture details
%   - Transfer learning from Kinetics400 pre-training
%   - Layer freezing strategy with rationale
%   - Alternative architectures explored - MC318
%   - MViT investigation and memory constraint challenges
%   - Parameter counts and computational considerations
%   - Implementation details - PyTorch, Weights \& Biases

% - **Training Framework and Implementation**

%   - PyTorch implementation with Weights \& Biases integration
%   - Hyperparameter selection process
%   - batch size
%   - Early stopping criteria
%   - Loss function (weighted cross-entropy) and optimization strategy
%   - Learning rate scheduling approach
%   - Hardware configuration and constraints
%   - Computational optimizations attempted

% - **Evaluation Methodology**
%   - Classification metrics selection and justification
%   - Validation strategy
%   - Statistical analysis approach
%   - Cross-validation approach

\subsection{Data Acquisition and Characteristics}

The Alzheimer's Disease Neuroimaging Initiative (ADNI) database served as the primary data source, providing standardized MRI acquisitions with corresponding clinical diagnoses. ADNI was selected over alternatives (including OASIS) for its comprehensive coverage, acquisition protocols, and expert-validated diagnoses.

\subsubsection{Dataset Composition}

All selected scans were T1-weighted MPRAGE sequences (1.5T or 3T, 1mm³ isotropic resolution), chosen for optimal gray/white matter contrast, standardized acquisition parameters, and sensitivity to atrophy biomarkers. Additionally, the widespread clinical availability and established role of MPRAGE in AD assessment made it an ideal choice for this study. The final dataset contained 1,300 scans from 408 unique subjects, balanced between diagnostic categories:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Partition} & \textbf{AD} & \textbf{CN} \\
\hline
Training & 512 scans (133 subjects) & 511 scans (115 subjects) \\
Validation & 69 scans (35 subjects) & 70 scans (45 subjects) \\
Test & 69 scans (35 subjects) & 69 scans (45 subjects) \\
\hline
\end{tabular}
\caption{Distribution of scans and subjects across dataset partitions}
\end{table}

\subsubsection{Diagnostic Criteria}

Subjects were classified as Alzheimer's Disease (AD) or Cognitively Normal (CN) based on NINCDS-ADRDA criteria. Initially, the dataset contained approximately 33\% AD and 67\% CN cases. To address class imbalance and potential overfitting issues identified during preliminary experiments, additional AD scans were incorporated and CN subjects carefully sampled to achieve a balanced 50/50 diagnostic distribution.

The binary classification focus (excluding Mild Cognitive Impairment) reflects the clearer structural changes observable in established AD, particularly hippocampal atrophy, which serves as a primary biomarker for disease progression. Subject-level isolation between dataset partitions was strictly enforced to prevent data leakage, ensuring realistic performance assessment for unseen individuals.

\subsection{Preprocessing Pipeline}

\subsubsection{Initial Processing and Skull Stripping}

Raw DICOM images were converted to NIfTI format using \texttt{dicom2nifti} with reorientation and compression enabled. This created unified volumetric files suitable for 3D analysis. Skull stripping was performed using SynthStrip, a deep learning-based method that represents the current state-of-the-art for brain extraction. It was selected for its superior performance with atrophied brains. Unlike traditional threshold-based methods (e.g., BET), SynthStrip preserved critical cortical boundaries even with atrophied brains and better handled the variability in the ADNI dataset. Despite requiring ~2.5 minutes per scan, the improved quality justified this approach by preventing potential misinterpretation of artifacts as disease-related changes.
% TODO add referneces from synth strip paper

\subsubsection{Volume Standardization}

All volumes were resampled to isotropic 1×1×1mm voxels using ANTs with third-order spline interpolation. This standardization ensured consistent spatial representation, eliminated scanner-specific resolution variability, and enabled uniform convolutional filter operations across all dimensions.

\subsubsection{Adaptive Cropping Strategy}

A key methodological innovation was the implementation of an adaptive cropping procedure followed by reshaping to 128×128×128 dimensions. The approach:

\begin{enumerate}
    \item Identified brain-containing regions using intensity thresholding
    \item Applied cropping with minimal padding (3 voxels)
    \item Used cubic interpolation to reach the target dimensions
\end{enumerate}
% TODO add appendix to crop brain from mri function

This method preserved approximately 35\% more effective resolution for critical structures like the hippocampus compared to naive downsampling. The 128³ dimension balanced preserving anatomical detail with memory constraints for model training.
% TODO include figures demostarting the 96 uncropped vs 128 cropped

\subsubsection{Intensity Normalization and Orientation}

N4 bias field correction was applied to mitigate intensity inhomogeneities from magnetic field variations. This prevents intensity variations that might be misinterpreted as structural changes. All volumes were reoriented to Right-Anterior-Superior (RAS) orientation to ensure consistent directionality, allowing the model to focus solely on relevant structural differences rather than arbitrary orientation variations.
% TODO reference code from appendix

\subsubsection{Omission of Spatial Normalization}

Despite its common use in neuroimaging pipelines, registration to standard space (e.g., MNI152) was deliberately omitted for several reasons:
\begin{enumerate}
    \item Preservation of native atrophy patterns that could be distorted during normalization
    \item Reliance on CNN translation invariance to identify structures without explicit alignment
    \item Avoidance of interpolation artifacts that might smooth critical structural boundaries
    \item Computational efficiency gains without compromising classification performance
\end{enumerate}

Validation experiments confirmed that models trained on native-space data performed comparably to or better than those using normalized data, supporting this methodological decision and aligning with recent literature suggesting deep learning models for brain MRI benefit from native-space learning.

The entire pipeline produced 1,300 preprocessed volumes with consistent dimensions, orientation, and intensity characteristics while preserving the structural variations essential for AD classification.

\subsection{Data Splitting Strategy}

A methodologically rigorous data splitting approach was implemented to prevent data leakage while maintaining diagnostic balance across partitions. Unlike conventional image classification tasks, neuroimaging datasets require subject-level rather than scan-level splitting since multiple scans often exist for the same individual.

\subsubsection{Subject-Level Isolation}

A strict subject-level isolation approach ensured no individual appeared in multiple dataset partitions—a critical decision after initial experiments revealed artificially inflated performance metrics (~90\% accuracy) when subjects were allowed to cross partition boundaries. Complete subject isolation produced a more realistic performance assessment (~77\% accuracy), better reflecting the model's generalization capability to unseen individuals.
% TODO add a figure showing the difference in accuracy with and without subject isolation

\subsubsection{Partition Distribution}

The dataset was divided following an 80/10/10 (train/validation/test) ratio using a round-robin algorithm that:
\begin{enumerate}
    \item Grouped subjects by diagnostic condition
    \item Sorted subjects in ascending order by scan count
    \item Allocated subjects to partitions round robin to insure subject diversity across partitions
    \item Final scan counts were balanced to maintain equal scan counts per diagnostic category
\end{enumerate}

This approach yielded a balanced distribution with 1,023 training scans (512 AD/511 CN), 139 validation scans (69 AD/70 CN), and 138 test scans (69 AD/69 CN). The strict isolation maintained 203 unique subjects in training, 80 in validation, and 80 in test sets, with diagnostic balance preserved in each partition.

\paragraph{Data Leakage Prevention}

To prevent subtle forms of data leakage, subject identifiers were rigorously tracked and preprocessing parameters (such as intensity normalization statistics) were computed independently within each partition. This methodologically sound approach ensured that performance metrics would accurately reflect the model's ability to generalize to entirely new individuals, rather than merely recognizing previously seen subjects in different scans.
% TODO refer to previous research that didn't do this

\subsection{Data Augmentation}

Data augmentation was strategically implemented to improve model generalization while preserving diagnostically relevant features. Through systematic experimentation, a minimal yet effective set of transformations was identified:

\begin{verbatim}
tio.Compose([
    tio.RandomNoise(mean=0.0, std=0.1, p=0.3),
    tio.RandomGamma(log_gamma=(-0.2, 0.2), p=0.3),
    tio.ZNormalization(),
])
\end{verbatim}

This approach was applied exclusively to the training set, while validation and test sets received only Z-normalization to maintain evaluation consistency.
% TODO add demonstration of normalisation effectiveness from post-aug-norm


Each technique addressed specific neuroimaging considerations: Random noise (30\% probability, $\sigma$=0.1) simulated scanner variability and promoted robustness to image quality differences; Gamma adjustment (±0.2 range, 30\% probability) mimicked contrast variations between scanners; Z-normalization standardized intensity values across all scans for consistent feature extraction.

Notably, several common augmentation techniques were deliberately excluded after experimental evaluation showed either no benefit or negative impact:

\begin{itemize}
    \item \textbf{Geometric transformations} (rotations, flips) significantly increased training time (~20 vs ~5 epochs) without improving validation accuracy, likely due to inherent orientation variability already present in MRI data.
    
    \item \textbf{Random scaling} (0.9-1.1) showed no generalization improvement and potentially disrupted the carefully standardized voxel dimensions.
\end{itemize}

The final strategy evolved from extensive transformations to this focused set through iterative evaluation of validation performance and convergence speed, representing an optimal balance between enhancing robustness and preserving critical structural features essential for AD classification.
\subsection{Model Architectures}
\subsubsection{3D ResNet Architecture}

The primary model was a modified 3D ResNet-18 (r3d\_18), selected for its residual connections that mitigate vanishing gradients, fully 3D convolutional operations to preserve volumetric spatial relationships, and parameter efficiency (33M parameters) enabling training on consumer hardware. The ResNet architecture family has demonstrated robust performance across numerous computer vision tasks, including medical imaging applications, and is used frequently in the literature. The implementation used PyTorch's pre-trained r3d\_18 model, with the first layer modified to accept single-channel MRI volumes and the final layer adapted for binary classification.

% TODO add resnet diagram
The model architecture consisted of 18 layers, with the first layer being a 3D convolutional layer followed by four residual blocks, each containing two 3D convolutional layers. The final fully connected layer was adapted to output binary classification scores. The model was trained using a transfer learning approach, leveraging pre-trained weights from the Kinetics400 dataset, which provided a strong initialization for the feature extraction layers.
% TODO add figure of the model architecture

\subsubsection{Transfer Learning Strategy}

We implemented a selective transfer learning approach, freezing early convolutional layers (25\% of parameters) while allowing the final residual block and fully connected layer (75\%) to adapt to MRI-specific features. This balanced preserving pre-trained knowledge with domain adaptation. Initial experiments with more aggressive freezing (keeping only the final fully connected layer trainable) resulted in numerical instabilities during training, manifested as NaN losses, suggesting that significant domain adaptation was necessary given the substantial differences between video action recognition and MRI classification.

A differential learning rate strategy applied a 10× higher learning rate to the newly initialized fully connected layer compared to the fine-tuned convolutional layers, enabling aggressive adaptation in the task-specific output layer while making more conservative updates to the pre-trained feature extraction layers.
% TODO add a figure showing the difference in learning rates maybe point to the code

\subsubsection{Architecture Comparison}

To validate architectural choices, models were systematically evaluated with decreasing levels of 3D feature extraction:

\begin{enumerate}
    \item \textbf{Mixed Convolution 3D Network}: This model (MC3-18) uses a hybrid approach combining 2D and 3D convolutions, hypothesized to potentially offer computational efficiency while maintaining performance.
      
      Experimental results with MC3-18 showed less stable training dynamics and inferior performance compared to the pure 3D approach of R3D-18, supporting the importance of fully volumetric feature extraction for structural MRI analysis. The differences in performance provided empirical justification for the primary architectural choice.
      % TODO add graph

    \item \textbf{(2+1)D Convolution Network}: Following the investigation of MC3-18, a (2+1)D architecture was also evaluated. This approach decomposes 3D convolutions into separate spatial (2D) and temporal (1D) convolutions, a technique that has shown promise in video classification tasks.
      
      Results with the (2+1)D architecture revealed performance that was slightly worse than MC3-18, continuing the observed trend that classification accuracy decreased as the model architecture incorporated more 2D elements. This progression (R3D > MC3 > (2+1)D) strongly suggests that preserving the full 3D spatial context through pure 3D convolutions is critical for detecting the subtle volumetric patterns associated with Alzheimer's disease in MRI data.
      % TODO add graph
      
    \item \textbf{Multiscale Vision Transformer}: Recent advances in vision transformers prompted investigation of their potential for 3D MRI classification. However, initial implementation attempts revealed significant computational barriers:
      
      \begin{enumerate}
        \item Memory requirements exceeded available hardware capabilities (32GB RAM requirement for 128×128×128 volumes)
        \item Architectural mismatch between the input dimensions required by MViT (designed for 16×224×224 video clips) and the cubical 128×128×128 MRI volumes
        \item Transformer architectures typically require substantially larger training datasets than were available
      \end{enumerate}
      
      These constraints prevented full evaluation of transformer-based approaches, highlighting an important practical limitation in applying state-of-the-art vision models to medical imaging with limited computational resources.
\end{enumerate}

\subsubsection{Parameter Counts and Computational Considerations}

The final model architecture parameters were:

\begin{itemize}
    \item \textbf{Total parameters}: 33,148,482
    \item \textbf{Trainable parameters}: 24,909,826 (75.15\%)
    \item \textbf{Frozen parameters}: 8,238,656 (24.85\%)
\end{itemize}

These figures represent a significant reduction compared to larger architectures like ResNet-50 or ViT variants, making training feasible on consumer-grade hardware while maintaining sufficient capacity for the classification task. The reduced parameter count also potentially mitigated overfitting given the relatively small dataset size.

\subsection{Training Framework and Implementation}

Training was conducted on an M1 Mac using Metal Performance Shaders, with each epoch requiring approximately one hour and full training runs taking ~20 hours. This hardware constrained batch size and architecture selection. Despite attempts at optimization through mixed precision training and CPU-GPU synchronization, computational bottlenecks in the model's forward pass remained.

Hyperparameters were selected through systematic experimentation and tracked with Weights \& Biases:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|p{5.5cm}|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Rationale} \\
\hline
Learning rate & 0.001 (FC), 0.0001 (conv) & Differential rates for aggressive output adaptation with conservative updates to pre-trained layers \\
\hline
Optimizer & AdamW (weight decay=0.01) & Effective regularization for the limited dataset \\
\hline
Batch size & 2 & Memory constraints from 128³ inputs \\
\hline
LR schedule & Cosine annealing ($T_0$=5) & Prevents convergence to local minima \\
\hline
\end{tabular}
\caption{Optimized hyperparameter configuration}
\end{table}

A weighted cross-entropy loss function addressed potential class imbalance with weights dynamically calculated based on class distribution, particularly important during initial experiments when the dataset had not yet been fully balanced. This ensured balanced contribution to loss regardless of class representation.

% TODO add to appendix

Early stopping with patience=5 monitored both validation accuracy and loss, ensuring training continued as long as either metric showed enhancement, preventing overfitting while optimizing computational resources. Most models converged within 5-10 epochs, with early stopping typically triggering around epoch 7-8—quick convergence attributable to the transfer learning initialization.

A comprehensive checkpoint system saved regular epoch checkpoints and best models based on both accuracy and loss metrics. Each checkpoint stored model weights, optimizer state, scheduler state, and performance metrics for seamless training resumption. The system integrated with Weights \& Biases to log best models as artifacts.

The training loop was implemented with careful attention to numerical stability and memory management. Memory optimization techniques included setting gradients to \texttt{None} rather than zero (reducing memory fragmentation) and using tensor operations that maintained computational efficiency. For MPS acceleration, explicit cache clearing was performed at the end of each epoch to prevent memory accumulation.

\subsection{Evaluation Methodology}

\subsubsection{Performance Metrics}

A comprehensive set of metrics was implemented to evaluate model performance beyond simple accuracy:

\begin{itemize}
    \item \textbf{Accuracy and balanced accuracy}: The latter particularly important for medical applications as it equalizes the contribution of each diagnostic class.
    
    \item \textbf{Precision and recall}: Critical for clinical utility, measuring correct positive predictions and the ability to identify true AD cases, respectively.
    
    \item \textbf{Specificity}: Quantified the model's ability to correctly identify CN cases ($TN/(TN+FP)$).
    
    \item \textbf{F1-score, ROC-AUC, and average precision}: Provided threshold-independent performance assessment.
\end{itemize}

All metrics were continuously tracked and logged using a custom \texttt{MetricsManager} class, with implementation details provided in Appendix X.
% TODO add to appendix

\subsubsection{Validation Strategy}

The evaluation framework employed strict subject-level isolation to prevent data leakage:

\begin{itemize}
    \item Dedicated validation (10\%) and test (10\%) sets maintained complete separation from training data.
    
    \item Multiple model checkpoints were saved (best accuracy and best loss) to mitigate selection bias.
    
    \item Final evaluation used only the held-out test set with the best validation accuracy checkpoint.
\end{itemize}

\subsubsection{Statistical Analysis}

Statistical rigor was ensured through:

\begin{itemize}
    \item Bootstrap confidence intervals for key metrics to quantify the uncertainty in performance estimates
    
    \item Confusion matrix analysis to identify classification patterns
    
    \item Comparison to baselines: random chance (50\%), clinical radiologist performance, and published algorithmic approaches
\end{itemize}

\subsubsection{Cross-Validation and Architecture Evaluation}

Despite computational constraints (20-hour training runs on M1 Mac), model robustness was verified through:

\begin{itemize}
    \item Subject-level 3-fold cross-validation with diagnostic balance and subject-level isolation maintained across all partitions.
    
    \item Systematic architecture comparison across R3D-18, MC3-18, and R2Plus1D-18 to assess the impact of dimensional processing on performance and to validate the choice of fully 3D convolutional architectures
    
    \item Visualization techniques to provide qualitative insights into model behavior, rather than relying solely on quantitative metrics
\end{itemize}

Cross-validation reveal performance consistency across subject groupings, while architectural evaluation demonstrate whether fully 3D convolutional approaches systematically outperform partial 2D/3D hybrid methods

\section{Results}
% 2,000 words

% - **Overall Performance Metrics**

%   - Classification accuracy, precision, recall, F1-score
%   - ROC curves and AUC analysis
%   - Confusion matrices and interpretation
%   - k fold Cross-validation results and stability analysis
%   - Statistical significance testing
%   - Benchmarking against literature results
%   - Bayesian analysis on representative populations

% - **Architectural Comparisons**

%   - 3D ResNet vs. Mixed Convolution vs 2plus1D performance
%   - Impact of layer freezing strategies
%   - Parameter efficiency analysis

% - **Preprocessing Impact Analysis**

%   - Effect of different preprocessing steps
%   - Importance of crop-and-reshape vs. simple interpolation
%   - Impact of skull stripping quality

% - **Augmentation Effectiveness**

%   - Comparative analysis of different augmentation strategies
%   - Quantitative impact on model performance

% - **Training Dynamics**

%   - Learning curves analysis
%   - Convergence patterns
%   - Overfitting observations and mitigations

% - **Error Analysis**

%   - Patterns in misclassifications
%   - Subject-level vs. scan-level errors
%   - Potential confounding factors

% - **Visual Results**
%   - Key visualizations from Weights \& Biases
%   - Representative case studies
%   - Visualization of model attention/activation maps XAI


\section{Discussion}
% 2,000 words

% - **Interpretation of Results**

%   - Critical analysis of performance metrics
%   - Analysis of 77% accuracy in clinical context
%   - Comparison with human radiologist performance
%   - Significance relative to existing literature
%   - Analysis of false positives and false negatives

% - **Technical Insights**

%   - Effectiveness of transfer learning from video domain
%   - Value of 3D vs. 2D/3D hybrid approaches
%   - Computational efficiency considerations
%   - Memory constraints and their implications

% - **Model Interpretability** (if implemented)

%   - Insights from XAI analysis
%   - Visualization techniques for model attention/activation
%   - Correlation with known AD-affected regions
%   - Clinical relevance of identified features

% - **Clinical Implications**

%   - Potential utility as a diagnostic aid
%   - Integration into existing clinical workflows
%   - Complementary role to other diagnostic measures

% - **Technical Challenges and Solutions**

%   - Memory optimization strategies
%   - Training time challenges on consumer hardware
%   - Data preprocessing optimization
%   - Hardware limitations and workarounds
%   - Data leakage prevention and subject isolation

% - **Limitations**
%   - Dataset representativeness and potential biases
%   - Focus on binary classification (AD vs. CN)
%   - Technical constraints (resolution, model capacity)
%   - Hardware constraints impact on model selection
%   - Need for prospective validation
%   - Generalizability concerns


\section{Conclusions}
% 1,000 words

% - **Summary of Contributions**

%   - Key findings on transfer learning effectiveness
%   - Revisiting research objectives
%   - Technical innovations in preprocessing pipeline
%   - Methodological contributions (subject-level validation)

% - **Future Directions**

%   - Architectural improvements
%   - Multi-class classification (including MCI)
%   - Multimodal approaches
%   - Longitudinal analysis potential
%   - Clinical validation pathway
%   - Integration of additional MRI sequences
%   - Consideration of larger/deeper architectures with more compute

% - **Broader Impact**
%   - Implications for AI in neuroimaging
%   - Potential for improving AD diagnosis workflow
%   - Ethical considerations and responsible deployment




\bibliographystyle{IEEEtran}
\bibliography{references} 

\appendix

% - **Detailed Implementation Specifics**

%   - Code snippets for key components
%   - Hyperparameter configurations
%   - Detailed architectures

% - **Additional Visualizations**

%   - Extended results tables
%   - Additional performance metrics
%   - Sample preprocessing visualizations
%   - Extended XAI visualizations

% - **Computational Resources Analysis**
%   - Detailed training times
%   - Memory usage patterns
%   - Optimization attempts



\end{document}



