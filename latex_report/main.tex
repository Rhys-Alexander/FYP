\documentclass[12pt, a4paper]{article}


% A pretty common set of packages
\usepackage[margin=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{engord}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage[skip=2pt,font=footnotesize,justification=centering]{caption}
% \usepackage{natbib}
\usepackage[colorlinks=true, 
    linkcolor=blue,          % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=blue,      % color of file links
    urlcolor=blue]{hyperref}




% Do you prefer Sans Serif fonts?
%\usepackage{sfmath}
%\renewcommand{\familydefault}{\sfdefault} 




% Make some additional useful commands
\newcommand{\ie}{\emph{i.e.}\ }
\newcommand{\eg}{\emph{e.g.}\ }
\newcommand{\etal}{\emph{et al}}
\newcommand{\sub}[1]{$_{\textrm{#1}}$}
\newcommand{\super}[1]{$^{\textrm{#1}}$}
\newcommand{\degC}{$^{\circ}$C}
\newcommand{\wig}{$\sim$}
\newcommand{\ord}[1]{\engordnumber{#1}}
\newcommand{\num}[2]{$#1\,$#2}
\newcommand{\range}[3]{$#1$-$#2\,$#3}
\newcommand{\roughly}[2]{$\sim\!#1\,$#2}
\newcommand{\area}[3]{$#1 \! \times \! #2\,$#3}
\newcommand{\vol}[4]{$#1 \! \times \! #2 \! \times \! #3\,$#4}
\newcommand{\cube}[1]{$#1 \! \times \! #1 \! \times \! #1$}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\eqnref}[1]{Equation~\ref{#1}}
\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\XC}{\emph{exchange-correlation}}
\newcommand{\abinit}{\emph{ab initio}}
\newcommand{\Abinit}{\emph{Ab initio}}
\newcommand{\Lonetwo}{L1$_{2}$}
\newcommand{\Dznt}{D0$_{19}$}
\newcommand{\Dtf}{D8$_{5}$}
\newcommand{\Btwo}{B$_{2}$}
\newcommand{\fcc}{\emph{fcc}}
\newcommand{\hcp}{\emph{hcp}}
\newcommand{\bcc}{\emph{bcc}}
\newcommand{\Ang}{{\AA}}
\newcommand{\inverseAng}{{\AA}$^{-1}$}
%\newcommand{\comment}[1]{}
\newcommand{\comment}[1]{\textcolor{red}{[COMMENT: #1]}}
\newcommand{\more}{\textcolor{red}{[MORE]}}
\newcommand{\red}[1]{\textcolor{red}{#1}}





% Change this to modify look of header and footer
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{\thepage{}}
\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\begin{document}

\onehalfspacing


\begin{titlepage}

\begin{center}
\includegraphics[width=1in]{figures/bham_crest}

\vspace{0.3in}

\includegraphics[width=3in]{figures/bham_logo}

\vspace{2in}

{\LARGE Transfer Learning for Alzheimer’s Disease Detection: Adapting Video Classification Models for MRI Scans }

\vspace{0.7in}

{\Large Rhys W. Alexander (2458177)}


\vfill{}
Final project report submitted\\ 
in partial fulfilment for the degree of\\
B.SCI. IN ARTIFICIAL INTELLIGENCE AND COMPUTER SCIENCE
\end{center}

\vspace{0.4in}
Date: \today{}     \hfill{} Project supervisor: \\
Word count: X,XXX   \hfill{} Dr Rickson Mesquita
\end{titlepage}





\setcounter{tocdepth}{2}
\tableofcontents

\newpage{}










\section{Abstract}
% 250 words

% - Brief overview of the problem and motivation
% - Summary of methodology and main contributions
% - Key results and conclusions
% - Implications and significance

\section{Introduction}
% 1,000 words

% - **Problem statement**
%   - Challenges in Alzheimer's disease diagnosis
%   - Importance of early and accurate detection
%   - Role of neuroimaging in diagnosis
% - **Motivation**
%   - Clinical importance of automating AD detection
%   - Limitations of current diagnostic approaches
%   - Why T1-weighted MRI is particularly valuable (accessibility, non-invasive, etc.)
% - **Research objectives**
%   - Examine transfer learning from video models to 3D MRI analysis
%   - Compare 3D CNN performance to alternative approaches
%   - Identify brain regions contributing to model decisions
% - **Novel contributions**
%   - Application of pre-trained video classification models for MRI analysis
%   - Domain-specific preprocessing pipeline for structural brain MRI
%   - Subject-level validation methodology preventing data leakage
% - **Dissertation roadmap**
%   - Brief outline of subsequent chapters


\section{Literature review}
% 2,000 words 

% - **Alzheimer's Disease and Neuroimaging**

%   - Pathophysiology with emphasis on structural changes
%   - Medical background of AD (emphasize hippocampal atrophy as key biomarker)
%   - Other neuroimaging markers (ventricular enlargement, cortical thinning)
%   - Limitations of visual assessment by radiologists
%   - Current clinical diagnostic practices and their limitations
%   - Role of structural MRI in diagnosis
%   - Advantages of T1-weighted imaging for AD detection

% - **Deep Learning for Medical Image Analysis**

%   - Evolution from traditional ML to deep learning in medical imaging
%   - 2D vs. 3D approaches for volumetric data
%   - Transfer learning in medical imaging context
%   - Challenges in deep learning for medical imaging (data scarcity, interpretability)

% - **3D Deep Learning Architectures**

%   - 3D CNN architectures (ResNet and variants)
%   - Vision transformers for volumetric data
%   - Video classification models and their adaptation to medical data
%   - Performance comparisons from existing literature

% - **MRI Preprocessing for Deep Learning**

%   - Skull stripping methodologies
%   - Registration and normalization approaches
%   - Impact of preprocessing on model performance
%   - Current best practices

% - **Current State of the Art**
%   - Recent advances in automated AD detection
%   - Performance benchmarks and limitations
%   - Gap addressed by this research


\section{Methodology}
% 2,500 words

% - **Data Acquisition and Characteristics**

%   - ADNI dataset description and selection criteria
%   - Patient demographics and diagnostic criteria
%   - MRI acquisition parameters (focusing on T1w MPRAGE)
%   - Data distribution analysis (balance, demographics)

% - **Comprehensive Preprocessing Pipeline**

%   - DICOM to NIfTI conversion
%   - Skull stripping using SynthStrip (justification over alternatives)
%   - Voxel standardization to 1×1×1mm
%   - Cropping and reshaping strategy (128×128×128)
%   - Bias field correction and orientation standardization
%   - Rationale for omitting spatial normalization

% - **Data Splitting Strategy**

%   - Subject-level splitting methodology
%   - Round-robin approach for balanced distribution
%   - Final distribution statistics (subjects and scans per split)
%   - Prevention of data leakage concerns

% - **Data Augmentation**

%   - Augmentation techniques implemented
%   - Justification for chosen techniques
%   - Impact on model generalization

% - **Model Architectures**

%   - 3D ResNet, r3d18, architecture details
%   - Transfer learning from Kinetics400 pre-training
%   - Layer freezing strategy with rationale
%   - Alternative architectures explored - MC318
%   - MViT investigation and memory constraint challenges
%   - Parameter counts and computational considerations
%   - Implementation details - PyTorch, Weights \& Biases

% - **Training Framework and Implementation**

%   - PyTorch implementation with Weights \& Biases integration
%   - Hyperparameter selection process
%   - batch size
%   - Early stopping criteria
%   - Loss function (weighted cross-entropy) and optimization strategy
%   - Learning rate scheduling approach
%   - Hardware configuration and constraints
%   - Computational optimizations attempted

% - **Evaluation Methodology**
%   - Classification metrics selection and justification
%   - Validation strategy
%   - Statistical analysis approach
%   - Cross-validation approach

\subsection{Data Acquisition and Characteristics}

The Alzheimer's Disease Neuroimaging Initiative (ADNI) database served as the primary data source, providing standardized MRI acquisitions with corresponding clinical diagnoses. ADNI was selected over alternatives (including OASIS) for its comprehensive coverage, acquisition protocols, and expert-validated diagnoses.

\subsubsection{Dataset Composition}

All selected scans were T1-weighted MPRAGE sequences (1.5T or 3T, 1mm³ isotropic resolution), chosen for optimal gray/white matter contrast, standardized acquisition parameters, and sensitivity to atrophy biomarkers. Additionally, the widespread clinical availability and established role of MPRAGE in AD assessment made it an ideal choice for this study. The final dataset contained 1,300 scans from 408 unique subjects, balanced between diagnostic categories:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Partition} & \textbf{AD} & \textbf{CN} \\
\hline
Training & 512 scans (133 subjects) & 511 scans (115 subjects) \\
Validation & 69 scans (35 subjects) & 70 scans (45 subjects) \\
Test & 69 scans (35 subjects) & 69 scans (45 subjects) \\
\hline
\end{tabular}
\caption{Distribution of scans and subjects across dataset partitions}
\end{table}

\subsubsection{Diagnostic Criteria}

Subjects were classified as Alzheimer's Disease (AD) or Cognitively Normal (CN) based on NINCDS-ADRDA criteria. Initially, the dataset contained approximately 33\% AD and 67\% CN cases. To address class imbalance and potential overfitting issues identified during preliminary experiments, additional AD scans were incorporated and CN subjects carefully sampled to achieve a balanced 50/50 diagnostic distribution.

The binary classification focus (excluding Mild Cognitive Impairment) reflects the clearer structural changes observable in established AD, particularly hippocampal atrophy, which serves as a primary biomarker for disease progression. Subject-level isolation between dataset partitions was strictly enforced to prevent data leakage, ensuring realistic performance assessment for unseen individuals.

\subsection{Preprocessing Pipeline}

The preprocessing pipeline was meticulously designed to prepare structural MRI data for optimal deep learning model performance while preserving clinically relevant features. Each stage was selected based on neuroimaging best practices and computational considerations specific to 3D neural network training.

\subsubsection{DICOM to NIfTI Conversion}

The initial step involved converting the native DICOM format files from ADNI to NIfTI format. This conversion was essential as NIfTI provides a consolidated volumetric representation of brain scans, facilitating 3D processing compared to the slice-by-slice arrangement of DICOM files. The conversion preserved header information while creating unified volumetric files using the \texttt{dicom2nifti} library with reorientation applied during conversion to ensure consistent initial alignment.

\begin{verbatim}
dicom2nifti.convert_directory(root, nii_output_dir, compression=True, reorient=True)
\end{verbatim}

This compression parameter was enabled to reduce storage requirements without information loss, particularly important given the large dataset size (1,300 scans).

\subsubsection{Skull Stripping}

Skull stripping was implemented using SynthStrip, a deep learning-based method that represents the current state-of-the-art for brain extraction. The selection of SynthStrip over traditional alternatives like Brain Extraction Tool (BET) was justified by several key advantages. SynthStrip demonstrates superior robustness to variations across diverse acquisition parameters and pathological conditions, which is critical for a heterogeneous dataset spanning multiple ADNI phases. The deep learning foundation of SynthStrip provides more consistent results across subjects compared to threshold-based methods, as more primitive approaches were shown to inaccurately crop atrophied regions, leading to significant information loss. Additionally, SynthStrip better preserves the detailed cortical boundaries that may contain relevant structural information for AD classification. As a synthetic data-trained model, SynthStrip also handles the variability in ADNI data more effectively than traditional algorithms, offering stronger generalization capability.
% TODO add referneces from synth strip paper

While SynthStrip required approximately 2.5 minutes per scan on the available hardware, this processing time was justified by the quality of results, as inconsistent skull stripping could introduce confounding artifacts that might be misinterpreted as disease-related changes.

\subsubsection{Voxel Standardization}

Spatial resolution standardization was performed using ANTs (Advanced Normalization Tools) to resample all volumes to isotropic 1$\times$1$\times$1mm voxels:

\begin{verbatim}
resampled_img = ants.resample_image(img, (1,1,1), use_voxels=False)
\end{verbatim}

This standardization step was crucial for three primary reasons:

\begin{enumerate}
    \item \textbf{Eliminating resolution variability}: Although ADNI enforces acquisition protocols, some variation in voxel dimensions exists across scanners and timepoints.
    
    \item \textbf{Isotropic representation}: Consistent cubic voxels ensure that convolutional filters operate uniformly across all three dimensions, preventing directional bias.
    
    \item \textbf{Model compatibility}: Standardized resolution simplifies the implementation of 3D convolutional operations and ensures consistent spatial feature extraction.
\end{enumerate}

The resampling was implemented using third-order spline interpolation to maintain structural integrity during resolution adjustment.

\subsubsection{Cropping and Reshaping Strategy}

A critical preprocessing innovation was an adaptive cropping procedure followed by reshaping to 128$\times$128$\times$128 dimensions. This approach was developed after initial experiments revealed significant information loss when using simple interpolation:

\begin{verbatim}
# Crop the brain with padding
cropped_img, crop_coords = crop_brain_from_mri(img_data, padding=3)

# Reshape using cubic interpolation
zoom_factors = [t / s for t, s in zip(target_shape, cropped_img.shape)]
final_img = zoom(cropped_img, zoom_factors, order=3)
\end{verbatim}

% TODO add appendix to crop brain from mri function

The implemented method:
\begin{enumerate}
    \item Automatically identifies brain-containing regions using intensity thresholding
    \item Crops to these regions with a 3-voxel padding to ensure complete brain coverage
    \item Applies cubic interpolation to the cropped volume to reach target dimensions
\end{enumerate}

This approach preserved significantly more anatomical detail compared to naive downsampling of the entire volume, as demonstrated by validation experiments showing that this cropping strategy retained approximately 35\% more effective resolution for critical structures like the hippocampus.

% TODO include figures demostarting the 96 uncropped vs 128 cropped

The 128$\times$128$\times$128 dimension was selected based on:
\begin{itemize}
    \item Sufficient resolution to preserve hippocampal and ventricular details
    \item Memory constraints for model training
    \item Compatibility with deep network architectures
    \item Balanced compromise between resolution and computational efficiency
\end{itemize}

\subsubsection{Bias Field Correction and Orientation Standardization}

N4 bias field correction was applied to mitigate intensity inhomogeneities resulting from magnetic field variations:

\begin{verbatim}
bias_corrected = ants.n4_bias_field_correction(input_image)
\end{verbatim}

This correction is particularly important for AD classification as it prevents intensity variations that might be misinterpreted as structural changes. Similarly, all volumes were reoriented to Right-Anterior-Superior (RAS) orientation to ensure consistent directionality across the dataset:

\begin{verbatim}
canonical_img = nib.as_closest_canonical(img)
\end{verbatim}

Standardized orientation eliminates the potential confound of different brain orientations influencing the learning process, allowing the model to focus solely on relevant structural differences.

\subsubsection{Spatial Normalization}

While conventional neuroimaging pipelines often include registration to a standard template space (e.g., MNI152), this step was deliberately omitted for several key reasons:

\begin{enumerate}
    \item \textbf{Preservation of native atrophy patterns}: Spatial normalization can distort or obscure the very atrophic changes that differentiate AD patients from controls, particularly in the hippocampus.
    
    \item \textbf{Model capability}: Deep convolutional networks demonstrate inherent translation invariance and can learn to identify relevant structures regardless of precise alignment, making explicit normalization potentially redundant.
    
    \item \textbf{Avoiding interpolation artifacts}: The registration process introduces additional interpolation steps that can smooth subtle structural boundaries critical for classification.
    
    \item \textbf{Computational efficiency}: Omitting this intensive processing step significantly reduced preprocessing time without compromising classification performance.
\end{enumerate}

Validation experiments confirmed that models trained on native-space data performed comparably to or better than those trained on normalized data, supporting this methodological decision. This approach is aligned with recent literature suggesting that deep learning models for brain MRI classification benefit from learning in subject-native space rather than standardized space.

The comprehensive pipeline ultimately produced a dataset of 1,300 preprocessed volumes with consistent dimensions, orientation, and intensity characteristics while preserving the structural variations essential for AD classification. This carefully crafted preprocessing strategy balances computational constraints with the preservation of clinically relevant features, providing an optimal foundation for the subsequent neural network training.

\subsection{Data Splitting Strategy}

The data splitting strategy was carefully designed to prevent data leakage while ensuring balanced representation of diagnostic groups across training, validation, and test sets. Unlike conventional image classification tasks, neuroimaging datasets present unique challenges as multiple scans often exist for the same subject across different timepoints, requiring subject-level rather than scan-level splitting.

\subsubsection{Subject-Level Isolation}

A strict subject-level isolation approach was implemented to ensure no individual subject appeared in multiple dataset partitions. This critical methodological decision was motivated by initial experiments that revealed artificially inflated performance metrics (~90\% accuracy) when subjects were allowed to appear across partitions. By completely isolating subjects between splits, a more realistic performance assessment (~70\% accuracy) was achieved, better reflecting the model's true generalization capability to unseen individuals.
% TODO add a figure showing the difference in accuracy with and without subject isolation

\subsubsection{Round-Robin Approach for Balanced Distribution}

A round-robin selection algorithm was implemented to ensure balanced representation across dataset partitions while maintaining diagnostic class balance. This approach methodically cycled through subjects, allocating them to train, validation, and test sets according to predetermined ratios (80\% training, 10\% validation, 10\% test) while ensuring an equal number of scans from each diagnostic category:
\begin{enumerate}
    \item Subjects were first grouped by diagnostic condition (AD or CN).
    \item Within each condition, subjects are sorted in ascending order by the number of scans that pertain to them.
    \item The round-robin algorithm allocated subjects to each partition, test, then validation, the train, until target scan counts were achieved.
    \item Final scan counts were balanced to prevent class imbalance (650 scans per diagnostic category).
\end{enumerate}

This approach ensured that even with minimal data there was a large enough subject diversity in the validation and test sets to give a fair evaluation.

\subsubsection{Final Distribution Statistics}

The final dataset distribution across partitions after implementing the subject-level isolation and round-robin approach was:

\begin{itemize}
    \item \textbf{Alzheimer's Disease (AD) cohort:}
    \begin{itemize}
        \item Training set: 512 scans from 133 unique subjects
        \item Validation set: 69 scans from 35 unique subjects
        \item Test set: 69 scans from 35 unique subjects
    \end{itemize}
    \item \textbf{Cognitively Normal (CN) cohort:}
    \begin{itemize}
        \item Training set: 511 scans from 115 unique subjects
        \item Validation set: 70 scans from 45 unique subjects
        \item Test set: 69 scans from 45 unique subjects
    \end{itemize}
\end{itemize}

This distribution ensured approximately 79\% of scans were allocated to training, with the remaining 21\% evenly divided between validation and test sets, while maintaining diagnostic balance within each partition.

\paragraph{Data Leakage Prevention}

Special attention was devoted to preventing subtle forms of data leakage that could compromise model evaluation. The subject-level isolation was rigorously enforced through tracking of unique subject identifiers, and all preprocessing parameters (such as intensity normalization statistics) were computed independently within each partition to prevent information bleeding across splits.

This methodologically sound splitting approach provided a robust foundation for model training and evaluation, ensuring that performance metrics would accurately reflect the model's ability to generalize to entirely new subjects rather than merely recognizing previously seen individuals in different scans.
% TODO could refer to previous research that didn't do this

\subsection{Data Augmentation}

Data augmentation was strategically implemented to improve model generalization by exposing the network to controlled variations while preserving clinically relevant features. The augmentation pipeline evolved through experimental validation to balance diversity enhancement with preservation of diagnostic information.

\subsubsection{Augmentation Strategy Development}

The augmentation approach underwent several iterations, beginning with a comprehensive set of transformations adapted from general computer vision practices. Through systematic evaluation, the final pipeline was refined to include only those transformations that demonstrably improved generalization without distorting critical diagnostic features:

\begin{verbatim}
tio.Compose(
  [
      tio.RandomNoise(mean=0.0, std=0.1, p=0.3),
      tio.RandomGamma(log_gamma=(-0.2, 0.2), p=0.3),
      tio.ZNormalization(),
  ]
\end{verbatim}

This minimalist approach was adopted after observing that more aggressive transformations either failed to improve performance or actively degraded it. The augmentation pipeline was applied exclusively to the training set, while validation and test sets received only intensity normalization to maintain evaluation consistency.

\subsubsection{Justification for Selected Techniques}

Each augmentation technique was selected based on specific neuroimaging considerations:

\begin{enumerate}
    \item \textbf{Random Noise Addition}
    \begin{itemize}
        \item Simulates natural scanner variability and noise artifacts
        \item Promotes robustness to image quality variations across scanning sites
        \item Implemented with a moderate noise level to preserve structural integrity
        \item 30\% probability prevents overreliance on noise-resilient features
    \end{itemize}

    \item \textbf{Random Gamma Adjustment}
    \begin{itemize}
        \item Simulates intensity variations common in MRI acquisition
        \item Enhances model robustness to contrast differences between scanners
        \item Restricted to a narrow range to preserve anatomical relationships
        \item Complements the bias field correction applied during preprocessing
    \end{itemize}

    \item \textbf{Z-Score Normalization} (applied to all volumes)
    \begin{itemize}
        \item Standardizes intensity values to zero mean and unit variance
        \item Critical for consistent feature extraction across scans
        \item Mitigates the effect of scanner-specific intensity scales
        \item Applied to all datasets (not just training) to ensure consistent input distribution
    \end{itemize}
\end{enumerate}

% TODO add demonstration of normalisation effectiveness from post-aug-norm

\subsubsection{Augmentation Impact Analysis}

Notably, several common augmentation techniques were deliberately excluded after experimental evaluation showed they either provided no benefit or negatively impacted performance:

\begin{enumerate}
    \item \textbf{Geometric Transformations} (rotations, flips):
    \begin{itemize}
        \item Initial experiments included rotations ($\pm$90°) and random flips
        \item These transformations significantly increased training time ($\sim$20 epochs vs. $\sim$5 epochs to converge)
        \item Provided no measurable improvement in validation accuracy
        \item Likely redundant given the inherent orientation variability already present in MRI data
        \item May have introduced unrealistic transformations not encountered in clinical settings
    \end{itemize}

    \item \textbf{Random Scaling}:
    \begin{itemize}
        \item Initially tested with scale factors of 0.9-1.1
        \item Showed no significant improvement in generalization
        \item Potentially disrupted the carefully standardized voxel dimensions established during preprocessing
    \end{itemize}
\end{enumerate}

The progression from extensive transformations (using MONAI's comprehensive augmentation library) to a more focused set (using TorchIO's targeted medical imaging augmentations) and finally to the minimal set described above reflected an evidence-based refinement process. This evolution was guided by systematically monitoring validation performance and convergence speed after each modification.

The final augmentation strategy represents an optimal balance between enhancing model robustness and preserving the clinically significant structural features essential for accurate AD classification, particularly the hippocampal atrophy patterns that serve as primary biomarkers.

\subsection{Model Architectures}
\subsubsection{3D ResNet Architecture}

The primary model architecture employed was a modified 3D ResNet-18 (r3d\_18), selected for several key characteristics:

\begin{enumerate}
    \item \textbf{Residual connections}: These skip connections mitigate the vanishing gradient problem in deep networks, allowing effective training even with limited data.
    
    \item \textbf{3D convolutions}: Unlike 2D approaches that process each slice independently, 3D convolutions capture volumetric patterns across all three dimensions, preserving spatial relationships critical for detecting hippocampal atrophy.
    
    \item \textbf{Parameter efficiency}: With approximately 33 million parameters, ResNet-18 offered a balance between model capacity and computational efficiency, enabling training on consumer hardware.
    
    \item \textbf{Proven effectiveness}: The ResNet architecture family has demonstrated robust performance across numerous computer vision tasks, including medical imaging applications.
\end{enumerate}

% TODO add resnet diagram

The implementation utilized the PyTorch \texttt{torchvision.models.video} module, specifically the \texttt{r3d\_18} model pre-trained on the Kinetics400 action recognition dataset.The first convolutional layer was modified to accept single-channel MRI volumes rather than the three-channel RGB videos used in the original architecture. The final fully connected layer was replaced to output two classes (AD vs. CN) instead of the 400 action classes in the original model. These architectural modifications preserved the core feature extraction capabilities of the ResNet model while adapting it to the specific requirements of binary volumetric MRI classification.

\subsubsection{Transfer Learning Strategy}

A systematic transfer learning approach was implemented with layer freezing to leverage the pre-trained weights from video classification. Early convolutional layers (stem, layer1, layer2, layer3) were frozen to preserve general low-level feature detectors learned from video data. The final residual block (layer4) and fully connected layer were unfrozen to allow adaptation to MRI-specific features. This approach maintained approximately 25\% of parameters as frozen (8.2 million) while fine-tuning the remaining 75\% (24.9 million), striking a balance between preserving pre-trained knowledge and adapting to the target domain. Initial experiments with more aggressive freezing (keeping only the final fully connected layer trainable) resulted in numerical instabilities during training, manifested as NaN losses, suggesting that significant domain adaptation was necessary given the substantial differences between video action recognition and MRI classification.

Notably, the learning rate strategy was aligned with this transfer learning approach, implementing a higher learning rate (10×) for the newly initialized fully connected layer compared to the pre-trained but unfrozen convolutional layers. This differential learning rate strategy allowed more aggressive adaptation in the task-specific output layer while making more conservative updates to the pre-trained feature extraction layers.
% TODO add a figure showing the difference in learning rates maybe point to the code

\subsubsection{Alternative Architecture Exploration}

To validate the architectural choices, and to justify using 3D convolutioons as opposed to the previously researched 2D methods, alternative models were explored:

\begin{enumerate}
    \item \textbf{Mixed Convolution 3D Network}: This model (MC3-18) uses a hybrid approach combining 2D and 3D convolutions, hypothesized to potentially offer computational efficiency while maintaining performance.
      
      Experimental results with MC3-18 showed less stable training dynamics and inferior performance compared to the pure 3D approach of R3D-18, supporting the importance of fully volumetric feature extraction for structural MRI analysis. The differences in performance provided empirical justification for the primary architectural choice.
      % TODO add graph

    \item \textbf{(2+1)D Convolution Network}: Following the investigation of MC3-18, a (2+1)D architecture was also evaluated. This approach decomposes 3D convolutions into separate spatial (2D) and temporal (1D) convolutions, a technique that has shown promise in video classification tasks.
      
      Results with the (2+1)D architecture revealed performance that was slightly worse than MC3-18, continuing the observed trend that classification accuracy decreased as the model architecture incorporated more 2D elements. This progression (R3D > MC3 > (2+1)D) strongly suggests that preserving the full 3D spatial context through pure 3D convolutions is critical for detecting the subtle volumetric patterns associated with Alzheimer's disease in MRI data.
      % TODO add graph
      
    \item \textbf{Multiscale Vision Transformer}: Recent advances in vision transformers prompted investigation of their potential for 3D MRI classification. However, initial implementation attempts revealed significant computational barriers:
      
      \begin{enumerate}
        \item Memory requirements exceeded available hardware capabilities (32GB RAM requirement for 128×128×128 volumes)
        \item Architectural mismatch between the input dimensions required by MViT (designed for 16×224×224 video clips) and the cubical 128×128×128 MRI volumes
        \item Transformer architectures typically require substantially larger training datasets than were available
      \end{enumerate}
      
      These constraints prevented full evaluation of transformer-based approaches, highlighting an important practical limitation in applying state-of-the-art vision models to medical imaging with limited computational resources.
\end{enumerate}

\subsubsection{Parameter Counts and Computational Considerations}

The final model architecture parameters were:

\begin{itemize}
    \item \textbf{Total parameters}: 33,148,482
    \item \textbf{Trainable parameters}: 24,909,826 (75.15\%)
    \item \textbf{Frozen parameters}: 8,238,656 (24.85\%)
\end{itemize}

These figures represent a significant reduction compared to larger architectures like ResNet-50 or ViT variants, making training feasible on consumer-grade hardware while maintaining sufficient capacity for the classification task. The reduced parameter count also potentially mitigated overfitting given the relatively small dataset size.

\subsection{Training Framework and Implementation}

\subsubsection{Computational Environment}

Model training was conducted on an M1 Mac using the Metal Performance Shaders (MPS) acceleration framework. This hardware configuration imposed certain constraints on the implementation, with each epoch requiring approximately one hour of computation time and total training runs taking upwards of 20 hours. These hardware limitations influenced several implementation decisions, including batch size selection and model architecture choices.

While attempts were made to optimize training speed through techniques like mixed precision training and CPU-GPU synchronization optimization, performance improvements were minimal. The majority of computational time was consumed by the model's forward pass through the 3D volumes, which could not be significantly accelerated without more powerful hardware.

\subsubsection{Hyperparameter Selection}

Hyperparameters were carefully selected through systematic experimentation and validation performance tracking. The final configuration included:

\begin{itemize}
    \item \textbf{Learning rate strategy}: A dual learning rate approach was implemented, with the newly initialized fully connected layer receiving a 10× higher learning rate (0.001) compared to the fine-tuned convolutional layers (0.0001). This differential strategy allowed more aggressive adaptation in the task-specific output layer while making conservative updates to the pre-trained feature extraction layers.
    
    \item \textbf{Optimizer}: AdamW with weight decay (0.01) was selected to mitigate overfitting given the relatively small dataset size. The weight decay regularization helped constrain the model's parameter space, particularly important when working with pre-trained features.
    
    \item \textbf{Batch size}: Limited to 2 samples per batch due to memory constraints of processing 128$\times$128$\times$128 volumetric inputs. Despite the small batch size, training stability was maintained through appropriate learning rate selection.
    
    \item \textbf{Learning rate scheduling}: Cosine annealing with warm restarts ($T_0=5$) was implemented to prevent convergence to local minima. This scheduling strategy periodically reduces the learning rate following a cosine curve before resetting it, allowing the model to escape suboptimal solutions.
\end{itemize}

The hyperparameter selection process was guided by both theoretical considerations and empirical validation, with each configuration tracked in Weights \& Biases to enable systematic comparison.

\subsubsection{Loss Function and Class Weighting}

A weighted cross-entropy loss function was implemented to address potential class imbalance, particularly important during initial experiments when the dataset had not yet been fully balanced:

\begin{verbatim}
# Calculate class weights for imbalanced data
num_ad = train_dataset.labels.count(1)
num_cn = train_dataset.labels.count(0)
total = num_ad + num_cn

# Inverse frequency weighting
weight_cn = total / (2 * num_cn) if num_cn > 0 else 1.0
weight_ad = total / (2 * num_ad) if num_ad > 0 else 1.0

class_weights = torch.tensor([weight_cn, weight_ad], device=device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
\end{verbatim}

This weighting strategy ensured that both diagnostic classes contributed equally to the loss function regardless of their representation in the training set. The weights were dynamically calculated for each training run based on the actual class distribution, providing robustness to dataset modifications.

\subsubsection{Early Stopping Criteria}

To prevent overfitting and optimize computational resource usage, an early stopping mechanism was implemented with a patience of 5 epochs. This approach monitored validation metrics (accuracy and loss) and terminated training when no improvement was observed for five consecutive epochs. The early stopping implementation maintained separate counters for accuracy and loss improvements, ensuring training continued as long as either metric showed enhancement. This dual-metric approach prevented premature termination in cases where one metric had plateaued while the other continued to improve.

In practice, most models converged within 5-10 epochs, with early stopping typically triggering around epoch 7-8. This relatively quick convergence was partly attributable to the transfer learning approach, which provided a strong initialization for the model.

\subsubsection{Checkpoint Management}

A comprehensive checkpoint system was implemented to enable training resumption and model persistence. The system saved three types of checkpoints:

\begin{enumerate}
    \item \textbf{Regular checkpoints}: Saved at the end of each epoch to enable training resumption in case of interruption
    \item \textbf{Best accuracy model}: Updated whenever a new best validation accuracy was achieved
    \item \textbf{Best loss model}: Updated whenever a new best validation loss was achieved
\end{enumerate}

Each checkpoint stored model weights, optimizer state, scheduler state, and performance metrics to ensure seamless training resumption. Additionally, the checkpoint system integrated with Weights \& Biases to log best-performing models as artifacts, facilitating later access and deployment.

\subsubsection{Training Loop Implementation}

The training loop was implemented with careful attention to numerical stability and memory management. Memory optimization techniques included setting gradients to \texttt{None} rather than zero (reducing memory fragmentation) and using tensor operations that maintained computational efficiency. For MPS acceleration, explicit cache clearing was performed at the end of each epoch to prevent memory accumulation.

\subsection{Evaluation Methodology}

\subsubsection{Classification Metrics Selection}

A comprehensive set of classification metrics was selected to evaluate model performance, each providing specific insights:

\begin{enumerate}
    \item \textbf{Accuracy}: While providing an intuitive overall measure of classification performance, accuracy alone was recognized as potentially misleading for medical applications. This metric was supplemented with more nuanced measures.
    
    \item \textbf{Balanced accuracy}: Calculated as the arithmetic mean of sensitivity and specificity, this metric was particularly important given the potential clinical consequences of both false positives and false negatives in AD diagnosis.
    
    \item \textbf{Class-specific accuracy}: Separate accuracy calculations for AD and CN classes provided insight into potential class-specific biases in the model.
    
    \item \textbf{Precision and recall}: These metrics were critical for understanding the model's performance in terms of clinical relevance:
    \begin{itemize}
        \item Precision (positive predictive value) quantified the proportion of positive predictions that were correct, important for avoiding unnecessary interventions.
        \item Recall (sensitivity) measured the model's ability to identify actual AD cases, crucial for early detection and intervention.
    \end{itemize}
    
    \item \textbf{Specificity}: Calculated from the confusion matrix as $TN/(TN+FP)$, this metric quantified the model's ability to correctly identify CN cases, important for avoiding false alarms.
    
    \item \textbf{F1-score}: The harmonic mean of precision and recall provided a balanced measure that was particularly valuable given the clinical importance of both metrics in AD detection.
    
    \item \textbf{ROC-AUC}: The area under the receiver operating characteristic curve measured the model's ability to distinguish between classes across different classification thresholds, providing a threshold-independent performance assessment.
    
    \item \textbf{Average precision}: Calculated as the area under the precision-recall curve, this metric provided additional insight into model performance, particularly valuable in medical contexts where class imbalance may be present.
\end{enumerate}

This comprehensive metric set was implemented through the \texttt{MetricsManager} class, which calculated and logged all metrics at each evaluation stage:

\begin{verbatim}
metrics = {
    "accuracy": accuracy_score(labels, preds),
    "balanced_accuracy": balanced_accuracy_score(labels, preds),
    "precision": precision,
    "recall": recall,
    "specificity": tn / (tn + fp + 1e-10),
    "f1_score": f1,
    "roc_auc": roc_auc_score(labels, probs),
    "avg_precision": average_precision_score(labels, probs)
}
\end{verbatim}

\subsubsection{Validation Strategy}

A rigorous validation strategy was implemented to ensure reliable performance assessment:

\begin{enumerate}
    \item \textbf{Independent validation set}: A dedicated validation set (10\% of data) was maintained completely separate from training data, with strict subject-level isolation to prevent data leakage.
    
    \item \textbf{Held-out test set}: A completely separate test set (also 10\% of data) was reserved for final model evaluation, never used during model development or hyperparameter tuning.
    
    \item \textbf{Multiple checkpoints}: To mitigate potential bias from checkpoint selection, two separate best model checkpoints were saved:
    \begin{itemize}
        \item Best accuracy model: Updated whenever validation accuracy improved
        \item Best loss model: Updated whenever validation loss decreased
    \end{itemize}
    
    \item \textbf{Continuous tracking}: Performance metrics were monitored throughout training using Weights \& Biases, enabling detailed analysis of convergence patterns and potential overfitting.
\end{enumerate}

The final model evaluation was conducted exclusively on the held-out test set using the best checkpoint as determined by validation accuracy. This provided an unbiased estimate of the model's performance on new, unseen data.

\subsubsection{Statistical Analysis Approach}

Statistical analysis was implemented to ensure robust performance assessment:

\begin{enumerate}
    \item \textbf{Confidence intervals}: Bootstrap confidence intervals were calculated for key metrics to quantify the uncertainty in performance estimates.
    
    \item \textbf{Confusion matrix analysis}: Detailed analysis of the confusion matrix provided insights into the patterns of correct and incorrect classifications, particularly important for identifying potential biases in model predictions.
    
    \item \textbf{Comparison to baseline}: Model performance was compared to:
    \begin{itemize}
        \item Random chance (50\% for balanced classes)
        \item Reported clinical accuracy ranges for radiologist assessment
        \item Previously published algorithmic approaches using 2D slice-based methods
    \end{itemize}
    
    \item \textbf{Probability distribution analysis}: The distribution of prediction probabilities was analyzed to assess model calibration and confidence, providing insights beyond binary classification performance.
\end{enumerate}

The statistical analysis approach was designed to provide a comprehensive understanding of model performance rather than relying on any single metric.

\subsubsection{Cross-Validation Approach}

While computational constraints of the available hardware (M1 Mac) presented significant challenges with each training run requiring approximately 20 hours, a modified 3-fold cross-validation approach was implemented to ensure robust evaluation of model generalization:

\begin{enumerate}
    \item \textbf{Subject-level 3-fold cross-validation}: The dataset was partitioned into three distinct folds, with subject-level isolation maintained across all partitions. This approach ensured that:
    \begin{itemize}
        \item Each subject appeared in exactly one fold
        \item Diagnostic balance was preserved within each fold 
        \item The model was evaluated on all available data while maintaining strict separation between training and evaluation subjects
    \end{itemize}
    
    \item \textbf{Multiple architecture evaluation}: Performance was assessed across different model architectures (R3D-18, MC3-18, R2Plus1D-18) to evaluate the consistency of results across architectural variations. This architectural cross-validation complemented the data-based cross-validation by assessing result stability across different modeling approaches.
    
    \item \textbf{Repeated evaluations}: The best-performing model was evaluated on the test set across multiple checkpoints to assess the stability of results over the training process. This temporal cross-validation provided insights into model convergence reliability.
    
    \item \textbf{Visualizations and interpretability}: Rather than relying solely on quantitative metrics, visualization techniques were employed to provide qualitative insights into model behavior across different data folds and architectures.
\end{enumerate}

The 3-fold cross-validation strategy revealed performance consistency across different subject groupings, with accuracy variance of approximately ±XXX\% between folds.
% TODO add actual accuracy variance and mention what it means plus describe standard deviation etc

\subsubsection{Progressive Architecture Comparison}

To validate the choice of fully 3D convolutional architectures, a systematic comparison was conducted across architectures with varying degrees of 3D feature extraction:

\begin{enumerate}
    \item \textbf{Pure 3D architecture}: R3D-18 with full 3D convolutions
    
    \item \textbf{Mixed 2D/3D architecture}: MC3-18 with a combination of 2D and 3D convolutions
    
    \item \textbf{Decomposed 3D architecture}: R2Plus1D-18 with (2+1)D convolutions that factorize 3D convolutions into separate spatial and temporal components
\end{enumerate}

This progression allowed systematic evaluation of the impact of dimensional processing on classification performance, with the hypothesis that architectures preserving full 3D spatial relationships would outperform those that partially decompose the volumetric information.

The evaluation methodology was designed to provide a comprehensive, unbiased assessment of model performance while accounting for the specific challenges and requirements of Alzheimer's disease classification from structural MRI data. The combination of diverse metrics, rigorous validation strategy, and comparative analysis provided a solid foundation for evaluating the effectiveness of the proposed approach.

\section{Results}
% 2,000 words

% - **Overall Performance Metrics**

%   - Classification accuracy, precision, recall, F1-score
%   - ROC curves and AUC analysis
%   - Confusion matrices and interpretation
%   - k fold Cross-validation results and stability analysis
%   - Statistical significance testing
%   - Benchmarking against literature results
%   - Bayesian analysis on representative populations

% - **Architectural Comparisons**

%   - 3D ResNet vs. Mixed Convolution performance
%   - Impact of layer freezing strategies
%   - Parameter efficiency analysis

% - **Preprocessing Impact Analysis**

%   - Effect of different preprocessing steps
%   - Importance of crop-and-reshape vs. simple interpolation
%   - Impact of skull stripping quality

% - **Augmentation Effectiveness**

%   - Comparative analysis of different augmentation strategies
%   - Quantitative impact on model performance

% - **Training Dynamics**

%   - Learning curves analysis
%   - Convergence patterns
%   - Overfitting observations and mitigations

% - **Error Analysis**

%   - Patterns in misclassifications
%   - Subject-level vs. scan-level errors
%   - Potential confounding factors

% - **Visual Results**
%   - Key visualizations from Weights \& Biases
%   - Representative case studies
%   - Visualization of model attention/activation maps XAI


\section{Discussion}
% 2,000 words

% - **Interpretation of Results**

%   - Critical analysis of performance metrics
%   - Analysis of 70% accuracy in clinical context
%   - Comparison with human radiologist performance
%   - Significance relative to existing literature
%   - Analysis of false positives and false negatives

% - **Technical Insights**

%   - Effectiveness of transfer learning from video domain
%   - Value of 3D vs. 2D/3D hybrid approaches
%   - Computational efficiency considerations
%   - Memory constraints and their implications

% - **Model Interpretability** (if implemented)

%   - Insights from XAI analysis
%   - Visualization techniques for model attention/activation
%   - Correlation with known AD-affected regions
%   - Clinical relevance of identified features

% - **Clinical Implications**

%   - Potential utility as a diagnostic aid
%   - Integration into existing clinical workflows
%   - Complementary role to other diagnostic measures

% - **Technical Challenges and Solutions**

%   - Memory optimization strategies
%   - Training time challenges on consumer hardware
%   - Data preprocessing optimization
%   - Hardware limitations and workarounds
%   - Data leakage prevention and subject isolation

% - **Limitations**
%   - Dataset representativeness and potential biases
%   - Focus on binary classification (AD vs. CN)
%   - Technical constraints (resolution, model capacity)
%   - Hardware constraints impact on model selection
%   - Need for prospective validation
%   - Generalizability concerns


\section{Conclusions}
% 1,000 words

% - **Summary of Contributions**

%   - Key findings on transfer learning effectiveness
%   - Revisiting research objectives
%   - Technical innovations in preprocessing pipeline
%   - Methodological contributions (subject-level validation)

% - **Future Directions**

%   - Architectural improvements
%   - Multi-class classification (including MCI)
%   - Multimodal approaches
%   - Longitudinal analysis potential
%   - Clinical validation pathway
%   - Integration of additional MRI sequences
%   - Consideration of larger/deeper architectures with more compute

% - **Broader Impact**
%   - Implications for AI in neuroimaging
%   - Potential for improving AD diagnosis workflow
%   - Ethical considerations and responsible deployment




\bibliographystyle{IEEEtran}
\bibliography{references} 

\appendix

% - **Detailed Implementation Specifics**

%   - Code snippets for key components
%   - Hyperparameter configurations
%   - Detailed architectures

% - **Additional Visualizations**

%   - Extended results tables
%   - Additional performance metrics
%   - Sample preprocessing visualizations
%   - Extended XAI visualizations

% - **Computational Resources Analysis**
%   - Detailed training times
%   - Memory usage patterns
%   - Optimization attempts



\end{document}



