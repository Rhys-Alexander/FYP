{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Define directory paths\n",
    "base_dir = \"./DATA\"\n",
    "data_dir = os.path.join(\n",
    "    base_dir, \"ADNI_CROPPED_128\"\n",
    ")  # Directory with train/val/test folders\n",
    "xml_dir = os.path.join(base_dir, \"ADNI_METADATA\")  # Directory with XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've a directory with 3 subdirs, train, val, and test. Each subdir has 2 subdirs, AD and CN. Each AD and CN dir has many nii.gz files in it, their name is an id like \"I12345\". I then have another dir with lots of xml files in it, each of them is named like such \"ADNI_013_S_0575_MPRAGE_S28210_I44926\" where the final part is the scan id \"I44926\" and the second third and fourth part is the subject id \"013_S_0575\". For all the scans in the first dir append the subject id corresponding to that scan as the new name of the scan. DO this in python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_subject_id(data_dir, xml_dir):\n",
    "    # Create mapping from scan ID to subject ID\n",
    "    scan_to_subject = {}\n",
    "\n",
    "    # Parse XML filenames to extract subject ID and scan ID\n",
    "    print(\"Creating scan-to-subject mapping...\")\n",
    "    for xml_file in os.listdir(xml_dir):\n",
    "        if xml_file.endswith(\".xml\"):\n",
    "            # From format like \"ADNI_013_S_0575_MPRAGE_S28210_I44926.xml\"\n",
    "            # Extract the 013_S_0575 (subject ID) and I44926 (scan ID)\n",
    "            parts = xml_file.split(\"_\")\n",
    "            if len(parts) >= 6 and parts[-1].startswith(\"I\"):\n",
    "                scan_id = parts[-1].split(\".\")[0]  # Extract scan ID (e.g., \"I12345\")\n",
    "                subject_id = \"_\".join(\n",
    "                    parts[1:4]\n",
    "                )  # Parts 2-4 form subject ID (013_S_0575)\n",
    "                scan_to_subject[scan_id] = subject_id\n",
    "\n",
    "    print(f\"Found {len(scan_to_subject)} scan-to-subject mappings\")\n",
    "\n",
    "    # Process each directory and rename files\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for condition in [\"AD\", \"CN\"]:\n",
    "            dir_path = os.path.join(data_dir, split, condition)\n",
    "            if not os.path.exists(dir_path):\n",
    "                print(f\"Directory {dir_path} does not exist, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing {dir_path}...\")\n",
    "            for file_path in glob.glob(os.path.join(dir_path, \"*.nii.gz\")):\n",
    "                file_name = os.path.basename(file_path)\n",
    "                scan_id = file_name.split(\".\")[0]  # Extract scan ID (e.g., \"I12345\")\n",
    "\n",
    "                if scan_id in scan_to_subject:\n",
    "                    subject_id = scan_to_subject[scan_id]\n",
    "                    new_name = f\"{subject_id}_{file_name}\"\n",
    "                    new_path = os.path.join(dir_path, new_name)\n",
    "                    print(f\"Renaming {file_name} to {new_name}\")\n",
    "                    os.rename(file_path, new_path)\n",
    "                else:\n",
    "                    print(f\"Could not find subject ID for scan {scan_id}\")\n",
    "\n",
    "    print(\"File renaming complete!\")\n",
    "\n",
    "\n",
    "# prepend_subject_id(data_dir, xml_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now create another function that checks if any subjects that are in either the train or validations splits are in the test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subject_overlap_and_scans(data_dir):\n",
    "    \"\"\"\n",
    "    Check if any subjects in train or validation splits also appear in test split\n",
    "    and identify which scans belong to the same subject.\n",
    "    \"\"\"\n",
    "    # Sets to store subject IDs\n",
    "    total_subjects = set()\n",
    "    train_val_subjects = set()\n",
    "    test_subjects = set()\n",
    "\n",
    "    # Track all scans for each subject\n",
    "    subject_to_scans = defaultdict(list)\n",
    "\n",
    "    # Collect subject IDs and scans from each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for condition in [\"AD\", \"CN\"]:\n",
    "            dir_path = os.path.join(data_dir, split, condition)\n",
    "            if not os.path.exists(dir_path):\n",
    "                continue\n",
    "\n",
    "            for file_path in glob.glob(os.path.join(dir_path, \"*.nii.gz\")):\n",
    "                file_name = os.path.basename(file_path)\n",
    "                # Files should now be named like \"013_S_0575_I44926.nii.gz\"\n",
    "                if \"_\" in file_name:\n",
    "                    parts = file_name.split(\"_\")\n",
    "                    subject_id = \"_\".join(parts[:3])\n",
    "                    scan_id = parts[-1].split(\".\")[0]\n",
    "\n",
    "                    total_subjects.add(subject_id)\n",
    "\n",
    "                    if split in [\"train\", \"val\"]:\n",
    "                        train_val_subjects.add(subject_id)\n",
    "                    elif split == \"test\":\n",
    "                        test_subjects.add(subject_id)\n",
    "\n",
    "                    # Store scan info with split and condition\n",
    "                    subject_to_scans[subject_id].append(\n",
    "                        {\n",
    "                            \"scan_id\": scan_id,\n",
    "                            \"split\": split,\n",
    "                            \"condition\": condition,\n",
    "                            \"filename\": file_name,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    # Find subjects that appear in both train/val and test\n",
    "    overlapping_subjects = train_val_subjects.intersection(test_subjects)\n",
    "\n",
    "    # Report basic results\n",
    "    print(f\"Total subjects in train/val: {len(train_val_subjects)}\")\n",
    "    print(f\"Total subjects in test: {len(test_subjects)}\")\n",
    "    print(f\"Total subjects in dataset: {len(total_subjects)}\")\n",
    "\n",
    "    # Report subjects with multiple scans\n",
    "    subjects_with_multiple_scans = {\n",
    "        subj: scans for subj, scans in subject_to_scans.items() if len(scans) > 1\n",
    "    }\n",
    "    print(f\"\\nSubjects with multiple scans: {len(subjects_with_multiple_scans)}\")\n",
    "\n",
    "    # Report overlap results\n",
    "    if overlapping_subjects:\n",
    "        print(\n",
    "            f\"\\nWARNING: Found {len(overlapping_subjects)} subjects in both train/val and test!\"\n",
    "        )\n",
    "        print(\"\\nOverlapping subjects and their scans:\")\n",
    "        for subject in sorted(overlapping_subjects):\n",
    "            print(f\"\\nSubject {subject} appears in multiple splits:\")\n",
    "            for scan in subject_to_scans[subject]:\n",
    "                print(f\"  - {scan['filename']} ({scan['split']}/{scan['condition']})\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n✓ No subject overlap found between train/val and test splits\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# Run the enhanced overlap check\n",
    "check_subject_overlap_and_scans(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition AD:\n",
      "  - Total subjects: 94\n",
      "  - Total scans: 306\n",
      "Condition CN:\n",
      "  - Total subjects: 117\n",
      "  - Total scans: 543\n",
      "\n",
      "AD target scan counts:\n",
      "  - Train: 244 scans (80.0%)\n",
      "  - Val: 30 scans (10.0%)\n",
      "  - Test: 32 scans (10.0%)\n",
      "\n",
      "AD actual split:\n",
      "  - train: 84 subjects, 241 scans (78.8%)\n",
      "  - val: 6 subjects, 32 scans (10.5%)\n",
      "  - test: 4 subjects, 33 scans (10.8%)\n",
      "\n",
      "CN target scan counts:\n",
      "  - Train: 434 scans (80.0%)\n",
      "  - Val: 54 scans (10.0%)\n",
      "  - Test: 55 scans (10.0%)\n",
      "\n",
      "CN actual split:\n",
      "  - train: 105 subjects, 429 scans (79.0%)\n",
      "  - val: 6 subjects, 54 scans (9.9%)\n",
      "  - test: 6 subjects, 60 scans (11.0%)\n",
      "\n",
      "Copied 849 files to new split structure\n",
      "\n",
      "Verification results:\n",
      "Total subjects in train/val: 201\n",
      "Total subjects in test: 10\n",
      "Total subjects in dataset: 211\n",
      "\n",
      "✓ No subject overlap found between train/val and test splits\n",
      "\n",
      "Verifying scan distribution:\n",
      "\n",
      "AD scan distribution:\n",
      "  - Train: 241 scans (78.8%), target: 80.0%\n",
      "  - Val: 32 scans (10.5%), target: 10.0%\n",
      "  - Test: 33 scans (10.8%), target: 10.0%\n",
      "  ✓ AD scan distribution is close to target percentages\n",
      "\n",
      "CN scan distribution:\n",
      "  - Train: 429 scans (79.0%), target: 80.0%\n",
      "  - Val: 54 scans (9.9%), target: 10.0%\n",
      "  - Test: 60 scans (11.0%), target: 10.0%\n",
      "  ✓ CN scan distribution is close to target percentages\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "\n",
    "def reorganize_data_split(\n",
    "    data_dir, output_dir, train_pct=0.8, val_pct=0.1, test_pct=0.1, random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Reorganize data splits to ensure no subject overlap between train/val and test sets\n",
    "    while maintaining scan percentages close to the specified train/val/test ratios.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing original data\n",
    "        output_dir: Directory to save reorganized data\n",
    "        train_pct: Percentage for training set (default 0.8)\n",
    "        val_pct: Percentage for validation set (default 0.1)\n",
    "        test_pct: Percentage for test set (default 0.1)\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    # Check percentages\n",
    "    if abs(train_pct + val_pct + test_pct - 1.0) > 0.001:\n",
    "        raise ValueError(\"Split percentages must sum to 1.0\")\n",
    "\n",
    "    # Create output directory structure\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for condition in [\"AD\", \"CN\"]:\n",
    "            os.makedirs(os.path.join(output_dir, split, condition), exist_ok=True)\n",
    "\n",
    "    # Collect all subjects and their scans\n",
    "    subjects_by_condition = defaultdict(list)\n",
    "    subject_to_scans = defaultdict(list)\n",
    "    total_scans_by_condition = defaultdict(int)\n",
    "\n",
    "    # First pass: collect all subjects and their scans\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for condition in [\"AD\", \"CN\"]:\n",
    "            dir_path = os.path.join(data_dir, split, condition)\n",
    "            if not os.path.exists(dir_path):\n",
    "                continue\n",
    "\n",
    "            for file_path in glob.glob(os.path.join(dir_path, \"*.nii.gz\")):\n",
    "                file_name = os.path.basename(file_path)\n",
    "\n",
    "                # Files should be named like \"013_S_0575_I44926.nii.gz\"\n",
    "                if \"_\" in file_name:\n",
    "                    parts = file_name.split(\"_\")\n",
    "                    subject_id = \"_\".join(parts[:3])\n",
    "                    scan_id = parts[-1].split(\".\")[0]\n",
    "\n",
    "                    # Track unique subjects by condition\n",
    "                    if subject_id not in [s for s in subjects_by_condition[condition]]:\n",
    "                        subjects_by_condition[condition].append(subject_id)\n",
    "\n",
    "                    # Store scan information\n",
    "                    subject_to_scans[subject_id].append(\n",
    "                        {\n",
    "                            \"scan_id\": scan_id,\n",
    "                            \"condition\": condition,\n",
    "                            \"filename\": file_name,\n",
    "                            \"original_path\": file_path,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Count total scans by condition\n",
    "                    total_scans_by_condition[condition] += 1\n",
    "\n",
    "    # Get subject scan counts\n",
    "    subject_scan_counts = {\n",
    "        subject: len(scans) for subject, scans in subject_to_scans.items()\n",
    "    }\n",
    "\n",
    "    # Print summary statistics\n",
    "    for condition in subjects_by_condition:\n",
    "        print(f\"Condition {condition}:\")\n",
    "        print(f\"  - Total subjects: {len(subjects_by_condition[condition])}\")\n",
    "        print(f\"  - Total scans: {total_scans_by_condition[condition]}\")\n",
    "\n",
    "    # Create new splits aiming for scan percentages\n",
    "    new_splits = {}\n",
    "\n",
    "    for condition in subjects_by_condition:\n",
    "        subjects = subjects_by_condition[condition].copy()\n",
    "        random.shuffle(subjects)\n",
    "\n",
    "        # Calculate target scan counts\n",
    "        total_scans = total_scans_by_condition[condition]\n",
    "        target_train_scans = int(total_scans * train_pct)\n",
    "        target_val_scans = int(total_scans * val_pct)\n",
    "        target_test_scans = total_scans - target_train_scans - target_val_scans\n",
    "\n",
    "        print(f\"\\n{condition} target scan counts:\")\n",
    "        print(f\"  - Train: {target_train_scans} scans ({train_pct*100:.1f}%)\")\n",
    "        print(f\"  - Val: {target_val_scans} scans ({val_pct*100:.1f}%)\")\n",
    "        print(f\"  - Test: {target_test_scans} scans ({test_pct*100:.1f}%)\")\n",
    "\n",
    "        # Initialize splits\n",
    "        new_splits[condition] = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        current_counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "\n",
    "        # Sort subjects by number of scans (descending)\n",
    "        subjects_by_scans = sorted(\n",
    "            subjects, key=lambda subject: subject_scan_counts[subject], reverse=True\n",
    "        )\n",
    "\n",
    "        # First, assign subjects to test set (to ensure it gets close to target)\n",
    "        remaining_subjects = []\n",
    "        for subject in subjects_by_scans:\n",
    "            scan_count = subject_scan_counts[subject]\n",
    "\n",
    "            # If adding this subject would get test closer to target, add it\n",
    "            if (\n",
    "                current_counts[\"test\"] < target_test_scans\n",
    "                and current_counts[\"test\"] + scan_count <= target_test_scans * 1.1\n",
    "            ):\n",
    "                new_splits[condition][\"test\"].append(subject)\n",
    "                current_counts[\"test\"] += scan_count\n",
    "            else:\n",
    "                remaining_subjects.append(subject)\n",
    "\n",
    "        # Then assign to validation set\n",
    "        subjects_for_train = []\n",
    "        for subject in remaining_subjects:\n",
    "            scan_count = subject_scan_counts[subject]\n",
    "\n",
    "            # If adding this subject would get val closer to target, add it\n",
    "            if (\n",
    "                current_counts[\"val\"] < target_val_scans\n",
    "                and current_counts[\"val\"] + scan_count <= target_val_scans * 1.1\n",
    "            ):\n",
    "                new_splits[condition][\"val\"].append(subject)\n",
    "                current_counts[\"val\"] += scan_count\n",
    "            else:\n",
    "                subjects_for_train.append(subject)\n",
    "\n",
    "        # Remaining subjects go to training set\n",
    "        for subject in subjects_for_train:\n",
    "            new_splits[condition][\"train\"].append(subject)\n",
    "            current_counts[\"train\"] += subject_scan_counts[subject]\n",
    "\n",
    "        # Print actual scan counts\n",
    "        print(f\"\\n{condition} actual split:\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            subjects_count = len(new_splits[condition][split])\n",
    "            scans_count = current_counts[split]\n",
    "            percentage = scans_count / total_scans * 100 if total_scans > 0 else 0\n",
    "            print(\n",
    "                f\"  - {split}: {subjects_count} subjects, {scans_count} scans ({percentage:.1f}%)\"\n",
    "            )\n",
    "\n",
    "    # Copy files to new locations\n",
    "    copied_files = 0\n",
    "    for subject_id, scans in subject_to_scans.items():\n",
    "        for scan in scans:\n",
    "            condition = scan[\"condition\"]\n",
    "\n",
    "            # Determine which split this subject belongs to\n",
    "            target_split = None\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                if subject_id in new_splits[condition][split]:\n",
    "                    target_split = split\n",
    "                    break\n",
    "\n",
    "            if target_split:\n",
    "                # Copy file to new location\n",
    "                src_path = scan[\"original_path\"]\n",
    "                dst_path = os.path.join(\n",
    "                    output_dir, target_split, condition, scan[\"filename\"]\n",
    "                )\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                copied_files += 1\n",
    "\n",
    "    print(f\"\\nCopied {copied_files} files to new split structure\")\n",
    "\n",
    "    # Verify no subject overlap\n",
    "    verify_no_overlap(output_dir)\n",
    "\n",
    "    # Verify actual scan percentages\n",
    "    verify_scan_percentages(\n",
    "        output_dir,\n",
    "        target_train_pct=train_pct,\n",
    "        target_val_pct=val_pct,\n",
    "        target_test_pct=test_pct,\n",
    "    )\n",
    "\n",
    "\n",
    "def verify_no_overlap(data_dir):\n",
    "    \"\"\"\n",
    "    Verify that there is no subject overlap between train/val and test splits.\n",
    "    \"\"\"\n",
    "    # Sets to store subject IDs\n",
    "    train_val_subjects = set()\n",
    "    test_subjects = set()\n",
    "\n",
    "    # Collect subject IDs from each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for condition in [\"AD\", \"CN\"]:\n",
    "            dir_path = os.path.join(data_dir, split, condition)\n",
    "            if not os.path.exists(dir_path):\n",
    "                continue\n",
    "\n",
    "            for file_path in glob.glob(os.path.join(dir_path, \"*.nii.gz\")):\n",
    "                file_name = os.path.basename(file_path)\n",
    "\n",
    "                # Files should be named like \"013_S_0575_I44926.nii.gz\"\n",
    "                if \"_\" in file_name:\n",
    "                    parts = file_name.split(\"_\")\n",
    "                    subject_id = \"_\".join(parts[:3])\n",
    "\n",
    "                    if split in [\"train\", \"val\"]:\n",
    "                        train_val_subjects.add(subject_id)\n",
    "                    elif split == \"test\":\n",
    "                        test_subjects.add(subject_id)\n",
    "\n",
    "    # Find subjects that appear in both train/val and test\n",
    "    overlapping_subjects = train_val_subjects.intersection(test_subjects)\n",
    "\n",
    "    # Report basic results\n",
    "    print(f\"\\nVerification results:\")\n",
    "    print(f\"Total subjects in train/val: {len(train_val_subjects)}\")\n",
    "    print(f\"Total subjects in test: {len(test_subjects)}\")\n",
    "    print(f\"Total subjects in dataset: {len(train_val_subjects.union(test_subjects))}\")\n",
    "\n",
    "    # Report overlap results\n",
    "    if overlapping_subjects:\n",
    "        print(\n",
    "            f\"\\nWARNING: Found {len(overlapping_subjects)} subjects in both train/val and test!\"\n",
    "        )\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n✓ No subject overlap found between train/val and test splits\")\n",
    "        return True\n",
    "\n",
    "\n",
    "def verify_scan_percentages(\n",
    "    data_dir, target_train_pct=0.8, target_val_pct=0.1, target_test_pct=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Verify the actual percentages of scans in each split.\n",
    "    \"\"\"\n",
    "    print(\"\\nVerifying scan distribution:\")\n",
    "\n",
    "    # Count scans in each split\n",
    "    scan_counts_by_condition = defaultdict(lambda: {\"train\": 0, \"val\": 0, \"test\": 0})\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for condition in [\"AD\", \"CN\"]:\n",
    "            dir_path = os.path.join(data_dir, split, condition)\n",
    "            if not os.path.exists(dir_path):\n",
    "                continue\n",
    "\n",
    "            scan_count = len(glob.glob(os.path.join(dir_path, \"*.nii.gz\")))\n",
    "            scan_counts_by_condition[condition][split] = scan_count\n",
    "\n",
    "    # Calculate percentages\n",
    "    for condition, counts in scan_counts_by_condition.items():\n",
    "        total_scans = sum(counts.values())\n",
    "\n",
    "        if total_scans > 0:\n",
    "            train_pct = counts[\"train\"] / total_scans\n",
    "            val_pct = counts[\"val\"] / total_scans\n",
    "            test_pct = counts[\"test\"] / total_scans\n",
    "\n",
    "            print(f\"\\n{condition} scan distribution:\")\n",
    "            print(\n",
    "                f\"  - Train: {counts['train']} scans ({train_pct*100:.1f}%), target: {target_train_pct*100:.1f}%\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  - Val: {counts['val']} scans ({val_pct*100:.1f}%), target: {target_val_pct*100:.1f}%\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  - Test: {counts['test']} scans ({test_pct*100:.1f}%), target: {target_test_pct*100:.1f}%\"\n",
    "            )\n",
    "\n",
    "            # Check if actual percentages are close to targets\n",
    "            train_diff = abs(train_pct - target_train_pct)\n",
    "            val_diff = abs(val_pct - target_val_pct)\n",
    "            test_diff = abs(test_pct - target_test_pct)\n",
    "\n",
    "            max_diff = max(train_diff, val_diff, test_diff)\n",
    "\n",
    "            if max_diff < 0.05:  # Within 5% of target\n",
    "                print(\n",
    "                    f\"  ✓ {condition} scan distribution is close to target percentages\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"  ⚠️ {condition} scan distribution deviates from targets by up to {max_diff*100:.1f}%\"\n",
    "                )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"./DATA/OUTPUT\"\n",
    "    reorganize_data_split(data_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
