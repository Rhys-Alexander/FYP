{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.10\n",
    "\n",
    "%pip install torch torchvision torchaudio torchinfo nibabel numpy tqdm wandb monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models.video as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nilearn import plotting\n",
    "from torchinfo import torchinfo\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    RandFlip,\n",
    "    RandGaussianNoise,\n",
    "    RandGaussianSmooth,\n",
    "    RandAdjustContrast,\n",
    "    RandScaleIntensity,\n",
    "    NormalizeIntensity,\n",
    ")\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"./DATA/ADNI_CROPPED_128\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Scans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scans(dataset_path, split=\"train\", num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize the first few MRI scans from AD and CN directories.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset directory.\n",
    "        split (str): Dataset split to visualize ('train', 'val', 'test').\n",
    "        num_samples (int): Number of samples to visualize from each class.\n",
    "    \"\"\"\n",
    "    # Define directories for AD and CN\n",
    "    ad_dir = os.path.join(dataset_path, split, \"AD\")\n",
    "    cn_dir = os.path.join(dataset_path, split, \"CN\")\n",
    "\n",
    "    # Get the first few files from each directory\n",
    "    ad_files = [\n",
    "        os.path.join(ad_dir, f) for f in os.listdir(ad_dir) if f.endswith(\".nii.gz\")\n",
    "    ][:num_samples]\n",
    "    cn_files = [\n",
    "        os.path.join(cn_dir, f) for f in os.listdir(cn_dir) if f.endswith(\".nii.gz\")\n",
    "    ][:num_samples]\n",
    "\n",
    "    # Plot the first few AD scans\n",
    "    print(\"AD Scans:\")\n",
    "    for file in ad_files:\n",
    "        plotting.plot_anat(file, title=os.path.basename(file))\n",
    "    plotting.show()\n",
    "\n",
    "    # Plot the first few CN scans\n",
    "    print(\"CN Scans:\")\n",
    "    for file in cn_files:\n",
    "        plotting.plot_anat(file, title=os.path.basename(file))\n",
    "    plotting.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# visualize_scans(DATASET, split=\"train\", num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Metal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal) device\n"
     ]
    }
   ],
   "source": [
    "# Check if Metal is available on macOS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal) device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available, using CPU\")\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    # Empty CUDA cache periodically during training to avoid memory fragmentation\n",
    "    def empty_cache():\n",
    "        try:\n",
    "            # For newer PyTorch versions with MPS cache management\n",
    "            torch.mps.empty_cache()\n",
    "        except:\n",
    "            print(\"MPS cache management not available\")\n",
    "            pass  # Ignore if this function doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", apply_augmentation=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "\n",
    "        # Get all files from AD and CN directories\n",
    "        ad_dir = os.path.join(root_dir, split, \"AD\")\n",
    "        cn_dir = os.path.join(root_dir, split, \"CN\")\n",
    "\n",
    "        # Load AD samples (label 1)\n",
    "        for file in os.listdir(ad_dir):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                self.samples.append(os.path.join(ad_dir, file))\n",
    "                self.labels.append(1)  # AD class\n",
    "\n",
    "        # Load CN samples (label 0)\n",
    "        for file in os.listdir(cn_dir):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                self.samples.append(os.path.join(cn_dir, file))\n",
    "                self.labels.append(0)  # CN class\n",
    "\n",
    "        # Setup augmentation transforms using MONAI - WITHOUT normalization\n",
    "        if apply_augmentation:\n",
    "            self.transforms = Compose(\n",
    "                [\n",
    "                    RandRotate90(prob=0.5, spatial_axes=(1, 2)),\n",
    "                    RandFlip(prob=0.5, spatial_axis=0),\n",
    "                    RandGaussianNoise(prob=0.2, mean=0.0, std=0.1),\n",
    "                    RandGaussianSmooth(prob=0.2, sigma_x=(0.5, 1.5)),\n",
    "                    RandAdjustContrast(prob=0.3, gamma=(0.7, 1.3)),\n",
    "                    RandScaleIntensity(prob=0.3, factors=0.2),\n",
    "                    NormalizeIntensity(nonzero=True),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transforms = Compose([NormalizeIntensity(nonzero=True)])\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} samples for {split} split\")\n",
    "        print(f\"Augmentation applied: {apply_augmentation}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the .nii.gz file\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image using nibabel\n",
    "        img = nib.load(img_path)\n",
    "        img_data = img.get_fdata()\n",
    "\n",
    "        # Ensure the size is exactly 128x128x128\n",
    "        current_d, current_h, current_w = img_data.shape\n",
    "        if current_d != 128 or current_h != 128 or current_w != 128:\n",
    "            raise ValueError(\n",
    "                f\"Expected image size 128x128x128 but got {current_d}x{current_h}x{current_w}\"\n",
    "            )\n",
    "\n",
    "        # Add channel dimension to numpy array\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "\n",
    "        # Apply transforms (MONAI transforms work with both numpy arrays and tensors)\n",
    "        img_data = self.transforms(img_data)\n",
    "\n",
    "        # Convert to tensor if not already a tensor\n",
    "        if not isinstance(img_data, torch.Tensor):\n",
    "            img_data = torch.tensor(img_data, dtype=torch.float32)\n",
    "\n",
    "        # Ensure the label is also a tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return img_data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified 3D ResNet model with layer freezing\n",
    "class MRIModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_layers=True):\n",
    "        super(MRIModel, self).__init__()\n",
    "        # Using a video ResNet and modifying it for 3D MRI\n",
    "        self.resnet = models.r3d_18(weights=models.R3D_18_Weights.KINETICS400_V1)\n",
    "\n",
    "        # Replace the first layer to accept single-channel input instead of 3\n",
    "        self.resnet.stem[0] = nn.Conv3d(\n",
    "            1,\n",
    "            64,\n",
    "            kernel_size=(3, 7, 7),\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(1, 3, 3),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # Replace the final fully connected layer for binary classification\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        # Freeze specific layers if requested\n",
    "        if freeze_layers:\n",
    "            self._freeze_layers()\n",
    "\n",
    "    def _freeze_layers(self):\n",
    "        \"\"\"Freeze most layers of the ResNet model, leaving only layer4 and fc unfrozen\"\"\"\n",
    "        # Freeze stem and layers 1-3\n",
    "        # TODO loook at model in more detail and see where to freeze\n",
    "        for name, param in self.resnet.named_parameters():\n",
    "            if \"layer4\" not in name and \"fc\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def count_trainable_params(self):\n",
    "        \"\"\"Count and return trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def count_total_params(self):\n",
    "        \"\"\"Count and return total parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, 1, D, H, W)\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_summary(model, input_size=(1, 1, 128, 128, 128), detailed=True):\n",
    "    \"\"\"\n",
    "    Display a comprehensive summary of the model architecture and parameters.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to analyze\n",
    "        input_size: The input tensor size (batch_size, channels, depth, height, width)\n",
    "        detailed: Whether to show detailed layer information\n",
    "    \"\"\"\n",
    "    # Get basic model summary using torchinfo\n",
    "    summary = torchinfo.summary(\n",
    "        model,\n",
    "        input_size=input_size,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    print(f\"MODEL ARCHITECTURE SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary)\n",
    "\n",
    "    # Count parameters by layer type\n",
    "    layer_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        layer_type = module.__class__.__name__\n",
    "        if layer_type not in layer_counts:\n",
    "            layer_counts[layer_type] = {\"count\": 0, \"params\": 0, \"trainable_params\": 0}\n",
    "\n",
    "        layer_counts[layer_type][\"count\"] += 1\n",
    "        params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "        trainable_params = sum(\n",
    "            p.numel() for p in module.parameters(recurse=False) if p.requires_grad\n",
    "        )\n",
    "\n",
    "        layer_counts[layer_type][\"params\"] += params\n",
    "        layer_counts[layer_type][\"trainable_params\"] += trainable_params\n",
    "\n",
    "    # Create detailed layer information dataframe\n",
    "    if detailed:\n",
    "        layers_info = []\n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Only leaf modules\n",
    "                params = sum(p.numel() for p in module.parameters())\n",
    "                trainable = sum(\n",
    "                    p.numel() for p in module.parameters() if p.requires_grad\n",
    "                )\n",
    "\n",
    "                layers_info.append(\n",
    "                    {\n",
    "                        \"Layer\": name,\n",
    "                        \"Type\": module.__class__.__name__,\n",
    "                        \"Parameters\": params,\n",
    "                        \"Trainable\": trainable,\n",
    "                        \"Frozen\": params - trainable,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Create and display DataFrame\n",
    "        df = pd.DataFrame(layers_info)\n",
    "        if not df.empty:\n",
    "            print(\"\\nDETAILED LAYER INFORMATION:\")\n",
    "            print(\"=\" * 80)\n",
    "            display(df)\n",
    "\n",
    "    # Show frozen vs trainable stats\n",
    "    total_params = model.count_total_params()\n",
    "    trainable_params = model.count_trainable_params()\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(\"\\nPARAMETER STATISTICS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total parameters:    {total_params:,}\")\n",
    "    print(\n",
    "        f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Frozen parameters:    {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Display model architecture as text\n",
    "    print(\"\\nMODEL ARCHITECTURE DETAILS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(model)\n",
    "\n",
    "    # Return summary for potential further use\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# model = MRIModel(num_classes=2, freeze_layers=True)\n",
    "# display_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    # Use tensors instead of scalar values\n",
    "    running_loss_tensor = torch.tensor(0.0, device=device)\n",
    "    correct_tensor = torch.tensor(0, device=device)\n",
    "    total = 0\n",
    "\n",
    "    # Store predictions and labels on GPU during training\n",
    "    batch_predictions = []\n",
    "    batch_labels = []\n",
    "\n",
    "    # Timing statistics - with more detailed metrics breakdown\n",
    "    time_points = {\n",
    "        \"batch_start\": [],\n",
    "        \"data_loaded\": [],\n",
    "        \"transfer_complete\": [],\n",
    "        \"optimizer_zeroed\": [],\n",
    "        \"forward_complete\": [],\n",
    "        \"loss_complete\": [],\n",
    "        \"backward_complete\": [],\n",
    "        \"optimizer_complete\": [],\n",
    "        \"running_loss_updated\": [],  # New timing point\n",
    "        \"accuracy_calc_start\": [],  # New timing point\n",
    "        \"accuracy_calc_complete\": [],  # New timing point\n",
    "        \"store_tensors_start\": [],  # New timing point\n",
    "        \"store_tensors_complete\": [],  # New timing point\n",
    "        \"metrics_complete\": [],\n",
    "        \"batch_end\": [],\n",
    "        \"next_batch_start\": [],\n",
    "    }\n",
    "\n",
    "    # For the first batch\n",
    "    time_points[\"batch_start\"].append(time.time())\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "\n",
    "        # Add this at the beginning of your training loop to check input size\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Input shape: {inputs.shape}, Memory: {inputs.element_size() * inputs.nelement() / 1024 / 1024:.2f} MB\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Labels shape: {labels.shape}, Memory: {labels.element_size() * labels.nelement() / 1024 / 1024:.2f} MB\"\n",
    "            )\n",
    "\n",
    "        # Record time when data is loaded from dataloader\n",
    "        time_points[\"data_loaded\"].append(time.time())\n",
    "\n",
    "        # Transfer to device\n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(\n",
    "            device, non_blocking=True\n",
    "        )\n",
    "        time_points[\"transfer_complete\"].append(time.time())\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        time_points[\"optimizer_zeroed\"].append(time.time())\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        time_points[\"forward_complete\"].append(time.time())\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        time_points[\"loss_complete\"].append(time.time())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        time_points[\"backward_complete\"].append(time.time())\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        time_points[\"optimizer_complete\"].append(time.time())\n",
    "\n",
    "        # Update running loss - KEEP AS TENSOR, AVOID .item()\n",
    "        running_loss_tensor += loss\n",
    "        time_points[\"running_loss_updated\"].append(time.time())\n",
    "\n",
    "        # Start accuracy calculation\n",
    "        time_points[\"accuracy_calc_start\"].append(time.time())\n",
    "\n",
    "        # Calculate accuracy - KEEP AS TENSOR, AVOID .item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct_tensor += (predicted == labels).sum()  # Keep as tensor\n",
    "\n",
    "        time_points[\"accuracy_calc_complete\"].append(time.time())\n",
    "\n",
    "        # Store predictions and labels on GPU (no CPU transfer)\n",
    "        time_points[\"store_tensors_start\"].append(time.time())\n",
    "\n",
    "        # Store detached tensors to avoid memory leak\n",
    "        batch_predictions.append(predicted.detach())\n",
    "        batch_labels.append(labels.detach())\n",
    "\n",
    "        time_points[\"store_tensors_complete\"].append(time.time())\n",
    "        time_points[\"metrics_complete\"].append(time.time())\n",
    "\n",
    "        # Record end of batch\n",
    "        time_points[\"batch_end\"].append(time.time())\n",
    "\n",
    "        # Start next batch timing\n",
    "        if batch_idx < len(dataloader) - 1:  # Don't record after last batch\n",
    "            time_points[\"next_batch_start\"].append(time.time())\n",
    "            # Also record this as the start of the next batch\n",
    "            time_points[\"batch_start\"].append(time.time())\n",
    "\n",
    "        if batch_idx > 0:  # We have complete timing data starting from batch 1\n",
    "            i = batch_idx if batch_idx < len(time_points[\"data_loaded\"]) else -1\n",
    "\n",
    "            # Standard timings\n",
    "            data_loading_time = (\n",
    "                time_points[\"data_loaded\"][i] - time_points[\"batch_start\"][i]\n",
    "            )\n",
    "            transfer_time = (\n",
    "                time_points[\"transfer_complete\"][i] - time_points[\"data_loaded\"][i]\n",
    "            )\n",
    "            zero_grad_time = (\n",
    "                time_points[\"optimizer_zeroed\"][i] - time_points[\"transfer_complete\"][i]\n",
    "            )\n",
    "            forward_time = (\n",
    "                time_points[\"forward_complete\"][i] - time_points[\"optimizer_zeroed\"][i]\n",
    "            )\n",
    "            loss_time = (\n",
    "                time_points[\"loss_complete\"][i] - time_points[\"forward_complete\"][i]\n",
    "            )\n",
    "            backward_time = (\n",
    "                time_points[\"backward_complete\"][i] - time_points[\"loss_complete\"][i]\n",
    "            )\n",
    "            optim_time = (\n",
    "                time_points[\"optimizer_complete\"][i]\n",
    "                - time_points[\"backward_complete\"][i]\n",
    "            )\n",
    "\n",
    "            # Detailed metrics timing breakdown\n",
    "            running_loss_update_time = (\n",
    "                time_points[\"running_loss_updated\"][i]\n",
    "                - time_points[\"optimizer_complete\"][i]\n",
    "            )\n",
    "            accuracy_calc_time = (\n",
    "                time_points[\"accuracy_calc_complete\"][i]\n",
    "                - time_points[\"accuracy_calc_start\"][i]\n",
    "            )\n",
    "            store_tensors_time = (\n",
    "                time_points[\"store_tensors_complete\"][i]\n",
    "                - time_points[\"store_tensors_start\"][i]\n",
    "            )\n",
    "\n",
    "            # Total metrics time\n",
    "            metrics_time = (\n",
    "                time_points[\"metrics_complete\"][i]\n",
    "                - time_points[\"optimizer_complete\"][i]\n",
    "            )\n",
    "            end_time = time_points[\"batch_end\"][i] - time_points[\"metrics_complete\"][i]\n",
    "\n",
    "            total_time = time_points[\"batch_end\"][i] - time_points[\"batch_start\"][i]\n",
    "\n",
    "            if i > 0:\n",
    "                dataloader_overhead = (\n",
    "                    time_points[\"data_loaded\"][i] - time_points[\"batch_end\"][i - 1]\n",
    "                )\n",
    "            else:\n",
    "                dataloader_overhead = 0\n",
    "\n",
    "            print(f\"\\nDetailed timing for Batch {batch_idx} (seconds):\")\n",
    "            print(f\"  Data loading:       {data_loading_time:.4f}\")\n",
    "            print(f\"  Transfer to device: {transfer_time:.4f}\")\n",
    "            print(f\"  Zero gradients:     {zero_grad_time:.4f}\")\n",
    "            print(f\"  Forward pass:       {forward_time:.4f}\")\n",
    "            print(f\"  Loss calculation:   {loss_time:.4f}\")\n",
    "            print(f\"  Backward pass:      {backward_time:.4f}\")\n",
    "            print(f\"  Optimizer step:     {optim_time:.4f}\")\n",
    "\n",
    "            # Detailed metrics breakdown\n",
    "            print(\"\\n  METRICS BREAKDOWN:\")\n",
    "            print(f\"    Running loss update:    {running_loss_update_time:.4f}\")\n",
    "            print(f\"    Accuracy calculation:   {accuracy_calc_time:.4f}\")\n",
    "            print(f\"    Store tensors:          {store_tensors_time:.4f}\")\n",
    "            print(f\"    Total metrics time:     {metrics_time:.4f}\")\n",
    "\n",
    "            print(f\"\\n  End batch overhead: {end_time:.4f}\")\n",
    "            print(f\"  Dataloader overhead:{dataloader_overhead:.4f}\")\n",
    "            print(f\"  Total batch time:   {total_time:.4f}\")\n",
    "\n",
    "            # Sum all measured components\n",
    "            measured_time = (\n",
    "                data_loading_time\n",
    "                + transfer_time\n",
    "                + zero_grad_time\n",
    "                + forward_time\n",
    "                + loss_time\n",
    "                + backward_time\n",
    "                + optim_time\n",
    "                + metrics_time\n",
    "                + end_time\n",
    "            )\n",
    "\n",
    "            print(f\"  Sum of measured ops: {measured_time:.4f}\")\n",
    "            missing = total_time - measured_time\n",
    "            print(\n",
    "                f\"  Missing time:       {missing:.4f} ({missing/total_time*100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "    # Process all predictions and labels at the end of the epoch\n",
    "    print(\"\\nProcessing stored tensors at end of epoch...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Concatenate all tensors on GPU\n",
    "    all_predictions_tensor = torch.cat(batch_predictions)\n",
    "    all_labels_tensor = torch.cat(batch_labels)\n",
    "\n",
    "    # Calculate class-wise metrics on GPU first\n",
    "    class_0_mask = all_labels_tensor == 0\n",
    "    class_1_mask = all_labels_tensor == 1\n",
    "\n",
    "    class_0_correct = ((all_predictions_tensor == 0) & class_0_mask).sum()\n",
    "    class_1_correct = ((all_predictions_tensor == 1) & class_1_mask).sum()\n",
    "    class_0_total = class_0_mask.sum()\n",
    "    class_1_total = class_1_mask.sum()\n",
    "\n",
    "    # Convert to numpy only once at the end (if needed for other metrics)\n",
    "    # all_preds = all_predictions_tensor.cpu().numpy()\n",
    "    # all_labels = all_labels_tensor.cpu().numpy()\n",
    "\n",
    "    # Calculate class-wise accuracy\n",
    "    class_0_acc = 100 * class_0_correct.item() / max(class_0_total.item(), 1)\n",
    "    class_1_acc = 100 * class_1_correct.item() / max(class_1_total.item(), 1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"End-of-epoch tensor processing time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    # Convert accumulated tensors to scalars ONCE at the end\n",
    "    epoch_loss = running_loss_tensor.item() / len(dataloader)\n",
    "    epoch_acc = 100 * correct_tensor.item() / total\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"train_acc\": epoch_acc,\n",
    "            \"train_CN_acc\": class_0_acc,\n",
    "            \"train_AD_acc\": class_1_acc,\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Track for metrics\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # For accuracy\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store for metrics\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of AD class\n",
    "\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Class-wise accuracy\n",
    "    class_0_mask = all_labels == 0\n",
    "    class_1_mask = all_labels == 1\n",
    "\n",
    "    class_0_acc = (\n",
    "        100\n",
    "        * np.sum(all_preds[class_0_mask] == all_labels[class_0_mask])\n",
    "        / (np.sum(class_0_mask) + 1e-10)\n",
    "    )\n",
    "    class_1_acc = (\n",
    "        100\n",
    "        * np.sum(all_preds[class_1_mask] == all_labels[class_1_mask])\n",
    "        / (np.sum(class_1_mask) + 1e-10)\n",
    "    )\n",
    "\n",
    "    # Custom metrics\n",
    "    true_positives = np.sum((all_preds == 1) & (all_labels == 1))\n",
    "    false_positives = np.sum((all_preds == 1) & (all_labels == 0))\n",
    "    true_negatives = np.sum((all_preds == 0) & (all_labels == 0))\n",
    "    false_negatives = np.sum((all_preds == 0) & (all_labels == 1))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-10)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    # Generate confusion matrix for visualization\n",
    "    confusion_matrix = wandb.plot.confusion_matrix(\n",
    "        preds=all_preds, y_true=all_labels, class_names=[\"CN\", \"AD\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create proper format for probabilities\n",
    "        y_probas = np.zeros((len(all_labels), 2))\n",
    "        y_probas[:, 0] = 1 - np.array(all_probs)  # CN probabilities\n",
    "        y_probas[:, 1] = np.array(all_probs)  # AD probabilities\n",
    "\n",
    "        # Now call the function with properly formatted data\n",
    "        roc_curve = wandb.plot.roc_curve(\n",
    "            all_labels,\n",
    "            y_probas,\n",
    "            classes_to_plot=[1],  # Plot ROC for AD class (positive class)\n",
    "            labels=[\"CN\", \"AD\"],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: ROC curve calculation failed: {e}\")\n",
    "        roc_curve = None\n",
    "\n",
    "    # Log validation metrics (with conditional ROC curve)\n",
    "    log_dict = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_CN_acc\": class_0_acc,\n",
    "        \"val_AD_acc\": class_1_acc,\n",
    "        \"val_precision\": precision,\n",
    "        \"val_recall\": recall,\n",
    "        \"val_f1\": f1_score,\n",
    "        \"confusion_matrix\": confusion_matrix,\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "\n",
    "    if roc_curve is not None:\n",
    "        log_dict[\"roc_curve\"] = roc_curve\n",
    "\n",
    "    wandb.log(log_dict)\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrhys-alexander\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rhysalexander/Desktop/FYP/code/wandb/run-20250306_192014-mvpth4ur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rhys-alexander/mri-alzheimers-classification/runs/mvpth4ur' target=\"_blank\">flowing-monkey-37</a></strong> to <a href='https://wandb.ai/rhys-alexander/mri-alzheimers-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rhys-alexander/mri-alzheimers-classification' target=\"_blank\">https://wandb.ai/rhys-alexander/mri-alzheimers-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rhys-alexander/mri-alzheimers-classification/runs/mvpth4ur' target=\"_blank\">https://wandb.ai/rhys-alexander/mri-alzheimers-classification/runs/mvpth4ur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 678 samples for train split\n",
      "Augmentation applied: True\n",
      "Loaded 85 samples for val split\n",
      "Augmentation applied: False\n",
      "Total parameters: 33,148,482\n",
      "Trainable parameters: 24,909,826 (75.15%)\n",
      "Frozen parameters: 8,238,656 (24.85%)\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/339 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1, 128, 128, 128]), Memory: 16.00 MB\n",
      "Labels shape: torch.Size([2]), Memory: 0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 2/339 [00:06<19:51,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 1 (seconds):\n",
      "  Data loading:       0.3919\n",
      "  Transfer to device: 0.0005\n",
      "  Zero gradients:     0.0005\n",
      "  Forward pass:       4.2107\n",
      "  Loss calculation:   0.0069\n",
      "  Backward pass:      0.5108\n",
      "  Optimizer step:     0.1065\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0002\n",
      "    Accuracy calculation:   0.0036\n",
      "    Store tensors:          0.0002\n",
      "    Total metrics time:     0.0040\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.3920\n",
      "  Total batch time:   5.2318\n",
      "  Sum of measured ops: 5.2318\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/339 [00:12<25:55,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 2 (seconds):\n",
      "  Data loading:       0.4148\n",
      "  Transfer to device: 0.0005\n",
      "  Zero gradients:     0.0008\n",
      "  Forward pass:       4.8929\n",
      "  Loss calculation:   0.0018\n",
      "  Backward pass:      0.5149\n",
      "  Optimizer step:     0.1067\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0002\n",
      "    Accuracy calculation:   0.0008\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0010\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.4151\n",
      "  Total batch time:   5.9332\n",
      "  Sum of measured ops: 5.9332\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 4/339 [00:18<28:43,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 3 (seconds):\n",
      "  Data loading:       0.2832\n",
      "  Transfer to device: 0.0515\n",
      "  Zero gradients:     0.0005\n",
      "  Forward pass:       4.9720\n",
      "  Loss calculation:   0.0009\n",
      "  Backward pass:      0.5207\n",
      "  Optimizer step:     0.1059\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0002\n",
      "    Accuracy calculation:   0.0008\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0009\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.2834\n",
      "  Total batch time:   5.9357\n",
      "  Sum of measured ops: 5.9357\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 5/339 [00:24<30:14,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 4 (seconds):\n",
      "  Data loading:       0.2901\n",
      "  Transfer to device: 0.0449\n",
      "  Zero gradients:     0.0006\n",
      "  Forward pass:       4.9768\n",
      "  Loss calculation:   0.0074\n",
      "  Backward pass:      0.5073\n",
      "  Optimizer step:     0.1068\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0002\n",
      "    Accuracy calculation:   0.0036\n",
      "    Store tensors:          0.0002\n",
      "    Total metrics time:     0.0039\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.2903\n",
      "  Total batch time:   5.9379\n",
      "  Sum of measured ops: 5.9379\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 6/339 [00:30<31:11,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 5 (seconds):\n",
      "  Data loading:       0.5852\n",
      "  Transfer to device: 0.0004\n",
      "  Zero gradients:     0.0007\n",
      "  Forward pass:       4.7181\n",
      "  Loss calculation:   0.0008\n",
      "  Backward pass:      0.5326\n",
      "  Optimizer step:     0.1462\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0007\n",
      "    Accuracy calculation:   0.0045\n",
      "    Store tensors:          0.0002\n",
      "    Total metrics time:     0.0053\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.5854\n",
      "  Total batch time:   5.9893\n",
      "  Sum of measured ops: 5.9893\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 7/339 [00:36<31:44,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 6 (seconds):\n",
      "  Data loading:       0.4449\n",
      "  Transfer to device: 0.0004\n",
      "  Zero gradients:     0.0007\n",
      "  Forward pass:       4.8948\n",
      "  Loss calculation:   0.0058\n",
      "  Backward pass:      0.5094\n",
      "  Optimizer step:     0.1116\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0003\n",
      "    Accuracy calculation:   0.0077\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0081\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.4451\n",
      "  Total batch time:   5.9757\n",
      "  Sum of measured ops: 5.9757\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 8/339 [00:42<31:58,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 7 (seconds):\n",
      "  Data loading:       0.2959\n",
      "  Transfer to device: 0.0296\n",
      "  Zero gradients:     0.0007\n",
      "  Forward pass:       4.9576\n",
      "  Loss calculation:   0.0010\n",
      "  Backward pass:      0.5253\n",
      "  Optimizer step:     0.1057\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0002\n",
      "    Accuracy calculation:   0.0007\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0009\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.2961\n",
      "  Total batch time:   5.9167\n",
      "  Sum of measured ops: 5.9167\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 9/339 [00:47<32:02,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 8 (seconds):\n",
      "  Data loading:       0.2632\n",
      "  Transfer to device: 0.0719\n",
      "  Zero gradients:     0.0004\n",
      "  Forward pass:       4.9409\n",
      "  Loss calculation:   0.0007\n",
      "  Backward pass:      0.5178\n",
      "  Optimizer step:     0.1054\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0001\n",
      "    Accuracy calculation:   0.0005\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0006\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.2634\n",
      "  Total batch time:   5.9009\n",
      "  Sum of measured ops: 5.9009\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 10/339 [01:00<42:44,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 9 (seconds):\n",
      "  Data loading:       0.4001\n",
      "  Transfer to device: 0.0004\n",
      "  Zero gradients:     0.0005\n",
      "  Forward pass:       11.0342\n",
      "  Loss calculation:   0.0025\n",
      "  Backward pass:      0.7094\n",
      "  Optimizer step:     0.0506\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0002\n",
      "    Accuracy calculation:   0.0008\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0010\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.4001\n",
      "  Total batch time:   12.1986\n",
      "  Sum of measured ops: 12.1986\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 11/339 [01:00<30:10,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 10 (seconds):\n",
      "  Data loading:       0.3020\n",
      "  Transfer to device: 0.0003\n",
      "  Zero gradients:     0.0002\n",
      "  Forward pass:       0.0201\n",
      "  Loss calculation:   0.0003\n",
      "  Backward pass:      0.0029\n",
      "  Optimizer step:     0.0400\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0001\n",
      "    Accuracy calculation:   0.0006\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0007\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.3021\n",
      "  Total batch time:   0.3665\n",
      "  Sum of measured ops: 0.3665\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▎         | 12/339 [01:06<30:40,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 11 (seconds):\n",
      "  Data loading:       0.3212\n",
      "  Transfer to device: 0.0004\n",
      "  Zero gradients:     0.0002\n",
      "  Forward pass:       4.9324\n",
      "  Loss calculation:   0.0004\n",
      "  Backward pass:      0.5190\n",
      "  Optimizer step:     0.1060\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0001\n",
      "    Accuracy calculation:   0.0004\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0005\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.3212\n",
      "  Total batch time:   5.8800\n",
      "  Sum of measured ops: 5.8800\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 13/339 [01:12<31:07,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 12 (seconds):\n",
      "  Data loading:       0.3893\n",
      "  Transfer to device: 0.0006\n",
      "  Zero gradients:     0.0010\n",
      "  Forward pass:       4.9379\n",
      "  Loss calculation:   0.0008\n",
      "  Backward pass:      0.5194\n",
      "  Optimizer step:     0.1081\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0002\n",
      "    Accuracy calculation:   0.0006\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0008\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.3893\n",
      "  Total batch time:   5.9578\n",
      "  Sum of measured ops: 5.9578\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 14/339 [01:18<31:26,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 13 (seconds):\n",
      "  Data loading:       0.3741\n",
      "  Transfer to device: 0.0005\n",
      "  Zero gradients:     0.0006\n",
      "  Forward pass:       4.9463\n",
      "  Loss calculation:   0.0061\n",
      "  Backward pass:      0.5224\n",
      "  Optimizer step:     0.1139\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0006\n",
      "    Accuracy calculation:   0.0030\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0036\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.3742\n",
      "  Total batch time:   5.9675\n",
      "  Sum of measured ops: 5.9675\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 15/339 [01:24<31:34,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed timing for Batch 14 (seconds):\n",
      "  Data loading:       0.5548\n",
      "  Transfer to device: 0.0006\n",
      "  Zero gradients:     0.0011\n",
      "  Forward pass:       4.7627\n",
      "  Loss calculation:   0.0011\n",
      "  Backward pass:      0.5238\n",
      "  Optimizer step:     0.1094\n",
      "\n",
      "  METRICS BREAKDOWN:\n",
      "    Running loss update:    0.0001\n",
      "    Accuracy calculation:   0.0005\n",
      "    Store tensors:          0.0000\n",
      "    Total metrics time:     0.0007\n",
      "\n",
      "  End batch overhead: 0.0000\n",
      "  Dataloader overhead:0.5550\n",
      "  Total batch time:   5.9543\n",
      "  Sum of measured ops: 5.9543\n",
      "  Missing time:       0.0000 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 15/339 [01:28<31:48,  5.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 286\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    284\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 286\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device, epoch)\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Update learning rate based on scheduler\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 60\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m     57\u001b[0m time_points[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_zeroed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m time_points[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_complete\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Loss calculation\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mMRIModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Input: (B, 1, D, H, W)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torchvision/models/video/resnet.py:254\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    251\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[1;32m    253\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 254\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    256\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torchvision/models/video/resnet.py:117\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[0;32m--> 117\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    118\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/monai/data/meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/torch/_tensor.py:1648\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1648\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x320483cd0>> (for post_run_cell), with arguments args (<ExecutionResult object at 1073ed840, execution_count=12 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 1073ed420, raw_cell=\"def main():\n",
      "    # Initialize wandb\n",
      "    wandb.init(..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/rhysalexander/Desktop/FYP/code/train.ipynb#X32sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:565\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:771\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:297\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:224\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    222\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    223\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FYP/code/myenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"mri-alzheimers-classification\",\n",
    "        config={\n",
    "            \"architecture\": \"3D-ResNet18-FrozenLayers\",\n",
    "            \"dataset\": \"MRI-AD-CN\",\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 2,\n",
    "            \"learning_rate\": 0.0001,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"device\": str(device),\n",
    "            \"input_dimensions\": \"128x128x128\",\n",
    "            \"freeze_layers\": True,\n",
    "            \"data_augmentation\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Parameters\n",
    "    data_root = DATASET  # Update this to your dataset path\n",
    "    batch_size = 2  # Reduced batch size for memory constraints\n",
    "    num_epochs = 20  # Reduced epochs for testing\n",
    "    learning_rate = 0.0001\n",
    "    freeze_layers = True\n",
    "    use_augmentation = True\n",
    "\n",
    "    # Create datasets with augmentation for training\n",
    "    train_dataset = MRIDataset(\n",
    "        data_root, split=\"train\", apply_augmentation=use_augmentation\n",
    "    )\n",
    "    val_dataset = MRIDataset(\n",
    "        data_root, split=\"val\", apply_augmentation=False\n",
    "    )  # No augmentation for validation\n",
    "\n",
    "    # Log dataset stats\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"train_samples\": len(train_dataset),\n",
    "            \"val_samples\": len(val_dataset),\n",
    "            \"train_AD_samples\": train_dataset.labels.count(1),\n",
    "            \"train_CN_samples\": train_dataset.labels.count(0),\n",
    "            \"val_AD_samples\": val_dataset.labels.count(1),\n",
    "            \"val_CN_samples\": val_dataset.labels.count(0),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Initialize the model with layer freezing\n",
    "    model = MRIModel(num_classes=2, freeze_layers=freeze_layers)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Log parameter statistics\n",
    "    trainable_params = model.count_trainable_params()\n",
    "    total_params = model.count_total_params()\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(\n",
    "        f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\"\n",
    "    )\n",
    "    print(f\"Frozen parameters: {frozen_params:,} ({frozen_params/total_params:.2%})\")\n",
    "\n",
    "    # Log model architecture and parameter stats\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params,\n",
    "            \"frozen_params\": frozen_params,\n",
    "            \"frozen_percentage\": frozen_params / total_params,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Loss function with class weighting to handle imbalance\n",
    "    # Calculate class weights based on sample distribution\n",
    "    num_ad = train_dataset.labels.count(1)\n",
    "    num_cn = train_dataset.labels.count(0)\n",
    "    total = num_ad + num_cn\n",
    "\n",
    "    # Inverse frequency weighting\n",
    "    weight_cn = total / (2 * num_cn) if num_cn > 0 else 1.0\n",
    "    weight_ad = total / (2 * num_ad) if num_ad > 0 else 1.0\n",
    "\n",
    "    class_weights = torch.tensor([weight_cn, weight_ad], device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Optimizer with parameter groups and different learning rates\n",
    "    # Higher learning rate for new/unfrozen layers, lower for pre-trained unfrozen layers\n",
    "    fc_params = list(model.resnet.fc.parameters())\n",
    "    other_params = [\n",
    "        p\n",
    "        for name, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(p is fc_param for fc_param in fc_params)\n",
    "    ]\n",
    "\n",
    "    # Set up parameter groups with different learning rates\n",
    "    param_groups = [\n",
    "        {\"params\": other_params, \"lr\": learning_rate},\n",
    "        {\n",
    "            \"params\": fc_params,\n",
    "            \"lr\": learning_rate * 10,\n",
    "        },  # Higher learning rate for final layer\n",
    "    ]\n",
    "\n",
    "    optimizer = optim.AdamW(param_groups, lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "    # Learning rate scheduler with cosine annealing for better convergence\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=1, eta_min=learning_rate / 100\n",
    "    )\n",
    "\n",
    "    # Early stopping implementation\n",
    "    patience = 5\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Check for checkpoint and load if exists\n",
    "    checkpoint_path = \"checkpoints/checkpoint.pth\"\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_val_acc = checkpoint.get(\n",
    "            \"best_val_acc\", 0.0\n",
    "        )  # Handle if best_val_acc wasn't saved.\n",
    "        best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n",
    "        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "    else:\n",
    "        best_val_acc = 0.0\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, epoch)\n",
    "\n",
    "        # Update learning rate based on scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save the best model by validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                },\n",
    "                \"best_model_acc.pth\",\n",
    "            )\n",
    "            print(\"Model saved (best accuracy)!\")\n",
    "\n",
    "            # Log best model as artifact\n",
    "            artifact = wandb.Artifact(\"best_model_acc\", type=\"model\")\n",
    "            artifact.add_file(\"best_model_acc.pth\")\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "            # Reset early stopping counter on improvement\n",
    "            early_stopping_counter = 0\n",
    "\n",
    "        # Save best model by validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                },\n",
    "                \"best_model_loss.pth\",\n",
    "            )\n",
    "            print(\"Model saved (best loss)!\")\n",
    "\n",
    "            # Log best model as artifact\n",
    "            artifact = wandb.Artifact(\"best_model_loss\", type=\"model\")\n",
    "            artifact.add_file(\"best_model_loss.pth\")\n",
    "            wandb.log_artifact(artifact)\n",
    "        else:\n",
    "            # Increment early stopping counter\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"best_val_acc\": best_val_acc,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "            },\n",
    "            checkpoint_path,\n",
    "        )\n",
    "\n",
    "        if device.type == \"mps\":\n",
    "            empty_cache()\n",
    "\n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = MRIDataset(\n",
    "        data_root, split=\"test\", apply_augmentation=False\n",
    "    )  # No augmentation for test set\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # After training, load best model for final evaluation\n",
    "    checkpoint = torch.load(\"best_model_acc.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(\n",
    "        f\"Loaded best model from epoch {checkpoint['epoch']+1} with accuracy {checkpoint['val_acc']:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    final_test_loss, final_test_acc = validate(\n",
    "        model, test_loader, criterion, device, num_epochs\n",
    "    )\n",
    "    print(f\"Final test accuracy: {final_test_acc:.2f}%\")\n",
    "\n",
    "    # Log final model summary\n",
    "    wandb.run.summary[\"best_val_acc\"] = best_val_acc\n",
    "    wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "    wandb.run.summary[\"final_val_acc\"] = final_test_acc\n",
    "    wandb.run.summary[\"final_val_loss\"] = final_test_loss\n",
    "    wandb.run.summary[\"total_epochs\"] = epoch + 1 if \"epoch\" in locals() else 0\n",
    "\n",
    "    # Close wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
