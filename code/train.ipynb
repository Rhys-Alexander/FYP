{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.10\n",
    "\n",
    "%pip install torch torchvision torchaudio nibabel numpy tqdm wandb monai\n",
    "\n",
    "DATASET = \"./DATA/ADNI_96_TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.backends.mps.is_available())  # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.video as models\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import random\n",
    "from scipy.ndimage import rotate, shift\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    RandFlip,\n",
    "    RandGaussianNoise,\n",
    "    RandGaussianSmooth,\n",
    "    RandAdjustContrast,\n",
    "    RandScaleIntensity,\n",
    ")\n",
    "\n",
    "# Check if Metal is available on macOS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal) device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available, using CPU\")\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    # Empty CUDA cache periodically during training to avoid memory fragmentation\n",
    "    def empty_cache():\n",
    "        try:\n",
    "            # For newer PyTorch versions with MPS cache management\n",
    "            torch.mps.empty_cache()\n",
    "        except:\n",
    "            pass  # Ignore if this function doesn't exist\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", apply_augmentation=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "\n",
    "        # Get all files from AD and CN directories\n",
    "        ad_dir = os.path.join(root_dir, split, \"AD\")\n",
    "        cn_dir = os.path.join(root_dir, split, \"CN\")\n",
    "\n",
    "        # Load AD samples (label 1)\n",
    "        for file in os.listdir(ad_dir):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                self.samples.append(os.path.join(ad_dir, file))\n",
    "                self.labels.append(1)  # AD class\n",
    "\n",
    "        # Load CN samples (label 0)\n",
    "        for file in os.listdir(cn_dir):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                self.samples.append(os.path.join(cn_dir, file))\n",
    "                self.labels.append(0)  # CN class\n",
    "\n",
    "        # Setup augmentation transforms using MONAI - WITHOUT normalization\n",
    "        if apply_augmentation:\n",
    "            self.transforms = Compose(\n",
    "                [\n",
    "                    RandRotate90(prob=0.5, spatial_axes=(1, 2)),\n",
    "                    RandFlip(prob=0.5, spatial_axis=0),\n",
    "                    RandGaussianNoise(prob=0.2, mean=0.0, std=0.1),\n",
    "                    RandGaussianSmooth(prob=0.2, sigma_x=(0.5, 1.5)),\n",
    "                    RandAdjustContrast(prob=0.3, gamma=(0.7, 1.3)),\n",
    "                    RandScaleIntensity(prob=0.3, factors=0.2),\n",
    "                    # NormalizeIntensity line removed\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # Empty transform composition - no normalization\n",
    "            self.transforms = Compose([])\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} samples for {split} split\")\n",
    "        print(f\"Augmentation applied: {apply_augmentation}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the .nii.gz file\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image using nibabel\n",
    "        img = nib.load(img_path)\n",
    "        img_data = img.get_fdata()\n",
    "\n",
    "        # Ensure the size is exactly 96x96x96\n",
    "        current_d, current_h, current_w = img_data.shape\n",
    "        if current_d != 96 or current_h != 96 or current_w != 96:\n",
    "            raise ValueError(\n",
    "                f\"Expected image size 96x96x96 but got {current_d}x{current_h}x{current_w}\"\n",
    "            )\n",
    "\n",
    "        # Convert to tensor\n",
    "        img_tensor = torch.tensor(img_data, dtype=torch.float32).unsqueeze(\n",
    "            0\n",
    "        )  # Add channel dim\n",
    "\n",
    "        # Apply transforms\n",
    "        if len(self.transforms.transforms) > 0:  # Only apply if transforms exist\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "\n",
    "        return img_tensor, label\n",
    "\n",
    "\n",
    "# Modified 3D ResNet model with layer freezing\n",
    "class MRIModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_layers=True):\n",
    "        super(MRIModel, self).__init__()\n",
    "        # Using a video ResNet and modifying it for 3D MRI\n",
    "        self.resnet = models.r3d_18(weights=models.R3D_18_Weights.KINETICS400_V1)\n",
    "\n",
    "        # Replace the first layer to accept single-channel input instead of 3\n",
    "        self.resnet.stem[0] = nn.Conv3d(\n",
    "            1,\n",
    "            64,\n",
    "            kernel_size=(3, 7, 7),\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(1, 3, 3),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # Replace the final fully connected layer for binary classification\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        # Freeze specific layers if requested\n",
    "        if freeze_layers:\n",
    "            self._freeze_layers()\n",
    "\n",
    "    def _freeze_layers(self):\n",
    "        \"\"\"Freeze early layers of the ResNet model\"\"\"\n",
    "        # Freeze stem and layer1 (early feature extractors)\n",
    "        for param in self.resnet.stem.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.resnet.layer1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(\"Frozen stem and layer1 of ResNet\")\n",
    "\n",
    "    def count_trainable_params(self):\n",
    "        \"\"\"Count and return trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def count_total_params(self):\n",
    "        \"\"\"Count and return total parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, 1, D, H, W)\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Track batch-level metrics\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad(set_to_none=True)  # Faster than setting to zero\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for metrics\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Log batch-level metrics\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_acc\": 100 * (predicted == labels).sum().item() / labels.size(0),\n",
    "                \"batch\": epoch * len(dataloader) + batch_idx,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    # Class-wise accuracy\n",
    "    class_0_mask = all_labels == 0\n",
    "    class_1_mask = all_labels == 1\n",
    "\n",
    "    class_0_acc = (\n",
    "        100\n",
    "        * np.sum(all_preds[class_0_mask] == all_labels[class_0_mask])\n",
    "        / (np.sum(class_0_mask) + 1e-10)\n",
    "    )\n",
    "    class_1_acc = (\n",
    "        100\n",
    "        * np.sum(all_preds[class_1_mask] == all_labels[class_1_mask])\n",
    "        / (np.sum(class_1_mask) + 1e-10)\n",
    "    )\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"train_acc\": epoch_acc,\n",
    "            \"train_CN_acc\": class_0_acc,\n",
    "            \"train_AD_acc\": class_1_acc,\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Track for metrics\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # For accuracy\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store for metrics\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of AD class\n",
    "\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Class-wise accuracy\n",
    "    class_0_mask = all_labels == 0\n",
    "    class_1_mask = all_labels == 1\n",
    "\n",
    "    class_0_acc = (\n",
    "        100\n",
    "        * np.sum(all_preds[class_0_mask] == all_labels[class_0_mask])\n",
    "        / (np.sum(class_0_mask) + 1e-10)\n",
    "    )\n",
    "    class_1_acc = (\n",
    "        100\n",
    "        * np.sum(all_preds[class_1_mask] == all_labels[class_1_mask])\n",
    "        / (np.sum(class_1_mask) + 1e-10)\n",
    "    )\n",
    "\n",
    "    # Custom metrics\n",
    "    true_positives = np.sum((all_preds == 1) & (all_labels == 1))\n",
    "    false_positives = np.sum((all_preds == 1) & (all_labels == 0))\n",
    "    true_negatives = np.sum((all_preds == 0) & (all_labels == 0))\n",
    "    false_negatives = np.sum((all_preds == 0) & (all_labels == 1))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-10)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    # Generate confusion matrix for visualization\n",
    "    confusion_matrix = wandb.plot.confusion_matrix(\n",
    "        preds=all_preds, y_true=all_labels, class_names=[\"CN\", \"AD\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create proper format for probabilities\n",
    "        y_probas = np.zeros((len(all_labels), 2))\n",
    "        y_probas[:, 0] = 1 - np.array(all_probs)  # CN probabilities\n",
    "        y_probas[:, 1] = np.array(all_probs)  # AD probabilities\n",
    "\n",
    "        # Now call the function with properly formatted data\n",
    "        roc_curve = wandb.plot.roc_curve(\n",
    "            all_labels,\n",
    "            y_probas,\n",
    "            classes_to_plot=[1],  # Plot ROC for AD class (positive class)\n",
    "            labels=[\"CN\", \"AD\"],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: ROC curve calculation failed: {e}\")\n",
    "        roc_curve = None\n",
    "\n",
    "    # Log validation metrics (with conditional ROC curve)\n",
    "    log_dict = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_CN_acc\": class_0_acc,\n",
    "        \"val_AD_acc\": class_1_acc,\n",
    "        \"val_precision\": precision,\n",
    "        \"val_recall\": recall,\n",
    "        \"val_f1\": f1_score,\n",
    "        \"confusion_matrix\": confusion_matrix,\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "\n",
    "    if roc_curve is not None:\n",
    "        log_dict[\"roc_curve\"] = roc_curve\n",
    "\n",
    "    wandb.log(log_dict)\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"mri-alzheimers-classification\",\n",
    "        config={\n",
    "            \"architecture\": \"3D-ResNet18-FrozenLayers\",\n",
    "            \"dataset\": \"MRI-AD-CN\",\n",
    "            \"epochs\": 10,  # Increased from 5 to 10\n",
    "            \"batch_size\": 2,\n",
    "            \"learning_rate\": 0.0001,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"device\": str(device),\n",
    "            \"input_dimensions\": \"96x96x96\",\n",
    "            \"freeze_layers\": True,\n",
    "            \"data_augmentation\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Parameters\n",
    "    data_root = DATASET  # Update this to your dataset path\n",
    "    batch_size = 2  # Reduced batch size for memory constraints\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.0001\n",
    "    freeze_layers = True\n",
    "    use_augmentation = True\n",
    "\n",
    "    # Create datasets with augmentation for training\n",
    "    train_dataset = MRIDataset(\n",
    "        data_root, split=\"train\", apply_augmentation=use_augmentation\n",
    "    )\n",
    "    val_dataset = MRIDataset(\n",
    "        data_root, split=\"val\", apply_augmentation=False\n",
    "    )  # No augmentation for validation\n",
    "\n",
    "    # Log dataset stats\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"train_samples\": len(train_dataset),\n",
    "            \"val_samples\": len(val_dataset),\n",
    "            \"train_AD_samples\": train_dataset.labels.count(1),\n",
    "            \"train_CN_samples\": train_dataset.labels.count(0),\n",
    "            \"val_AD_samples\": val_dataset.labels.count(1),\n",
    "            \"val_CN_samples\": val_dataset.labels.count(0),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    # Initialize the model with layer freezing\n",
    "    model = MRIModel(num_classes=2, freeze_layers=freeze_layers)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Log parameter statistics\n",
    "    trainable_params = model.count_trainable_params()\n",
    "    total_params = model.count_total_params()\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(\n",
    "        f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\"\n",
    "    )\n",
    "    print(f\"Frozen parameters: {frozen_params:,} ({frozen_params/total_params:.2%})\")\n",
    "\n",
    "    # Log model architecture and parameter stats\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params,\n",
    "            \"frozen_params\": frozen_params,\n",
    "            \"frozen_percentage\": frozen_params / total_params,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Loss function with class weighting to handle imbalance\n",
    "    # Calculate class weights based on sample distribution\n",
    "    num_ad = train_dataset.labels.count(1)\n",
    "    num_cn = train_dataset.labels.count(0)\n",
    "    total = num_ad + num_cn\n",
    "\n",
    "    # Inverse frequency weighting\n",
    "    weight_cn = total / (2 * num_cn) if num_cn > 0 else 1.0\n",
    "    weight_ad = total / (2 * num_ad) if num_ad > 0 else 1.0\n",
    "\n",
    "    class_weights = torch.tensor([weight_cn, weight_ad], device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Optimizer with parameter groups and different learning rates\n",
    "    # Higher learning rate for new/unfrozen layers, lower for pre-trained unfrozen layers\n",
    "    fc_params = list(model.resnet.fc.parameters())\n",
    "    other_params = [\n",
    "        p\n",
    "        for name, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(p is fc_param for fc_param in fc_params)\n",
    "    ]\n",
    "\n",
    "    # Set up parameter groups with different learning rates\n",
    "    param_groups = [\n",
    "        {\"params\": other_params, \"lr\": learning_rate},\n",
    "        {\n",
    "            \"params\": fc_params,\n",
    "            \"lr\": learning_rate * 10,\n",
    "        },  # Higher learning rate for final layer\n",
    "    ]\n",
    "\n",
    "    optimizer = optim.AdamW(param_groups, lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "    # Learning rate scheduler with cosine annealing for better convergence\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=1, eta_min=learning_rate / 100\n",
    "    )\n",
    "\n",
    "    # Early stopping implementation\n",
    "    patience = 5\n",
    "    early_stopping_counter = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, epoch)\n",
    "\n",
    "        # Update learning rate based on scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save the best model by validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                },\n",
    "                \"best_model_acc.pth\",\n",
    "            )\n",
    "            print(\"Model saved (best accuracy)!\")\n",
    "\n",
    "            # Log best model as artifact\n",
    "            artifact = wandb.Artifact(\"best_model_acc\", type=\"model\")\n",
    "            artifact.add_file(\"best_model_acc.pth\")\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "            # Reset early stopping counter on improvement\n",
    "            early_stopping_counter = 0\n",
    "\n",
    "        # Save best model by validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                },\n",
    "                \"best_model_loss.pth\",\n",
    "            )\n",
    "            print(\"Model saved (best loss)!\")\n",
    "\n",
    "            # Log best model as artifact\n",
    "            artifact = wandb.Artifact(\"best_model_loss\", type=\"model\")\n",
    "            artifact.add_file(\"best_model_loss.pth\")\n",
    "            wandb.log_artifact(artifact)\n",
    "        else:\n",
    "            # Increment early stopping counter\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        if device.type == \"mps\":\n",
    "            empty_cache()\n",
    "\n",
    "    # After training, load best model for final evaluation\n",
    "    checkpoint = torch.load(\"best_model_acc.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(\n",
    "        f\"Loaded best model from epoch {checkpoint['epoch']+1} with accuracy {checkpoint['val_acc']:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Final evaluation on validation set\n",
    "    final_val_loss, final_val_acc = validate(\n",
    "        model, val_loader, criterion, device, num_epochs\n",
    "    )\n",
    "    print(f\"Final validation accuracy: {final_val_acc:.2f}%\")\n",
    "\n",
    "    # Log final model summary\n",
    "    wandb.run.summary[\"best_val_acc\"] = best_val_acc\n",
    "    wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "    wandb.run.summary[\"final_val_acc\"] = final_val_acc\n",
    "    wandb.run.summary[\"final_val_loss\"] = final_val_loss\n",
    "    wandb.run.summary[\"total_epochs\"] = epoch + 1\n",
    "\n",
    "    # Close wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
