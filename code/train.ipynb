{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio nibabel numpy tqdm\n",
    "\n",
    "DATASET = \"./DATA/ADNI_SPLIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.backends.mps.is_available())  # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.video as models\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if Metal is available on macOS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal) device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available, using CPU\")\n",
    "\n",
    "\n",
    "# Dataset class for loading .nii.gz files\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Get all files from AD and CN directories\n",
    "        ad_dir = os.path.join(root_dir, split, \"AD\")\n",
    "        cn_dir = os.path.join(root_dir, split, \"CN\")\n",
    "\n",
    "        # Load AD samples (label 1)\n",
    "        for file in os.listdir(ad_dir):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                self.samples.append(os.path.join(ad_dir, file))\n",
    "                self.labels.append(1)  # AD class\n",
    "\n",
    "        # Load CN samples (label 0)\n",
    "        for file in os.listdir(cn_dir):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                self.samples.append(os.path.join(cn_dir, file))\n",
    "                self.labels.append(0)  # CN class\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} samples for {split} split\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the .nii.gz file\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image using nibabel\n",
    "        img = nib.load(img_path)\n",
    "        img_data = img.get_fdata()\n",
    "\n",
    "        # Normalize to [0, 1] if not already\n",
    "        if img_data.max() > 1.0:\n",
    "            img_data = img_data / img_data.max()\n",
    "\n",
    "        # Center crop to 128x128x128 (optional - reduces memory requirements)\n",
    "        # Adjust these values based on your needs\n",
    "        d, h, w = img_data.shape\n",
    "        d_center, h_center, w_center = d // 2, h // 2, w // 2\n",
    "        img_data = img_data[\n",
    "            max(0, d_center - 64) : min(d, d_center + 64),\n",
    "            max(0, h_center - 64) : min(h, h_center + 64),\n",
    "            max(0, w_center - 64) : min(w, w_center + 64),\n",
    "        ]\n",
    "\n",
    "        # Ensure the cropped size is exactly 128x128x128\n",
    "        current_d, current_h, current_w = img_data.shape\n",
    "        if current_d != 128 or current_h != 128 or current_w != 128:\n",
    "            temp = np.zeros((128, 128, 128))\n",
    "            temp[\n",
    "                : min(current_d, 128), : min(current_h, 128), : min(current_w, 128)\n",
    "            ] = img_data[\n",
    "                : min(current_d, 128), : min(current_h, 128), : min(current_w, 128)\n",
    "            ]\n",
    "            img_data = temp\n",
    "\n",
    "        # Convert to tensor and add channel dimension\n",
    "        img_tensor = torch.tensor(img_data, dtype=torch.float32).unsqueeze(\n",
    "            0\n",
    "        )  # Add channel dim\n",
    "\n",
    "        return img_tensor, label\n",
    "\n",
    "\n",
    "# Modified 3D ResNet model\n",
    "class MRIModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MRIModel, self).__init__()\n",
    "        # Using a video ResNet and modifying it for 3D MRI\n",
    "        # Fix the deprecation warning by using weights parameter\n",
    "        self.resnet = models.r3d_18(weights=models.R3D_18_Weights.KINETICS400_V1)\n",
    "\n",
    "        # Replace the first layer to accept single-channel input instead of 3\n",
    "        self.resnet.stem[0] = nn.Conv3d(\n",
    "            1,\n",
    "            64,\n",
    "            kernel_size=(3, 7, 7),\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(1, 3, 3),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # Replace the final fully connected layer for binary classification\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, 1, D, H, W)\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parameters\n",
    "    data_root = DATASET  # Update this to your dataset path\n",
    "    batch_size = 2  # Reduced batch size for memory constraints\n",
    "    num_epochs = 5\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = MRIDataset(data_root, split=\"train\")\n",
    "    val_dataset = MRIDataset(data_root, split=\"val\")\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MRIModel(num_classes=2)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # We're skipping torch.compile which doesn't work well with MPS\n",
    "    print(\"Using standard model without compilation for MPS compatibility\")\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
