{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow nibabel dicom2nifti nilearn matplotlib numpy antspyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import dicom2nifti\n",
    "import ants\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import nilearn.plotting as plotting\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nii_files(base_dir, prefix=None):\n",
    "    \"\"\"Retrieve all NIfTI file paths in the directory, optionally filtering by prefix.\"\"\"\n",
    "    found = []\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                if prefix:\n",
    "                    if file.startswith(prefix):\n",
    "                        found.append(file_path)\n",
    "                else:\n",
    "                    found.append(file_path)\n",
    "\n",
    "    return found\n",
    "\n",
    "\n",
    "def display_nii_stats(base_dir):\n",
    "    \"\"\"Display statistics about NIfTI files in the directory\"\"\"\n",
    "    nii_files = get_nii_files(base_dir)\n",
    "    prefix_counts = {}\n",
    "\n",
    "    for file in nii_files:\n",
    "        filename = os.path.basename(file)\n",
    "        prefix = filename.split(\"_\")[0]\n",
    "        if prefix in prefix_counts:\n",
    "            prefix_counts[prefix] += 1\n",
    "        else:\n",
    "            prefix_counts[prefix] = 1\n",
    "\n",
    "    total_files = len(nii_files)\n",
    "\n",
    "    # Display prefix counts in a pandas grid\n",
    "    prefix_data = {\n",
    "        \"Prefix\": list(prefix_counts.keys()),\n",
    "        \"Count\": list(prefix_counts.values()),\n",
    "    }\n",
    "    prefix_df = pd.DataFrame(prefix_data)\n",
    "    prefix_df = prefix_df.sort_values(by=\"Count\", ascending=False)\n",
    "    prefix_df.loc[\"Total\"] = prefix_df.sum(numeric_only=True)\n",
    "    print(\"\\nGrid Display for Prefix Counts:\")\n",
    "    display(prefix_df)\n",
    "\n",
    "\n",
    "def display_comprehensive_stats(base_dir, prefix=\"\"):\n",
    "    \"\"\"Display comprehensive statistics about all NIfTI files starting with the given prefix.\"\"\"\n",
    "    display_nii_stats(base_dir)\n",
    "\n",
    "    print(f\"Analysing files with prefix '{prefix}'\")\n",
    "    nii_files = get_nii_files(base_dir, prefix=prefix)\n",
    "\n",
    "    if not nii_files:\n",
    "        if prefix:\n",
    "            print(f\"No files found with prefix '{prefix}'.\")\n",
    "        else:\n",
    "            print(\"No files found.\")\n",
    "        return\n",
    "\n",
    "    # Plot the first NIfTI file\n",
    "    first_img = nib.load(nii_files[0])\n",
    "    plotting.plot_anat(first_img, title=f\"Displaying: {nii_files[0]}\")\n",
    "    plotting.show()\n",
    "\n",
    "    total_files = len(nii_files)\n",
    "    dimensions = []\n",
    "    voxel_sizes = []\n",
    "    orientations = []\n",
    "\n",
    "    for file in nii_files:\n",
    "        try:\n",
    "            img = nib.load(file)\n",
    "            dimensions.append(img.shape)\n",
    "            voxel_sizes.append(img.header.get_zooms())\n",
    "            orientations.append(nib.aff2axcodes(img.affine))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    unique_dimensions = {dim: dimensions.count(dim) for dim in set(dimensions)}\n",
    "    unique_voxel_sizes = {size: voxel_sizes.count(size) for size in set(voxel_sizes)}\n",
    "    unique_orientations = {\n",
    "        orient: orientations.count(orient) for orient in set(orientations)\n",
    "    }\n",
    "\n",
    "    # Display in grids\n",
    "    dim_data = {\n",
    "        \"Dimension\": list(unique_dimensions.keys()),\n",
    "        \"Frequency\": list(unique_dimensions.values()),\n",
    "    }\n",
    "    dim_df = pd.DataFrame(dim_data)\n",
    "    dim_df[\"Percentage\"] = (dim_df[\"Frequency\"] / total_files * 100).round(1)\n",
    "    dim_df = dim_df.sort_values(by=\"Frequency\", ascending=False)\n",
    "    print(\"\\nGrid Display for Dimensions:\")\n",
    "    display(dim_df)\n",
    "\n",
    "    voxel_data = {\n",
    "        \"Voxel Size\": list(unique_voxel_sizes.keys()),\n",
    "        \"Frequency\": list(unique_voxel_sizes.values()),\n",
    "    }\n",
    "    voxel_df = pd.DataFrame(voxel_data)\n",
    "    voxel_df[\"Percentage\"] = (voxel_df[\"Frequency\"] / total_files * 100).round(1)\n",
    "    voxel_df = voxel_df.sort_values(by=\"Frequency\", ascending=False)\n",
    "    print(\"\\nGrid Display for Voxel Sizes:\")\n",
    "    display(voxel_df)\n",
    "\n",
    "    orient_data = {\n",
    "        \"Orientation\": list(unique_orientations.keys()),\n",
    "        \"Frequency\": list(unique_orientations.values()),\n",
    "    }\n",
    "    orient_df = pd.DataFrame(orient_data)\n",
    "    orient_df[\"Percentage\"] = (orient_df[\"Frequency\"] / total_files * 100).round(1)\n",
    "    orient_df = orient_df.sort_values(by=\"Frequency\", ascending=False)\n",
    "    print(\"\\nGrid Display for Orientations:\")\n",
    "    display(orient_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dicom to NII\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dicom_to_nifti(base_dir):\n",
    "    \"\"\"Converts DICOM files to NIfTI while preserving folder structure.\"\"\"\n",
    "    # Pre-scan directories to convert\n",
    "    dirs_to_convert = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        if any(f.endswith(\".dcm\") for f in files):\n",
    "            dirs_to_convert.append((root, files))\n",
    "\n",
    "    # Process directories with a progress bar\n",
    "    for root, files in tqdm(\n",
    "        dirs_to_convert, desc=\"Converting DICOM to NIfTI\", unit=\"folder\"\n",
    "    ):\n",
    "        nii_output_dir = root  # Save in the same directory as DICOMs\n",
    "        nii_output_path = os.path.join(nii_output_dir, \"scan.nii.gz\")\n",
    "\n",
    "        if not os.path.exists(nii_output_path):  # Avoid redundant conversion\n",
    "            try:\n",
    "                dicom2nifti.convert_directory(\n",
    "                    root, nii_output_dir, compression=True, reorient=True\n",
    "                )\n",
    "                print(f\"\\nConverted: {root} -> {nii_output_path}\")\n",
    "\n",
    "                # Remove DICOM files after conversion\n",
    "                for file in files:\n",
    "                    os.remove(os.path.join(root, file))\n",
    "            except Exception as e:\n",
    "                print(f\"\\nFailed to convert {root}: {e}\")\n",
    "\n",
    "\n",
    "convert_dicom_to_nifti(\"./data/adni-2-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rename to image id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these paths as needed\n",
    "base_dir = \"./data/adni-2-4\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".nii.gz\"):\n",
    "            # The immediate directory of the file\n",
    "            parent_dir = os.path.basename(root)\n",
    "            source_path = os.path.join(root, file)\n",
    "\n",
    "            # Rename the file to parent's name (.nii.gz)\n",
    "            target_filename = f\"{parent_dir}.nii.gz\"\n",
    "            target_path = os.path.join(base_dir, target_filename)\n",
    "\n",
    "            print(f\"Moving {source_path} to {target_path}\")\n",
    "            shutil.move(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepend subject id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_subject_id_single_dir(data_dir, xml_dir):\n",
    "    # Create mapping from scan ID to subject ID\n",
    "    scan_to_subject = {}\n",
    "\n",
    "    # Parse XML filenames to extract subject ID and scan ID\n",
    "    print(\"Creating scan-to-subject mapping...\")\n",
    "    for xml_file in os.listdir(xml_dir):\n",
    "        if xml_file.endswith(\".xml\"):\n",
    "            # From format like \"ADNI_013_S_0575_MPRAGE_S28210_I44926.xml\"\n",
    "            # Extract the 013_S_0575 (subject ID) and I44926 (scan ID)\n",
    "            parts = xml_file.split(\"_\")\n",
    "            if len(parts) >= 6 and parts[-1].startswith(\"I\"):\n",
    "                scan_id = parts[-1].split(\".\")[0]  # e.g., \"I44926\"\n",
    "                subject_id = \"_\".join(parts[1:4])  # e.g., \"013_S_0575\"\n",
    "                scan_to_subject[scan_id] = subject_id\n",
    "\n",
    "    print(f\"Found {len(scan_to_subject)} scan-to-subject mappings\")\n",
    "\n",
    "    # Process all nii.gz files in the provided data_dir\n",
    "    for file_path in glob.glob(os.path.join(data_dir, \"*.nii.gz\")):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        scan_id = file_name.split(\".\")[0]  # e.g., \"I44926\"\n",
    "\n",
    "        if scan_id in scan_to_subject:\n",
    "            subject_id = scan_to_subject[scan_id]\n",
    "            new_name = f\"{subject_id}_{file_name}\"\n",
    "            new_path = os.path.join(data_dir, new_name)\n",
    "            print(f\"Renaming {file_name} to {new_name}\")\n",
    "            os.rename(file_path, new_path)\n",
    "        else:\n",
    "            print(f\"Could not find subject ID for scan {scan_id}\")\n",
    "\n",
    "    print(\"File renaming complete!\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "prepend_subject_id_single_dir(\"./data/adni-2-4\", \"./data/metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by research group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_research_group(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for elem in root.iter(\"researchGroup\"):\n",
    "            return elem.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {xml_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def restructure_dataset(data_dir, metadata_dir, output_dir):\n",
    "    # Create output subdirectories for AD and CN\n",
    "    for group in [\"AD\", \"CN\"]:\n",
    "        os.makedirs(os.path.join(output_dir, group), exist_ok=True)\n",
    "\n",
    "    # Process each nii.gz file in the data_dir\n",
    "    print(\"Processing nii.gz files...\")\n",
    "    for file_path in glob.glob(os.path.join(data_dir, \"*.nii.gz\")):\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        # Assume filename format: subjectID_imageID.nii.gz; extract imageID\n",
    "        parts = file_name.split(\"_\")\n",
    "        # If the image ID is at the end (removing extension)\n",
    "        image_id = parts[-1].split(\".\")[0] if parts else file_name.split(\".\")[0]\n",
    "\n",
    "        # Locate corresponding XML file in the metadata_dir\n",
    "        xml_file = None\n",
    "        for candidate in os.listdir(metadata_dir):\n",
    "            if candidate.endswith(f\"{image_id}.xml\"):\n",
    "                xml_file = candidate\n",
    "                break\n",
    "\n",
    "        if xml_file:\n",
    "            xml_path = os.path.join(metadata_dir, xml_file)\n",
    "            group = get_research_group(xml_path)\n",
    "            if group in {\"AD\", \"CN\"}:\n",
    "                dest_path = os.path.join(output_dir, group, file_name)\n",
    "                print(f\"Copying {file_name} to {group} folder\")\n",
    "                shutil.copy(file_path, dest_path)\n",
    "            else:\n",
    "                print(f\"Research group for {file_name} is invalid: {group}\")\n",
    "        else:\n",
    "            print(f\"No XML found for {file_name}\")\n",
    "\n",
    "    print(\"Restructuring complete!\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "restructure_dataset(\"./data/adni-2-4\", \"./data/metadata\", \"./data/adni-2-4-cond\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter files by subject spread\n",
    "\n",
    "I used this to get a certain number of files from the CN folder because I wanted to make the split 50/50 which meant only picking 107 files, but I wanted them to come from as wide an arrray of subjects as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files_wide_spread(data_dir, output_dir, count=100):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get list of nii.gz files in data_dir\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.nii.gz\"))\n",
    "\n",
    "    # Group files by subject id (subject id is everything up to the last underscore)\n",
    "    subject_files = {}\n",
    "    for file_path in files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) < 2:\n",
    "            continue  # Skip files that do not follow the expected naming format.\n",
    "        # Subject id is defined as everything until the last underscore.\n",
    "        subject_id = \"_\".join(parts[:-1])\n",
    "        subject_files.setdefault(subject_id, []).append(file_path)\n",
    "\n",
    "    selected_files = []\n",
    "    subjects = list(subject_files.keys())\n",
    "\n",
    "    # Case 1: We have at least \"count\" distinct subjects.\n",
    "    if len(subjects) >= count:\n",
    "        # Randomly choose count subjects and from each choose one random file.\n",
    "        chosen_subjects = random.sample(subjects, count)\n",
    "        for subj in chosen_subjects:\n",
    "            chosen_file = random.choice(subject_files[subj])\n",
    "            selected_files.append(chosen_file)\n",
    "    else:\n",
    "        # Case 2: fewer than count subjects.\n",
    "        # First, select one file from each subject.\n",
    "        for subj in subjects:\n",
    "            chosen_file = random.choice(subject_files[subj])\n",
    "            selected_files.append(chosen_file)\n",
    "\n",
    "        # Now fill up remainder from subjects that have extra files.\n",
    "        remaining = count - len(selected_files)\n",
    "        # Prepare a list of iterators for subjects with more than one file.\n",
    "        extra_files = []\n",
    "        for subj in subjects:\n",
    "            # Add extra files (exclude the one already used)\n",
    "            files_for_subj = subject_files[subj][:]\n",
    "            if len(files_for_subj) > 1:\n",
    "                # Remove the file already selected (if present)\n",
    "                file_already = selected_files.pop(\n",
    "                    0\n",
    "                )  # this line ensures we don't accidentally select the same file by accident.\n",
    "                files_for_subj = [f for f in subject_files[subj] if f != file_already]\n",
    "                selected_files.insert(0, file_already)\n",
    "                extra_files.append(files_for_subj)\n",
    "            else:\n",
    "                extra_files.append([])\n",
    "\n",
    "        # Flatten extra_files while preserving the order of subjects\n",
    "        # Use round-robin style selection.\n",
    "        added = 0\n",
    "        while added < remaining:\n",
    "            any_added = False\n",
    "            for subj, files_list in zip(subjects, extra_files):\n",
    "                if files_list:\n",
    "                    selected_files.append(files_list.pop(0))\n",
    "                    added += 1\n",
    "                    any_added = True\n",
    "                    if added >= remaining:\n",
    "                        break\n",
    "            if not any_added:\n",
    "                # No more extra files available.\n",
    "                break\n",
    "\n",
    "    # Copy the selected files to the output directory\n",
    "    for file_path in selected_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        dest_path = os.path.join(output_dir, file_name)\n",
    "        print(f\"Copying {file_name} to {output_dir}\")\n",
    "        shutil.copy(file_path, dest_path)\n",
    "\n",
    "    print(\"File filtering complete!\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "filter_files_wide_spread(\n",
    "    \"./data/adni-2-4-cond/CN\", \"./data/adni-2-4-cond/CN-filtered\", count=107\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skull Stripping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_synthstrip(freesurfer_home, input_path, ss_output_path):\n",
    "    \"\"\"Runs SynthStrip on a single NIfTI file.\"\"\"\n",
    "    if os.path.exists(ss_output_path):  # Avoid redundant processing\n",
    "        print(f\"Skipping {input_path}, output already exists.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        env = os.environ.copy()\n",
    "        env[\"FREESURFER_HOME\"] = freesurfer_home\n",
    "        env[\"SUBJECTS_DIR\"] = os.path.join(freesurfer_home, \"subjects\")\n",
    "\n",
    "        command = [\n",
    "            \"/bin/bash\",\n",
    "            \"-c\",  # Use bash explicitly\n",
    "            f\"source {freesurfer_home}/SetUpFreeSurfer.sh && \"\n",
    "            f\"mri_synthstrip -i {input_path} -o {ss_output_path}\",\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "        subprocess.run(command, check=True, env=env)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(\n",
    "            f\"✔ Processed: {input_path} -> {ss_output_path} (Time: {elapsed_time:.2f}s)\"\n",
    "        )\n",
    "\n",
    "        # Delete the original file after successful processing\n",
    "        os.remove(input_path)\n",
    "        print(f\"🗑️ Deleted original file: {input_path}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed to process {input_path}: {e}\")\n",
    "\n",
    "\n",
    "def skull_strip_nifti(base_dir, freesurfer_home=\"/Applications/freesurfer/7.4.1\"):\n",
    "    \"\"\"Runs SynthStrip on NIfTI files sequentially while preserving folder structure.\"\"\"\n",
    "    tasks = []\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".nii.gz\") and not file.startswith(\"ss_\"):\n",
    "                input_path = os.path.join(root, file)\n",
    "                ss_output_path = os.path.join(root, \"ss_\" + file)\n",
    "                tasks.append((freesurfer_home, input_path, ss_output_path))\n",
    "\n",
    "    total_tasks = len(tasks)\n",
    "    if total_tasks == 0:\n",
    "        print(\"✅ No new NIfTI files to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🔍 Found {total_tasks} files to process.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process tasks sequentially\n",
    "    for i, task in enumerate(tasks):\n",
    "        print(f\"[{i+1}/{total_tasks}] Processing: {task[1]}\")\n",
    "        try:\n",
    "            run_synthstrip(*task)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {task[1]}: {e}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"✅ Finished processing all files in {elapsed_time:.2f}s.\")\n",
    "\n",
    "\n",
    "skull_strip_nifti(\"./data/adni-2-4-cond\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the Stripped Scans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_files = get_nii_files(DATA, \"ss_\")\n",
    "\n",
    "# Display the first few NIfTI file paths\n",
    "print(\"First few NIfTI files:\")\n",
    "for file in ss_files[:5]:\n",
    "    print(file)\n",
    "\n",
    "# Visualize all NIfTI scans\n",
    "for scan in ss_files[::50]:\n",
    "    img = nib.load(scan)\n",
    "    plotting.plot_anat(\n",
    "        img,\n",
    "        title=f\"Anatomical View: {scan}\",\n",
    "        annotate=False,\n",
    "        draw_cross=False,\n",
    "        cut_coords=(0, 0, 0),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orientation Standardisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_las_orientation(file_list):\n",
    "    \"\"\"\n",
    "    Checks if each NIfTI file in the provided list is in LAS+ orientation.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): List of file paths to .nii.gz files.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the number of LAS+ files and lists any files that are not in LAS+.\n",
    "    \"\"\"\n",
    "    las_count = 0\n",
    "    non_las_files = []\n",
    "\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            img = nib.load(file)\n",
    "            original_orientation = nib.aff2axcodes(img.affine)\n",
    "\n",
    "            if original_orientation == (\"L\", \"A\", \"S\"):\n",
    "                las_count += 1\n",
    "            else:\n",
    "                non_las_files.append((file, original_orientation))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal LAS+ files: {las_count}/{len(file_list)}\")\n",
    "\n",
    "    if non_las_files:\n",
    "        print(\"\\nFiles not in LAS+ orientation:\")\n",
    "        for file, orientation in non_las_files:\n",
    "            print(f\"{file}: {orientation}\")\n",
    "    else:\n",
    "        print(\"All files are already in LAS+ orientation.\")\n",
    "\n",
    "\n",
    "nii_files = get_nii_files(DATA)\n",
    "\n",
    "check_las_orientation(nii_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't need it cuz it's applied at augementations step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop and Reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_nifti(file_path):\n",
    "    \"\"\"\n",
    "    Safely load NIfTI file with minimal fallback methods.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = nib.load(file_path)\n",
    "        return img.get_fdata(), img.affine, img.header\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {file_path}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def crop_brain_from_mri(img_data, padding=3):\n",
    "    \"\"\"\n",
    "    Crop out empty space around the brain in 3D MRI scans.\n",
    "    \"\"\"\n",
    "    # Use a low threshold to capture brain tissue while excluding noise\n",
    "    mask = img_data > np.mean(img_data) * 0.1\n",
    "    coords = np.argwhere(mask)\n",
    "\n",
    "    # If no significant tissue is found, return original image\n",
    "    if len(coords) == 0:\n",
    "        return img_data, (\n",
    "            (0, img_data.shape[0]),\n",
    "            (0, img_data.shape[1]),\n",
    "            (0, img_data.shape[2]),\n",
    "        )\n",
    "\n",
    "    mins = coords.min(axis=0)\n",
    "    maxs = coords.max(axis=0)\n",
    "    cropped_mins = [max(0, m - padding) for m in mins]\n",
    "    cropped_maxs = [min(img_data.shape[i], m + padding) for i, m in enumerate(maxs)]\n",
    "\n",
    "    cropped_img = img_data[\n",
    "        cropped_mins[0] : cropped_maxs[0],\n",
    "        cropped_mins[1] : cropped_maxs[1],\n",
    "        cropped_mins[2] : cropped_maxs[2],\n",
    "    ]\n",
    "\n",
    "    crop_coords = (\n",
    "        (cropped_mins[0], cropped_maxs[0]),\n",
    "        (cropped_mins[1], cropped_maxs[1]),\n",
    "        (cropped_mins[2], cropped_maxs[2]),\n",
    "    )\n",
    "    return cropped_img, crop_coords\n",
    "\n",
    "\n",
    "def preprocess_crop_and_reshape_mri(file_path, target_shape, padding=3):\n",
    "    \"\"\"\n",
    "    Load, crop, and reshape (by interpolation) an MRI scan to a target shape.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the MRI scan file (.nii.gz).\n",
    "    target_shape : tuple of ints\n",
    "        Desired output shape after cropping and reshaping.\n",
    "    padding : int, optional\n",
    "        Additional padding around the brain region (default: 3 voxels).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    final_img : numpy.ndarray\n",
    "        Processed image data after cropping and reshaping.\n",
    "    crop_coords : tuple\n",
    "        Coordinates of the crop used.\n",
    "    affine : numpy.ndarray\n",
    "        The affine transform of the original scan.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img_data, affine, header = safe_load_nifti(file_path)\n",
    "    if img_data is None:\n",
    "        return None, None, None\n",
    "\n",
    "    # Crop the brain\n",
    "    cropped_img, crop_coords = crop_brain_from_mri(img_data, padding=padding)\n",
    "\n",
    "    # Reshape using cubic interpolation if the shape differs\n",
    "    if cropped_img.shape != target_shape:\n",
    "        zoom_factors = [t / s for t, s in zip(target_shape, cropped_img.shape)]\n",
    "        final_img = zoom(cropped_img, zoom_factors, order=3)\n",
    "    else:\n",
    "        final_img = cropped_img\n",
    "\n",
    "    return final_img, crop_coords, affine\n",
    "\n",
    "\n",
    "# Example usage within a batch processing function\n",
    "def batch_preprocess_mri_dataset(input_dir, output_dir, target_shape, padding=3):\n",
    "    \"\"\"\n",
    "    Batch process MRI scans by cropping and reshaping, then save the results.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    processed_files = []\n",
    "    failed_files = []\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".nii.gz\"):\n",
    "                input_path = os.path.join(root, filename)\n",
    "                relative_path = os.path.relpath(root, input_dir)\n",
    "                output_subdir = os.path.join(output_dir, relative_path)\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "                output_path = os.path.join(output_subdir, filename)\n",
    "\n",
    "                try:\n",
    "                    processed_img, crop_coords, affine = (\n",
    "                        preprocess_crop_and_reshape_mri(\n",
    "                            input_path, target_shape, padding\n",
    "                        )\n",
    "                    )\n",
    "                    if processed_img is None:\n",
    "                        print(f\"Skipping {filename} due to loading error.\")\n",
    "                        failed_files.append(filename)\n",
    "                        continue\n",
    "\n",
    "                    # Save the processed image\n",
    "                    processed_nii = nib.Nifti1Image(processed_img, affine)\n",
    "                    nib.save(processed_nii, output_path)\n",
    "\n",
    "                    processed_files.append(filename)\n",
    "                    print(f\"Processed: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {filename}: {e}\")\n",
    "                    failed_files.append(filename)\n",
    "\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total files processed: {len(processed_files)}\")\n",
    "    print(f\"Total files failed: {len(failed_files)}\")\n",
    "    if failed_files:\n",
    "        print(\"Failed files:\")\n",
    "        for file in failed_files:\n",
    "            print(file)\n",
    "    return processed_files, failed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Normalisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasons I do NOT Need Spatial Normalization\n",
    "\n",
    "1. ADNI-1 T1W data is already preprocessed\n",
    "   - ADNI follows a standardized acquisition protocol, ensuring consistent voxel sizes and orientations across patients.\n",
    "   - If you’re only using ADNI-1 (no ADNI-2 or ADNI-3), there’s less variation in scanner settings, meaning alignment might already be sufficient.\n",
    "2. CNNs Learn Spatial Features\n",
    "   - If you're using a deep learning model (e.g., CNN), it can learn spatial variations on its own.\n",
    "   - Adding spatial normalization could remove subtle differences in brain shape that might be relevant for classification.\n",
    "3. Preserving Native Brain Shape\n",
    "   - Some models benefit from analyzing brain atrophy without forced alignment to MNI space.\n",
    "   - If you want to measure structural differences in their original form (e.g., hippocampal shrinkage), keeping scans in native space may be better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Field Correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all skull-stripped files\n",
    "nii_files = get_nii_files(DATA, \"ss_\")\n",
    "\n",
    "# Apply N4 Bias Field Correction to all files\n",
    "for file in nii_files:\n",
    "    bias_corrected_path = file.replace(\"ss_\", \"bc_ss_\")\n",
    "\n",
    "    if not os.path.exists(bias_corrected_path):\n",
    "        # Load the skull-stripped image\n",
    "        input_image = ants.image_read(file)\n",
    "\n",
    "        # Apply N4 Bias Field Correction\n",
    "        bias_corrected = ants.n4_bias_field_correction(input_image)\n",
    "\n",
    "        # Save the bias-corrected image\n",
    "        ants.image_write(bias_corrected, bias_corrected_path)\n",
    "        print(f\"Bias-corrected image saved to: {bias_corrected_path}\")\n",
    "    else:\n",
    "        print(f\"Skipping {file}, bias-corrected file already exists.\")\n",
    "\n",
    "# Load one bias-corrected image for comparison\n",
    "if len(nii_files) >= 1:\n",
    "    original_image = nib.load(nii_files[0])\n",
    "    bias_corrected_image = nib.load(nii_files[0].replace(\"ss_\", \"bc_ss_\"))\n",
    "\n",
    "    # Plot the middle slice of the original scan\n",
    "    plotting.plot_anat(original_image, title=\"Original Image\", cut_coords=(0, 0, 0))\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the middle slice of the bias-corrected scan\n",
    "    plotting.plot_anat(\n",
    "        bias_corrected_image, title=\"Bias-Corrected Image\", cut_coords=(0, 0, 0)\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    original_data = original_image.get_fdata()\n",
    "    bias_corrected_data = bias_corrected_image.get_fdata()\n",
    "\n",
    "    # Choose a slice index\n",
    "    slice_idx = original_data.shape[2] // 2  # Middle slice\n",
    "\n",
    "    # Compute absolute difference\n",
    "    difference = np.abs(\n",
    "        original_data[:, :, slice_idx] - bias_corrected_data[:, :, slice_idx]\n",
    "    )\n",
    "\n",
    "    # Plot difference heatmap\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(difference, cmap=\"hot\")\n",
    "    plt.colorbar(label=\"Intensity Difference\")\n",
    "    plt.title(\"Difference Map\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough files for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel Standardisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for uniform voxality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_uniform_voxel_size(file_list):\n",
    "    \"\"\"\n",
    "    Checks if all NIfTI files in the provided list have uniform voxel size.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): List of file paths to .nii.gz files.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the result of the check.\n",
    "    \"\"\"\n",
    "    voxel_size_counts = {}\n",
    "\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            img = nib.load(file)\n",
    "            voxel_size = img.header.get_zooms()\n",
    "            if voxel_size in voxel_size_counts:\n",
    "                voxel_size_counts[voxel_size] += 1\n",
    "            else:\n",
    "                voxel_size_counts[voxel_size] = 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if len(voxel_size_counts) == 1:\n",
    "        print(\"All files have uniform voxel size:\", list(voxel_size_counts.keys())[0])\n",
    "    else:\n",
    "        print(\"Files have different voxel sizes:\")\n",
    "        for voxel_size, count in voxel_size_counts.items():\n",
    "            print(f\"Voxel size: {voxel_size}, Count: {count}\")\n",
    "\n",
    "\n",
    "# Check voxel size for all NIfTI files\n",
    "nii_files = get_nii_files(DATA, prefix=\"bc_\")\n",
    "check_uniform_voxel_size(nii_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample Voxel Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(file_path, output_path, target=(1, 1, 1)):\n",
    "    \"\"\"Resample a NIfTI file to 1x1x1 mm voxel size using ANTs.\"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        img = ants.image_read(file_path)\n",
    "\n",
    "        # Resample the image to 1x1x1 mm voxel size\n",
    "        resampled_img = ants.resample_image(img, target, use_voxels=False)\n",
    "\n",
    "        # Save the resampled image\n",
    "        ants.image_write(resampled_img, output_path)\n",
    "        print(f\"Resampled and saved: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resampling {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def test_resample_single_image(file_path):\n",
    "    \"\"\"Test resampling on a single image and display the outputs.\"\"\"\n",
    "    resampled_file_path = file_path.replace(\"bc_\", \"resampled_bc_\")\n",
    "    resample(file_path, resampled_file_path)\n",
    "\n",
    "    # Load the original and resampled images\n",
    "    original_img = nib.load(file_path)\n",
    "    resampled_img = nib.load(resampled_file_path)\n",
    "\n",
    "    # Display the resolutions\n",
    "    original_resolution = original_img.header.get_zooms()\n",
    "    resampled_resolution = resampled_img.header.get_zooms()\n",
    "    print(f\"Original resolution: {original_resolution}\")\n",
    "    print(f\"Resampled resolution: {resampled_resolution}\")\n",
    "\n",
    "    # Plot the middle slice of the original scan\n",
    "    plotting.plot_anat(original_img, title=\"Original Image\", cut_coords=(0, 0, 0))\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the middle slice of the resampled scan\n",
    "    plotting.plot_anat(resampled_img, title=\"Resampled Image\", cut_coords=(0, 0, 0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test the resampling on a single image\n",
    "test_resample_single_image(nii_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_all_bc_files(base_dir):\n",
    "    \"\"\"Resample all bias-corrected NIfTI files in the directory to 1x1x1 mm voxel size.\"\"\"\n",
    "    bc_files = get_nii_files(base_dir, prefix=\"bc_\")\n",
    "    for file_path in bc_files:\n",
    "        resampled_file_path = file_path.replace(\"bc_\", \"resampled_bc_\")\n",
    "        if not os.path.exists(resampled_file_path):\n",
    "            resample(file_path, resampled_file_path)\n",
    "        else:\n",
    "            print(f\"Skipping {file_path}, resampled file already exists.\")\n",
    "\n",
    "\n",
    "resample_all_bc_files(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Preprocessing Clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_comprehensive_stats(DATA, \"resampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_resampled_files(base_dir):\n",
    "    \"\"\"Remove all files that don't begin with 'resampled_'.\"\"\"\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if not file.startswith(\"resampled_\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed: {file_path}\")\n",
    "\n",
    "\n",
    "remove_non_resampled_files(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_comprehensive_stats(DATA, \"resampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_as_directory(base_dir):\n",
    "    \"\"\"Rename all files in the directory to their immediate directory name.\"\"\"\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            immediate_dir = os.path.basename(root)\n",
    "            new_file_name = f\"{immediate_dir}.nii.gz\"\n",
    "            new_file_path = os.path.join(root, new_file_name)\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f\"Renamed: {file_path} -> {new_file_path}\")\n",
    "\n",
    "\n",
    "rename_files_as_directory(DATA)\n",
    "\n",
    "\n",
    "# Check for duplicate file names\n",
    "def check_duplicate_file_names(base_dir):\n",
    "    \"\"\"Check if any files in the directory have the same name.\"\"\"\n",
    "    file_names = {}\n",
    "    duplicates = []\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file in file_names:\n",
    "                duplicates.append(file)\n",
    "            else:\n",
    "                file_names[file] = root\n",
    "\n",
    "    if duplicates:\n",
    "        print(\"Duplicate file names found:\")\n",
    "        for file in duplicates:\n",
    "            print(file)\n",
    "    else:\n",
    "        print(\"No duplicate file names found.\")\n",
    "\n",
    "\n",
    "check_duplicate_file_names(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Split Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for group in [\"AD\", \"CN\"]:\n",
    "        os.makedirs(os.path.join(OUTPUT, split, group), exist_ok=True)\n",
    "\n",
    "\n",
    "# Function to get research group from XML\n",
    "def get_research_group(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for elem in root.iter(\"researchGroup\"):\n",
    "            return elem.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {xml_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Collect all NIfTI files and their corresponding research groups\n",
    "file_groups = {\"AD\": [], \"CN\": []}\n",
    "for root, _, files in os.walk(DATA):\n",
    "    for file in files:\n",
    "        if file.endswith(\".nii.gz\"):\n",
    "            image_id = file.split(\".\")[0]\n",
    "            # Find the corresponding XML file\n",
    "            xml_file = None\n",
    "            for xml in os.listdir(METADATA):\n",
    "                if xml.endswith(f\"{image_id}.xml\"):\n",
    "                    xml_file = xml\n",
    "                    break\n",
    "            if xml_file:\n",
    "                xml_path = os.path.join(METADATA, xml_file)\n",
    "                group = get_research_group(xml_path)\n",
    "                if group in file_groups:\n",
    "                    file_groups[group].append(os.path.join(root, file))\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_files = {\"AD\": [], \"CN\": []}\n",
    "val_files = {\"AD\": [], \"CN\": []}\n",
    "test_files = {\"AD\": [], \"CN\": []}\n",
    "\n",
    "for group in [\"AD\", \"CN\"]:\n",
    "    train, temp = train_test_split(file_groups[group], test_size=0.2, random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "    train_files[group].extend(train)\n",
    "    val_files[group].extend(val)\n",
    "    test_files[group].extend(test)\n",
    "\n",
    "\n",
    "# Function to copy files to the output directory\n",
    "def copy_files(file_list, split, group):\n",
    "    for nii_path in file_list:\n",
    "        nii_filename = os.path.basename(nii_path)\n",
    "        shutil.copy(nii_path, os.path.join(OUTPUT, split, group, nii_filename))\n",
    "\n",
    "\n",
    "# Copy files to the respective directories\n",
    "for group in [\"AD\", \"CN\"]:\n",
    "    copy_files(train_files[group], \"train\", group)\n",
    "    copy_files(val_files[group], \"val\", group)\n",
    "    copy_files(test_files[group], \"test\", group)\n",
    "\n",
    "print(\"Dataset split and creation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print dataset statistics\n",
    "def print_dataset_statistics():\n",
    "    total_ad = 0\n",
    "    total_cn = 0\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        ad_count = len(os.listdir(os.path.join(OUTPUT, split, \"AD\")))\n",
    "        cn_count = len(os.listdir(os.path.join(OUTPUT, split, \"CN\")))\n",
    "        total_ad += ad_count\n",
    "        total_cn += cn_count\n",
    "        print(f\"{split.capitalize()} set: AD={ad_count}, CN={cn_count}\")\n",
    "    print(f\"Total: AD={total_ad}, CN={total_cn}\")\n",
    "\n",
    "\n",
    "# Print dataset statistics\n",
    "print_dataset_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Current state before reshaping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_comprehensive_stats(OUTPUT_CROPPED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
