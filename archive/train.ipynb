{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.10\n",
    "\n",
    "%pip install torch torchvision torchaudio torchinfo nibabel numpy tqdm wandb torchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import torchio as tio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models.video as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nilearn import plotting\n",
    "from torchinfo import torchinfo\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from IPython.display import display\n",
    "\n",
    "device = (\n",
    "    torch.accelerator.current_accelerator().type\n",
    "    if torch.accelerator.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "if device == \"mps\":\n",
    "    # Empty CUDA cache periodically during training to avoid memory fragmentation\n",
    "    def empty_cache():\n",
    "        try:\n",
    "            # For newer PyTorch versions with MPS cache management\n",
    "            torch.mps.empty_cache()\n",
    "        except:\n",
    "            print(\"MPS cache management not available\")\n",
    "            pass  # Ignore if this function doesn't exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Scans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scans(dataset_path, split=\"train\", num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize the first few MRI scans from AD and CN directories.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset directory.\n",
    "        split (str): Dataset split to visualize ('train', 'val', 'test').\n",
    "        num_samples (int): Number of samples to visualize from each class.\n",
    "    \"\"\"\n",
    "    # Define directories for AD and CN\n",
    "    ad_dir = os.path.join(dataset_path, split, \"AD\")\n",
    "    cn_dir = os.path.join(dataset_path, split, \"CN\")\n",
    "\n",
    "    # Get the first few files from each directory\n",
    "    ad_files = [\n",
    "        os.path.join(ad_dir, f) for f in os.listdir(ad_dir) if f.endswith(\".nii.gz\")\n",
    "    ][:num_samples]\n",
    "    cn_files = [\n",
    "        os.path.join(cn_dir, f) for f in os.listdir(cn_dir) if f.endswith(\".nii.gz\")\n",
    "    ][:num_samples]\n",
    "\n",
    "    # Plot the first few AD scans\n",
    "    print(\"AD Scans:\")\n",
    "    for file in ad_files:\n",
    "        plotting.plot_anat(file, title=os.path.basename(file))\n",
    "    plotting.show()\n",
    "\n",
    "    # Plot the first few CN scans\n",
    "    print(\"CN Scans:\")\n",
    "    for file in cn_files:\n",
    "        plotting.plot_anat(file, title=os.path.basename(file))\n",
    "    plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        split=\"train\",\n",
    "        apply_augmentation=False,\n",
    "        target_size=(128, 128, 128),\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Validate inputs\n",
    "        if split not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError(\n",
    "                f\"Split must be one of 'train', 'val', 'test', got {split}\"\n",
    "            )\n",
    "\n",
    "        # Get all files from AD and CN directories\n",
    "        ad_dir = os.path.join(root_dir, split, \"AD\")\n",
    "        cn_dir = os.path.join(root_dir, split, \"CN\")\n",
    "\n",
    "        if not os.path.exists(ad_dir):\n",
    "            raise FileNotFoundError(f\"AD directory not found at {ad_dir}\")\n",
    "        if not os.path.exists(cn_dir):\n",
    "            raise FileNotFoundError(f\"CN directory not found at {cn_dir}\")\n",
    "\n",
    "        self._load_samples(ad_dir, cn_dir)\n",
    "        self._setup_transforms()\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} samples for {split} split\")\n",
    "        print(f\"Augmentation applied: {apply_augmentation}\")\n",
    "\n",
    "    def _load_samples(self, ad_dir, cn_dir):\n",
    "        \"\"\"Load samples from AD and CN directories\"\"\"\n",
    "        # Load AD samples (label 1)\n",
    "        ad_files = [f for f in os.listdir(ad_dir) if f.endswith(\".nii.gz\")]\n",
    "        for file in ad_files:\n",
    "            self.samples.append(os.path.join(ad_dir, file))\n",
    "            self.labels.append(1)  # AD class\n",
    "\n",
    "        # Load CN samples (label 0)\n",
    "        cn_files = [f for f in os.listdir(cn_dir) if f.endswith(\".nii.gz\")]\n",
    "        for file in cn_files:\n",
    "            self.samples.append(os.path.join(cn_dir, file))\n",
    "            self.labels.append(0)  # CN class\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(f\"No .nii.gz files found in {ad_dir} or {cn_dir}\")\n",
    "\n",
    "    def _setup_transforms(self):\n",
    "        if self.apply_augmentation:\n",
    "            self.transforms = tio.Compose(\n",
    "                [\n",
    "                    # Randomly flip images along the left-right axis (axis 1 relative to channel-first data)\n",
    "                    tio.RandomFlip(axes=(1,), p=0.5),\n",
    "                    # Apply slight affine transformations: modest scaling, rotation (±5°),\n",
    "                    # and translation limited to the 3-voxel padding.\n",
    "                    tio.RandomAffine(\n",
    "                        scales=(0.9, 1.1),\n",
    "                        degrees=10,\n",
    "                        translation=3.0,  # in mm; max translation matches the 3 voxel padding\n",
    "                        p=0.75,\n",
    "                    ),\n",
    "                    # Introduce subtle elastic deformations to simulate anatomical variability.\n",
    "                    tio.RandomElasticDeformation(\n",
    "                        num_control_points=7,\n",
    "                        max_displacement=3.0,  # limit displacement to avoid excessive warping\n",
    "                        p=0.3,\n",
    "                    ),\n",
    "                    # Add slight noise reflecting scanner variability.\n",
    "                    tio.RandomNoise(mean=0.0, std=0.1, p=0.3),\n",
    "                    # Adjust intensity minimally using gamma correction.\n",
    "                    tio.RandomGamma(log_gamma=(-0.2, 0.2), p=0.3),\n",
    "                    # Normalize intensities to zero mean and unit variance.\n",
    "                    tio.ZNormalization(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transforms = tio.Compose([tio.ZNormalization()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the .nii.gz file\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            # Load image using nibabel\n",
    "            img = nib.load(img_path)\n",
    "            img_data = img.get_fdata()\n",
    "\n",
    "            # Validate image dimensions\n",
    "            expected_d, expected_h, expected_w = self.target_size\n",
    "            current_d, current_h, current_w = img_data.shape\n",
    "\n",
    "            if (\n",
    "                current_d != expected_d\n",
    "                or current_h != expected_h\n",
    "                or current_w != expected_w\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Expected image size {expected_d}x{expected_h}x{expected_w} \"\n",
    "                    f\"but got {current_d}x{current_h}x{current_w} for {img_path}\"\n",
    "                )\n",
    "\n",
    "            # Add channel dimension to numpy array\n",
    "            img_data = np.expand_dims(img_data, axis=0)\n",
    "\n",
    "            # Apply transforms\n",
    "            img_data = self.transforms(img_data)\n",
    "\n",
    "            # Convert to tensor if not already a tensor\n",
    "            if not isinstance(img_data, torch.Tensor):\n",
    "                img_data = torch.tensor(img_data, dtype=torch.float32)\n",
    "\n",
    "            # Ensure the label is also a tensor\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "            return img_data, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a default or raise the exception\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_and_loaders(data_root, batch_size, use_augmentation):\n",
    "    \"\"\"Create datasets and data loaders.\"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = MRIDataset(\n",
    "        data_root, split=\"train\", apply_augmentation=use_augmentation\n",
    "    )\n",
    "    val_dataset = MRIDataset(data_root, split=\"val\", apply_augmentation=False)\n",
    "    test_dataset = MRIDataset(data_root, split=\"test\", apply_augmentation=False)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    dataset_stats = {\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"val_samples\": len(val_dataset),\n",
    "        \"test_samples\": len(test_dataset),\n",
    "        \"train_AD_samples\": train_dataset.labels.count(1),\n",
    "        \"train_CN_samples\": train_dataset.labels.count(0),\n",
    "        \"val_AD_samples\": val_dataset.labels.count(1),\n",
    "        \"val_CN_samples\": val_dataset.labels.count(0),\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        dataset_stats,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified 3D ResNet model with layer freezing\n",
    "class MRIModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_layers=True, architecture=\"r3d_18\"):\n",
    "        super(MRIModel, self).__init__()\n",
    "        # Using a video ResNet and modifying it for 3D MRI\n",
    "        if architecture == \"r3d_18\":\n",
    "            self.resnet = models.r3d_18(weights=models.R3D_18_Weights.KINETICS400_V1)\n",
    "        elif architecture == \"mc3_18\":\n",
    "            self.resnet = models.mc3_18(weights=models.MC3_18_Weights.KINETICS400_V1)\n",
    "        elif architecture == \"r2plus1d_18\":\n",
    "            self.resnet = models.r2plus1d_18(\n",
    "                weights=models.R2Plus1D_18_Weights.KINETICS400_V1\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported architecture: {architecture}\")\n",
    "\n",
    "        # Replace the first layer to accept single-channel input instead of 3\n",
    "        if architecture in [\"r3d_18\", \"mc3_18\"]:\n",
    "            self.resnet.stem[0] = nn.Conv3d(\n",
    "                1,\n",
    "                64,\n",
    "                kernel_size=(3, 7, 7),\n",
    "                stride=(1, 2, 2),\n",
    "                padding=(1, 3, 3),\n",
    "                bias=False,\n",
    "            )\n",
    "        elif architecture == \"r2plus1d_18\":\n",
    "            # R2Plus1D uses a slightly different stem structure\n",
    "            self.resnet.stem[0] = nn.Conv3d(\n",
    "                1,\n",
    "                45,  # This is the mid-channel count for R2Plus1D\n",
    "                kernel_size=(1, 7, 7),\n",
    "                stride=(1, 2, 2),\n",
    "                padding=(0, 3, 3),\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "        # Replace the final fully connected layer for binary classification\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        # Freeze specific layers if requested\n",
    "        if freeze_layers:\n",
    "            self._freeze_layers()\n",
    "\n",
    "    def _freeze_layers(self):\n",
    "        \"\"\"Freeze most layers of the ResNet model, leaving only layer4 and fc unfrozen\"\"\"\n",
    "        # Freeze stem and layers 1-3\n",
    "        # TODO loook at model in more detail and see where to freeze\n",
    "        for name, param in self.resnet.named_parameters():\n",
    "            if \"layer4\" not in name and \"fc\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def count_trainable_params(self):\n",
    "        \"\"\"Count and return trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def count_total_params(self):\n",
    "        \"\"\"Count and return total parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, 1, D, H, W)\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(freeze_layers, device, architecture):\n",
    "    \"\"\"Initialize and configure the model.\"\"\"\n",
    "    model = MRIModel(\n",
    "        num_classes=2, freeze_layers=freeze_layers, architecture=architecture\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Get parameter statistics\n",
    "    trainable_params = model.count_trainable_params()\n",
    "    total_params = model.count_total_params()\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    model_stats = {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"frozen_params\": frozen_params,\n",
    "        \"frozen_percentage\": frozen_params / total_params,\n",
    "    }\n",
    "\n",
    "    return model, model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_summary(model, input_size=(1, 1, 128, 128, 128), detailed=True):\n",
    "    \"\"\"\n",
    "    Display a comprehensive summary of the model architecture and parameters.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to analyze\n",
    "        input_size: The input tensor size (batch_size, channels, depth, height, width)\n",
    "        detailed: Whether to show detailed layer information\n",
    "    \"\"\"\n",
    "    # Get basic model summary using torchinfo\n",
    "    summary = torchinfo.summary(\n",
    "        model,\n",
    "        input_size=input_size,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    print(f\"MODEL ARCHITECTURE SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary)\n",
    "\n",
    "    # Count parameters by layer type\n",
    "    layer_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        layer_type = module.__class__.__name__\n",
    "        if layer_type not in layer_counts:\n",
    "            layer_counts[layer_type] = {\"count\": 0, \"params\": 0, \"trainable_params\": 0}\n",
    "\n",
    "        layer_counts[layer_type][\"count\"] += 1\n",
    "        params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "        trainable_params = sum(\n",
    "            p.numel() for p in module.parameters(recurse=False) if p.requires_grad\n",
    "        )\n",
    "\n",
    "        layer_counts[layer_type][\"params\"] += params\n",
    "        layer_counts[layer_type][\"trainable_params\"] += trainable_params\n",
    "\n",
    "    # Create detailed layer information dataframe\n",
    "    if detailed:\n",
    "        layers_info = []\n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Only leaf modules\n",
    "                params = sum(p.numel() for p in module.parameters())\n",
    "                trainable = sum(\n",
    "                    p.numel() for p in module.parameters() if p.requires_grad\n",
    "                )\n",
    "\n",
    "                layers_info.append(\n",
    "                    {\n",
    "                        \"Layer\": name,\n",
    "                        \"Type\": module.__class__.__name__,\n",
    "                        \"Parameters\": params,\n",
    "                        \"Trainable\": trainable,\n",
    "                        \"Frozen\": params - trainable,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Create and display DataFrame\n",
    "        df = pd.DataFrame(layers_info)\n",
    "        if not df.empty:\n",
    "            print(\"\\nDETAILED LAYER INFORMATION:\")\n",
    "            print(\"=\" * 80)\n",
    "            display(df)\n",
    "\n",
    "    # Show frozen vs trainable stats\n",
    "    total_params = model.count_total_params()\n",
    "    trainable_params = model.count_trainable_params()\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(\"\\nPARAMETER STATISTICS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total parameters:    {total_params:,}\")\n",
    "    print(\n",
    "        f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Frozen parameters:    {frozen_params:,} ({frozen_params/total_params*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Display model architecture as text\n",
    "    print(\"\\nMODEL ARCHITECTURE DETAILS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(model)\n",
    "\n",
    "    # Return summary for potential further use\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train model for one epoch with optimized PyTorch practices.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        dataloader: Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Computation device (CPU/GPU/MPS)\n",
    "        epoch: Current epoch number\n",
    "\n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        # Move data to device with non_blocking for potential performance gain\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = 100 * running_corrects / len(dataloader.dataset)\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"train_acc\": epoch_acc,\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if device.type == \"mps\":\n",
    "        empty_cache()\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    Validate the model and return performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to validate\n",
    "        dataloader: DataLoader containing validation data\n",
    "        criterion: Loss function\n",
    "        device: Device to run validation on (cuda/cpu)\n",
    "        epoch: Current training epoch\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (validation loss, validation accuracy, metrics dictionary)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    if dataloader.dataset.split == \"val\":\n",
    "        prefix = \"val\"\n",
    "        desc = f\"Validation Epoch {epoch+1}\"\n",
    "    else:\n",
    "        prefix = \"test\"\n",
    "        epoch = None\n",
    "        desc = \"Testing\"\n",
    "\n",
    "    # Collect predictions\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=desc):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Calculate metrics\n",
    "    loss = running_loss / len(dataloader.dataset)\n",
    "    metrics = compute_metrics(all_labels, all_preds, all_probs)\n",
    "    acc = metrics[\"accuracy\"] * 100  # Convert to percentage\n",
    "\n",
    "    log_to_wandb_dashboard(\n",
    "        all_labels, all_preds, all_probs, loss, metrics, epoch, prefix=prefix\n",
    "    )\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def compute_metrics(labels, preds, probs):\n",
    "    \"\"\"Calculate classification metrics using scikit-learn.\"\"\"\n",
    "\n",
    "    # Basic metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(labels, preds),\n",
    "    }\n",
    "\n",
    "    # Class-specific accuracy\n",
    "    for cls in np.unique(labels):\n",
    "        mask = labels == cls\n",
    "        if np.any(mask):\n",
    "            metrics[f\"class_{cls}_acc\"] = accuracy_score(labels[mask], preds[mask])\n",
    "        else:\n",
    "            metrics[f\"class_{cls}_acc\"] = 0\n",
    "\n",
    "    # Precision, recall, F1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    metrics.update(\n",
    "        {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Confusion matrix and derived metrics\n",
    "    cm = confusion_matrix(labels, preds, labels=[0, 1])\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, _, _ = cm.ravel()\n",
    "        metrics[\"specificity\"] = tn / (tn + fp + 1e-10)\n",
    "\n",
    "    # ROC AUC and PR AUC (with error handling)\n",
    "    try:\n",
    "        metrics[\"roc_auc\"] = roc_auc_score(labels, probs)\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc\"] = 0\n",
    "\n",
    "    try:\n",
    "        metrics[\"avg_precision\"] = average_precision_score(labels, probs)\n",
    "    except Exception:\n",
    "        metrics[\"avg_precision\"] = 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def log_to_wandb_dashboard(labels, preds, probs, loss, metrics, epoch, prefix=\"val\"):\n",
    "    \"\"\"Create visualizations and log metrics to W&B.\"\"\"\n",
    "\n",
    "    log_dict = {\n",
    "        f\"{prefix}_loss\": loss,\n",
    "        f\"{prefix}_acc\": metrics[\"accuracy\"] * 100,\n",
    "        f\"{prefix}_balanced_acc\": metrics[\"balanced_accuracy\"] * 100,\n",
    "        f\"{prefix}_CN_acc\": metrics[\"class_0_acc\"] * 100,\n",
    "        f\"{prefix}_AD_acc\": metrics[\"class_1_acc\"] * 100,\n",
    "        f\"{prefix}_precision\": metrics[\"precision\"],\n",
    "        f\"{prefix}_recall\": metrics[\"recall\"],\n",
    "        f\"{prefix}_specificity\": metrics.get(\"specificity\", 0),\n",
    "        f\"{prefix}_f1\": metrics[\"f1_score\"],\n",
    "        f\"{prefix}_roc_auc\": metrics[\"roc_auc\"],\n",
    "        f\"{prefix}_avg_precision\": metrics[\"avg_precision\"],\n",
    "    }\n",
    "\n",
    "    # Add epoch to log_dict only if it's not None\n",
    "    if epoch is not None:\n",
    "        log_dict[\"epoch\"] = epoch\n",
    "\n",
    "    # Create confusion matrix visualization\n",
    "    conf_matrix = wandb.plot.confusion_matrix(\n",
    "        preds=preds, y_true=labels, class_names=[\"CN\", \"AD\"]\n",
    "    )\n",
    "    log_dict[\"confusion_matrix\"] = conf_matrix\n",
    "\n",
    "    # ROC Curve\n",
    "    try:\n",
    "        roc_curve_plot = wandb.plot.roc_curve(\n",
    "            y_true=labels,\n",
    "            y_probas=np.stack([1 - probs, probs], axis=1),\n",
    "            labels=[\"CN\", \"AD\"],\n",
    "            classes_to_plot=[1],\n",
    "        )\n",
    "        log_dict[\"roc_curve\"] = roc_curve_plot\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating ROC curve: {e}\")\n",
    "        pass\n",
    "\n",
    "    # PR Curve\n",
    "    try:\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(labels, probs)\n",
    "        pr_data = [[x, y] for x, y in zip(recall_vals, precision_vals)]\n",
    "        pr_table = wandb.Table(data=pr_data, columns=[\"recall\", \"precision\"])\n",
    "        pr_plot = wandb.plot.line(\n",
    "            pr_table, \"recall\", \"precision\", title=\"Precision-Recall Curve\"\n",
    "        )\n",
    "        log_dict[\"pr_curve\"] = pr_plot\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating PR curve: {e}\")\n",
    "        pass\n",
    "\n",
    "    # Log everything to W&B\n",
    "    wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_model(model, test_loader, criterion, device, num_epochs):\n",
    "    \"\"\"Evaluate the best model on the test set.\"\"\"\n",
    "    # Load best model\n",
    "    checkpoint = torch.load(\"best_model_acc.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(\n",
    "        f\"Loaded best model from epoch {checkpoint['epoch']+1} with accuracy {checkpoint['val_acc']:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    final_test_loss, final_test_acc = validate(\n",
    "        model, test_loader, criterion, device, num_epochs\n",
    "    )\n",
    "    print(f\"Final test accuracy: {final_test_acc:.2f}%\")\n",
    "\n",
    "    return final_test_loss, final_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wandb(config):\n",
    "    \"\"\"Initialize and configure Weights & Biases.\"\"\"\n",
    "    wandb.init(\n",
    "        project=\"mri-alzheimers-classification\",\n",
    "        config=config,\n",
    "    )\n",
    "    return wandb.config\n",
    "\n",
    "\n",
    "def setup_training(model, train_dataset, learning_rate, device):\n",
    "    \"\"\"Set up training components: loss function, optimizer, scheduler.\"\"\"\n",
    "    # Calculate class weights for imbalanced data\n",
    "    num_ad = train_dataset.labels.count(1)\n",
    "    num_cn = train_dataset.labels.count(0)\n",
    "    total = num_ad + num_cn\n",
    "\n",
    "    # Inverse frequency weighting\n",
    "    weight_cn = total / (2 * num_cn) if num_cn > 0 else 1.0\n",
    "    weight_ad = total / (2 * num_ad) if num_ad > 0 else 1.0\n",
    "\n",
    "    class_weights = torch.tensor([weight_cn, weight_ad], device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Set up parameter groups with different learning rates\n",
    "    fc_params = list(model.resnet.fc.parameters())\n",
    "    other_params = [\n",
    "        p\n",
    "        for name, p in model.named_parameters()\n",
    "        if p.requires_grad and not any(p is fc_param for fc_param in fc_params)\n",
    "    ]\n",
    "\n",
    "    param_groups = [\n",
    "        {\"params\": other_params, \"lr\": learning_rate},\n",
    "        {\"params\": fc_params, \"lr\": learning_rate * 10},  # Higher LR for final layer\n",
    "    ]\n",
    "\n",
    "    optimizer = optim.AdamW(param_groups, lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=1, eta_min=learning_rate / 100\n",
    "    )\n",
    "\n",
    "    return criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load model checkpoint if available.\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_val_acc = checkpoint.get(\"best_val_acc\", 0.0)\n",
    "        best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n",
    "        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_val_acc = 0.0\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "    return start_epoch, best_val_acc, best_val_loss\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epoch,\n",
    "    val_acc,\n",
    "    val_loss,\n",
    "    best_val_acc,\n",
    "    best_val_loss,\n",
    "    filepath,\n",
    "    is_best=False,\n",
    "    metric_type=None,\n",
    "):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, filepath)\n",
    "\n",
    "    if is_best and metric_type:\n",
    "        try:\n",
    "            artifact = wandb.Artifact(f\"best_model_{metric_type}\", type=\"model\")\n",
    "            artifact.add_file(filepath)\n",
    "            wandb.log_artifact(artifact)\n",
    "            print(f\"Model saved (best {metric_type})!\")\n",
    "        except OSError as e:\n",
    "            print(f\"Failed to log artifact to W&B: {e}\")\n",
    "            print(\"Continuing training without W&B artifact logging...\")\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    start_epoch,\n",
    "    patience,\n",
    "    checkpoint_path,\n",
    "):\n",
    "    \"\"\"Execute the training loop with validation and early stopping.\"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, epoch)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save best model by accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                epoch,\n",
    "                val_acc,\n",
    "                val_loss,\n",
    "                best_val_acc,\n",
    "                best_val_loss,\n",
    "                \"best_model_acc.pth\",\n",
    "                is_best=True,\n",
    "                metric_type=\"acc\",\n",
    "            )\n",
    "            early_stopping_counter = 0\n",
    "\n",
    "        # Save best model by loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                epoch,\n",
    "                val_acc,\n",
    "                val_loss,\n",
    "                best_val_acc,\n",
    "                best_val_loss,\n",
    "                \"best_model_loss.pth\",\n",
    "                is_best=True,\n",
    "                metric_type=\"loss\",\n",
    "            )\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        # Save regular checkpoint\n",
    "        save_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            val_acc,\n",
    "            val_loss,\n",
    "            best_val_acc,\n",
    "            best_val_loss,\n",
    "            checkpoint_path,\n",
    "        )\n",
    "\n",
    "    return epoch + 1, best_val_acc, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_checkpoint(checkpoint_path, data_path, architecture, batch_size=2):\n",
    "    \"\"\"\n",
    "    Run validation on a specific checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the model checkpoint\n",
    "        data_path (str): Path to the dataset\n",
    "        batch_size (int): Batch size for validation\n",
    "\n",
    "    Returns:\n",
    "        dict: Validation metrics\n",
    "    \"\"\"\n",
    "    print(f\"Validating checkpoint: {checkpoint_path}\")\n",
    "\n",
    "    # Check if checkpoint exists\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Error: Checkpoint not found at {checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    # Create validation dataset and loader\n",
    "    val_dataset = MRIDataset(data_path, split=\"val\", apply_augmentation=False)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Create model with same architecture as used during training\n",
    "    model = MRIModel(num_classes=2, freeze_layers=True, architecture=architecture)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Load checkpoint\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        previous_val_acc = checkpoint.get(\"val_acc\", \"N/A\")\n",
    "        print(\n",
    "            f\"Loaded checkpoint from epoch {epoch+1} with previous validation accuracy: {previous_val_acc:.2f}%\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Set up criterion (without class weights since we're just evaluating)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Run validation\n",
    "    model.eval()\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device, epoch)\n",
    "\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # If MPS device, clear cache\n",
    "    if device.type == \"mps\":\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return {\"epoch\": epoch + 1, \"val_loss\": val_loss, \"val_acc\": val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path):\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"architecture\": \"r3d_18\",\n",
    "        \"dataset\": \"MRI-AD-CN\",\n",
    "        \"epochs\": 20,\n",
    "        \"batch_size\": 2,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"device\": str(device),\n",
    "        \"input_dimensions\": \"128x128x128\",\n",
    "        \"freeze_layers\": True,\n",
    "        \"data_augmentation\": True,\n",
    "        \"patience\": 5,\n",
    "    }\n",
    "\n",
    "    # Initialize wandb\n",
    "    config = setup_wandb(config)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    (\n",
    "        train_dataset,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        dataset_stats,\n",
    "    ) = create_datasets_and_loaders(\n",
    "        data_path, config.batch_size, config.data_augmentation\n",
    "    )\n",
    "\n",
    "    # Update wandb config with dataset stats\n",
    "    wandb.config.update(dataset_stats)\n",
    "\n",
    "    # Setup model\n",
    "    model, model_stats = setup_model(config.freeze_layers, device, config.architecture)\n",
    "\n",
    "    # Display model stats\n",
    "    print(f\"Total parameters: {model_stats['total_params']:,}\")\n",
    "    print(\n",
    "        f\"Trainable parameters: {model_stats['trainable_params']:,} ({model_stats['trainable_params']/model_stats['total_params']:.2%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Frozen parameters: {model_stats['frozen_params']:,} ({model_stats['frozen_params']/model_stats['total_params']:.2%})\"\n",
    "    )\n",
    "\n",
    "    # Update wandb config with model stats\n",
    "    wandb.config.update(model_stats)\n",
    "\n",
    "    # Watch model in wandb\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Setup training components\n",
    "    criterion, optimizer, scheduler = setup_training(\n",
    "        model, train_dataset, config.learning_rate, device\n",
    "    )\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    checkpoint_path = \"checkpoints/checkpoint.pth\"\n",
    "    start_epoch, best_val_acc, best_val_loss = load_checkpoint(\n",
    "        model, optimizer, scheduler, checkpoint_path\n",
    "    )\n",
    "\n",
    "    # Run to check validation pipeline changes\n",
    "    # validate_checkpoint(checkpoint_path, data_path, config.architecture, batch_size=config.batch_size)\n",
    "\n",
    "    # Train model\n",
    "    epochs_trained, best_val_acc, best_val_loss = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        config.epochs,\n",
    "        start_epoch,\n",
    "        config.patience,\n",
    "        checkpoint_path,\n",
    "    )\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    evaluate_final_model(model, test_loader, criterion, device, config.epochs)\n",
    "\n",
    "    # Log final metrics\n",
    "    wandb.run.summary[\"best_val_acc\"] = best_val_acc\n",
    "    wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "    wandb.run.summary[\"total_epochs\"] = epochs_trained\n",
    "\n",
    "    # Close wandb run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Ensure checkpoint directory exists\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "\n",
    "    data_path = \"./data/adni-split\"\n",
    "    main(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
